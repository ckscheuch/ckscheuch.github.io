[{"authors":["admin"],"categories":null,"content":"I am a PhD candidate at the Vienna Graduate School of Finance. My research focuses on the economics of technological innovations in the financial industry. I recently released a paper that shows how blockchain-based settlement introduces limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the availability of overdraft facilities through mobile banking apps. I currently investigate the potential of crowdfunding mechanisms to elicit demand information and improve the screening of viable projects.\nBefore starting my PhD studies, I was a research assistant in the field of labor economics at the Institute for Advanced Studies and an intern at Ithuba Capital and the Austrian Financial Market Authority.\n","date":1582243200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582243200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD candidate at the Vienna Graduate School of Finance. My research focuses on the economics of technological innovations in the financial industry. I recently released a paper that shows how blockchain-based settlement introduces limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the availability of overdraft facilities through mobile banking apps. I currently investigate the potential of crowdfunding mechanisms to elicit demand information and improve the screening of viable projects.","tags":null,"title":"Christoph Scheuch","type":"authors"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" In this note, I explore the relation between firm size and stock returns following the methodology outlined in Bali, Engle and Murray. In two earlier notes, I first prepared the CRSP sample in R and then estimated and analyzed market betas using the same data. I do not go into the theoretical foundations of the size effect, but rather focus on its estimation in a tidy manner. I just want to mention that the size effect refers to the observation that stocks with large market capitalization tend to have lower returns than stocks with small market capitalizations (e.g., Fama and French, 1992). The usual disclaimer applies, so the text below references an opinion and is for information purposes only and I do not intend to provide any investment advice. I also like to mention that the code below replicates most of the results of Bali et al up to a few basis points if I restrict the data to their sample period.\nI mainly use the following packages throughout this note:\nlibrary(tidyverse) # for grammar library(lubridate) # for working with dates library(broom) # for tidying up estimation results library(kableExtra) # for nicer html tables Calculating Firm Size First, I load the data that I prepared earlier and add the market betas. I follow Bali et al. and use the betas estimated using daily return data and a twelve month estimation window as the measure for a stock’s covariance with the market.\ncrsp \u0026lt;- read_rds(\u0026quot;data/crsp.rds\u0026quot;) betas \u0026lt;- read_rds(\u0026quot;data/betas_daily.rds\u0026quot;) crsp \u0026lt;- crsp %\u0026gt;% left_join(betas %\u0026gt;% select(permno, date_start, beta = beta_12m), by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) # note: date_start refers to beginning of month Therer are two approaches to measure the market capitalization of a stock. The simplest approach is to take the share price and number of shares outstanding as of the end of each month for which the market capitalization is measured. This is the approach I used in the earlier post to construct the variable mktcap. Alternatively, Fama and French calculate market capitalization as of the last trading day of June in each year and hold the value constant for the months from June of the same year until May of the next year. The benefit of this approach is that the market capitalization measure does not vary with short-term movements in the stock price which may cause unwanted correlation with the stock returns. I implement the Fama-French approach below by defining a reference date for each month and then adding the June market cap as an additional column.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date), reference_date = if_else(month \u0026lt; 6, paste(year-1, 6, 30, sep = \u0026quot;-\u0026quot;), paste(year, 6, 30, sep = \u0026quot;-\u0026quot;))) crsp_mktcap_ff \u0026lt;- crsp %\u0026gt;% filter(month == 6) %\u0026gt;% select(permno, reference_date, mktcap_ff = mktcap) crsp \u0026lt;- crsp %\u0026gt;% left_join(crsp_mktcap_ff, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) As it turns out, which approach we use has little impact on the results of the empirical analyses as the measures are highly correlated. While the timing of the calculation has a small effect, the distribution of measures might have a profound impact. As I show below, the distribution of market capitalization is highly skewed with a small number of stocks whose market capitalization is very large. For regression analyses, I hence use the log of market capitalization which I denote as size. For more meaningful interpretation of the market capitalization measures, I also calculate inflation-adjusted values (in terms of end of 2018 dollars).\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(mktcap_cpi = mktcap / cpi, mktcap_ff_cpi = mktcap_ff / cpi, size = log(mktcap), size_cpi = log(mktcap_cpi), size_ff = log(mktcap_ff), size_ff_cpi = log(mktcap_ff_cpi))  Summary Statistics I proceed to present summary statistics for the different measures of stock size using the full sample period from December 1925 until December 2018. I first compute corresponding summary statistics for each month and then average each summary statistics across all months. Note that I call the moments package to compute skewness and kurtosis.\n# compute summary statistics for each month crsp_ts \u0026lt;- crsp %\u0026gt;% select(date, mktcap, mktcap_ff:size_ff_cpi) %\u0026gt;% pivot_longer(cols = mktcap:size_ff_cpi, names_to = \u0026quot;measure\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(measure, date) %\u0026gt;% summarize(mean = mean(value), sd = sd(value), skew = moments::skewness(value), kurt = moments::kurtosis(value), min = min(value), q05 = quantile(value, 0.05), q25 = quantile(value, 0.25), q50 = quantile(value, 0.50), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95), max = max(value), n = n()) %\u0026gt;% ungroup() # average summary statistics across all months crsp_ts %\u0026gt;% select(-date) %\u0026gt;% group_by(measure) %\u0026gt;% summarize_all(list(mean)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   measure  mean  sd  skew  kurt  min  q05  q25  q50  q75  q95  max  n      mktcap  1025.13  4821.76  14.56  323.14  0.52  5.64  28.03  108.52  446.89  3891.08  131274.34  3159.74    mktcap_cpi  1870.46  8497.58  14.56  323.14  2.81  19.87  80.13  257.45  937.67  6992.14  225227.62  3159.74    mktcap_ff  1018.02  4736.35  14.34  311.39  0.67  6.02  28.67  109.70  448.02  3883.85  126975.24  3052.09    mktcap_ff_cpi  1860.41  8368.99  14.34  311.39  3.29  20.51  80.92  258.84  937.75  6978.79  218628.94  3052.09    size  3.92  1.80  0.31  2.99  -1.35  1.16  2.63  3.79  5.09  7.07  10.37  3159.74    size_cpi  5.44  1.80  0.31  2.99  0.17  2.68  4.16  5.32  6.61  8.59  11.89  3159.74    size_ff  3.94  1.79  0.33  2.98  -1.02  1.22  2.66  3.81  5.10  7.07  10.35  3052.09    size_ff_cpi  5.46  1.79  0.33  2.98  0.50  2.74  4.18  5.33  6.62  8.59  11.87  3052.09     To further investigate the distribution of market capitalization, I examine the percentage of total market cap that comprised very large stocks. The figure below plots the percentage of total stock market capitalization that is captured by the larges \\(x\\)% of stocks over the full sample period.\ncrsp_top \u0026lt;- crsp %\u0026gt;% select(permno, date, mktcap) %\u0026gt;% na.omit() %\u0026gt;% group_by(date) %\u0026gt;% mutate(top01 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.99), 1L, 0L), top05 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.95), 1L, 0L), top10 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.90), 1L, 0L), top25 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.75), 1L, 0L)) %\u0026gt;% summarize(total = sum(mktcap), top01 = sum(mktcap[top01 == 1]) / total, top05 = sum(mktcap[top05 == 1]) / total, top10 = sum(mktcap[top10 == 1]) / total, top25 = sum(mktcap[top25 == 1]) / total) %\u0026gt;% select(-total) %\u0026gt;% pivot_longer(cols = top01:top25, names_to = \u0026quot;group\u0026quot;) crsp_top %\u0026gt;% mutate(group = case_when(group == \u0026quot;top01\u0026quot; ~ \u0026quot;Largest 1% of Stocks\u0026quot;, group == \u0026quot;top05\u0026quot; ~ \u0026quot;Largest 5% of Stocks\u0026quot;, group == \u0026quot;top10\u0026quot; ~ \u0026quot;Largest 10% of Stocks\u0026quot;, group == \u0026quot;top25\u0026quot; ~ \u0026quot;Largest 25% of Stocks\u0026quot;), group = factor(group, levels = c(\u0026quot;Largest 1% of Stocks\u0026quot;, \u0026quot;Largest 5% of Stocks\u0026quot;, \u0026quot;Largest 10% of Stocks\u0026quot;, \u0026quot;Largest 25% of Stocks\u0026quot;))) %\u0026gt;% ggplot(aes(x = date, y = value, group = group)) + geom_line(aes(color = group, linetype = group)) + scale_y_continuous(labels = scales::percent, breaks = scales::pretty_breaks(), limits = c(0, 1)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Percentage of Total Market Capitalization\u0026quot;, fill = \u0026quot;\u0026quot;, color = \u0026quot;\u0026quot;, linetype = \u0026quot;\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic()  Correlations The next table presents the time-series averages of the monthly cross-sectional (Pearson product-moment) correlations between different measures of market capitalization.\ncor_matrix \u0026lt;- crsp %\u0026gt;% select(mktcap, size, mktcap_ff, size_ff, beta) %\u0026gt;% cor(use = \u0026quot;complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) cor_matrix[upper.tri(cor_matrix, diag = TRUE)] \u0026lt;- NA cor_matrix[-1, -5] %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))    mktcap  size  mktcap_ff  size_ff      size  0.34       mktcap_ff  0.98  0.33      size_ff  0.34  0.98  0.34     beta  0.05  0.28  0.05  0.28     Indeed, the different approaches are highly correlated, mostly because the total market capitalization of a firm in most cases changes very little over the course of a year (see next section). The correlation between beta and size indicates that larger stocks tend to have higher market betas.\n Persistence Next, I turn to the persistence analysis, i.e. the correlation of each variable with its lagged values over time. To compute these correlations, I define the following function where I use the fairly new curly-curly operator which simplifies writing functions around tidyverse pipelines.\ncompute_persistence \u0026lt;- function(var, tau) { dates \u0026lt;- crsp %\u0026gt;% distinct(date) %\u0026gt;% arrange(date) %\u0026gt;% mutate(date_lag = lag(date, tau)) correlation \u0026lt;- crsp %\u0026gt;% select(permno, date, {{ var }}) %\u0026gt;% left_join(dates, by = \u0026quot;date\u0026quot;) %\u0026gt;% left_join(crsp %\u0026gt;% select(permno, date, {{ var }}) %\u0026gt;% rename(\u0026quot;{{ var }}_lag\u0026quot; := {{ var }}), by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_lag\u0026quot;=\u0026quot;date\u0026quot;)) %\u0026gt;% select(contains(rlang::quo_text(enquo(var)))) %\u0026gt;% cor(use = \u0026quot;pairwise.complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) return(correlation[1, 2]) } persistence \u0026lt;- tribble(~tau, ~size, ~size_ff, 1, compute_persistence(size, 1), compute_persistence(size_ff, 1), 3, compute_persistence(size, 3), compute_persistence(size_ff, 3), 6, compute_persistence(size, 6), compute_persistence(size_ff, 6), 12, compute_persistence(size, 12), compute_persistence(size_ff, 12), 24, compute_persistence(size, 24), compute_persistence(size_ff, 24), 36, compute_persistence(size, 36), compute_persistence(size_ff, 36), 48, compute_persistence(size, 48), compute_persistence(size_ff, 48), 60, compute_persistence(size, 60), compute_persistence(size_ff, 60), 120, compute_persistence(size, 120), compute_persistence(size_ff, 120)) The following table provides the results of the persistence analysis. Indeed, the size measures exhibit a high persistence even up to 10 years. As the Fama-French measure is only updated every June, it mechanically exhibits a slightly higher persistence, but the impact is in fact minimal compared to the monthly updated variable.\npersistence %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   tau  size  size_ff      1  1.00  1.00    3  0.99  0.99    6  0.98  0.98    12  0.96  0.97    24  0.93  0.94    36  0.91  0.92    48  0.89  0.90    60  0.88  0.89    120  0.83  0.84      Portfolio Analysis To analyze the cross-sectional relation between market capitalization and future stock returns, I start with univariate portfolio analysis. The idea is to sort all stocks into portfolios based on quantiles calculated using NYSE stocks only and then calculate equal-weighted and value-weighted average returns for each portfolio.\nTurns out that I need to adjust the basic weighted.mean function to take out observations with missing weights because it would otherwise return missing values. This is important whenever a stock’s returns are define while the market capitalization is not (for whatever reason).\nweighted_mean \u0026lt;- function(x, w, ..., na.rm = FALSE){ if(na.rm){ x1 \u0026lt;- x[!is.na(x) \u0026amp; !is.na(w)] w \u0026lt;- w[!is.na(x) \u0026amp; !is.na(w)] x \u0026lt;- x1 } weighted.mean(x, w, ..., na.rm = FALSE) } The next function I use is a helper function that returns a list of quantile functions that I can throw into the summarize command to get a list of breakpoints for any given number of portfolios. The code snippet below is a modified version of the code found here. Most importantly, the function uses purrr:partial to fil in some arguments to the quantile functions.\nget_breakpoints_funs \u0026lt;- function(var, n_portfolios = 10) { # get relevant percentiles percentiles \u0026lt;- seq(0, 1, length.out = (n_portfolios + 1)) percentiles \u0026lt;- percentiles[percentiles \u0026gt; 0 \u0026amp; percentiles \u0026lt; 1] # construct set of named quantile functions percentiles_names \u0026lt;- map_chr(percentiles, ~paste0(rlang::quo_text(enquo(var)), \u0026quot;_q\u0026quot;, .x*100)) percentiles_funs \u0026lt;- map(percentiles, ~partial(quantile, probs = .x, na.rm = TRUE)) %\u0026gt;% set_names(nm = percentiles_names) return(percentiles_funs) } get_breakpoints_funs(mktcap, n_portfolios = 3) ## $mktcap_q33.3333333333333 ## \u0026lt;partialised\u0026gt; ## function (...) ## quantile(probs = .x, na.rm = TRUE, ...) ## ## $mktcap_q66.6666666666667 ## \u0026lt;partialised\u0026gt; ## function (...) ## quantile(probs = .x, na.rm = TRUE, ...) Given a list of breakpoints, I want to sort stocks into the corresponding portfolio. This turns out to be a bit tricky if I want to write a function that does the sorting for a flexible number of portfolios. Fortunately, my brilliant colleague Stefan Voigt pointed out the findInterval function to me.\nget_portfolio \u0026lt;- function(x, breakpoints) { portfolio \u0026lt;- as.integer(1 + findInterval(x, unlist(breakpoints))) return(portfolio) } The next function finally constructs univariate portfolios for any sorting variable using NYSE breakpoints. Note that the rationale behind using only NYSE stocks to calculate quantiles is that for a large portio of the sample period, NYSE stocks tended to be much larger than stocks listed on AMEX or NASDAQ. If the breakpoints were calculated using all stocks in the CRSP sample, the breakpoints would effectively just separate NYSE stocks from all other stocks. However, it does not ensure an equal number of NYSE stocks in each portfolios, as I show below. Again, I leverage the new curly-curly operator and associated naming capabilities.\nconstruct_univariate_portfolios \u0026lt;- function(data, var, n_portfolios = 10) { # keep only observations where the sorting variable is defined data \u0026lt;- data %\u0026gt;% filter(!is.na({{ var }})) # determine breakpoints based on NYSE stocks only data_nyse \u0026lt;- data %\u0026gt;% filter(exchange == \u0026quot;NYSE\u0026quot;) %\u0026gt;% select(date, {{ var }}) var_funs \u0026lt;- get_breakpoints_funs({{ var }}, n_portfolios) quantiles \u0026lt;- data_nyse %\u0026gt;% group_by(date) %\u0026gt;% summarize_at(vars({{ var }}), lst(!!! var_funs)) %\u0026gt;% group_by(date) %\u0026gt;% nest() %\u0026gt;% rename(quantiles = data) # independently sort all stocks into portfolios based on NYSE breakpoints portfolios \u0026lt;- data %\u0026gt;% left_join(quantiles, by = \u0026quot;date\u0026quot;) %\u0026gt;% mutate(portfolio = map2_dbl({{ var }}, quantiles, get_portfolio)) %\u0026gt;% select(-quantiles) # compute average portfolio characteristics portfolios_ts \u0026lt;- portfolios %\u0026gt;% group_by(portfolio, date) %\u0026gt;% summarize(ret_ew = mean(ret_adj_excess_f1, na.rm = TRUE), ret_vw = weighted_mean(ret_adj_excess_f1, mktcap, na.rm = TRUE), ret_mkt = mean(mkt_ff_excess_f1, na.rm = TRUE), \u0026quot;{{ var }}_mean\u0026quot; := mean({{ var }}, na.rm = TRUE), \u0026quot;{{ var }}_sum\u0026quot; := sum({{ var }}, na.rm = TRUE), beta = mean(beta, na.rm = TRUE), n_stocks = n(), n_stocks_nyse = sum(exchange == \u0026quot;NYSE\u0026quot;, na.rm = TRUE)) %\u0026gt;% na.omit() %\u0026gt;% ungroup() ## long-short portfolio portfolios_ts_ls \u0026lt;- portfolios_ts %\u0026gt;% select(portfolio, date, ret_ew, ret_vw, ret_mkt) %\u0026gt;% filter(portfolio %in% c(max(portfolio), min(portfolio))) %\u0026gt;% pivot_wider(names_from = portfolio, values_from = c(ret_ew, ret_vw)) %\u0026gt;% mutate(ret_ew = .[[4]] - .[[3]], ret_vw = .[[6]] - .[[5]], portfolio = paste0(n_portfolios, \u0026quot;-1\u0026quot;)) %\u0026gt;% select(portfolio, date, ret_ew, ret_vw, ret_mkt) ## combine everything out \u0026lt;- portfolios_ts %\u0026gt;% mutate(portfolio = as.character(portfolio)) %\u0026gt;% bind_rows(portfolios_ts_ls) %\u0026gt;% mutate(portfolio = factor(portfolio, levels = c(as.character(seq(1, 10, 1)), paste0(n_portfolios, \u0026quot;-1\u0026quot;)))) return(out) } portfolios_mktcap \u0026lt;- construct_univariate_portfolios(crsp, mktcap) portfolios_mktcap_ff \u0026lt;- construct_univariate_portfolios(crsp, mktcap_ff) The table below provides the summary statistics for each portfolio.\n# compute market cap totals mktcap_totals \u0026lt;- crsp %\u0026gt;% group_by(date) %\u0026gt;% summarize(mktcap_total = sum(mktcap, na.rm = TRUE), mktcap_ff_total = sum(mktcap_ff, na.rm = TRUE)) # portfolio characteristics portfolios_mktcap_table \u0026lt;- portfolios_mktcap %\u0026gt;% filter(portfolio != \u0026quot;10-1\u0026quot;) %\u0026gt;% left_join(mktcap_totals, by = \u0026quot;date\u0026quot;) %\u0026gt;% group_by(portfolio) %\u0026gt;% summarize(mktcap_mean = mean(mktcap_mean), mktcap_share = mean(mktcap_sum / mktcap_total) * 100, n = mean(n_stocks), n_nyse = mean(n_stocks_nyse / n_stocks) * 100, beta = mean(beta)) %\u0026gt;% t() portfolios_mktcap_table \u0026lt;- portfolios_mktcap_table[-1, ] %\u0026gt;% as.numeric() %\u0026gt;% matrix(., ncol = 10) colnames(portfolios_mktcap_table) \u0026lt;- seq(1, 10, 1) rownames(portfolios_mktcap_table) \u0026lt;- c(\u0026quot;Market Cap (in $B)\u0026quot;, \u0026quot;% of Total Market Cap\u0026quot;, \u0026quot;Number of Stocks\u0026quot;, \u0026quot;% NYSE Stocks\u0026quot; , \u0026quot;Beta\u0026quot;) # repeat for fama-french measure portfolios_mktcap_ff_table \u0026lt;- portfolios_mktcap_ff %\u0026gt;% filter(portfolio != \u0026quot;10-1\u0026quot;) %\u0026gt;% left_join(mktcap_totals, by = \u0026quot;date\u0026quot;) %\u0026gt;% group_by(portfolio) %\u0026gt;% summarize(mktcap_ff_mean = mean(mktcap_ff_mean), mktcap_ff_share = mean(mktcap_ff_sum / mktcap_ff_total) * 100, n = mean(n_stocks), n_nyse = mean(n_stocks_nyse / n_stocks) * 100, beta = mean(beta)) %\u0026gt;% t() portfolios_mktcap_ff_table \u0026lt;- portfolios_mktcap_ff_table[-1, ] %\u0026gt;% as.numeric() %\u0026gt;% matrix(., ncol = 10) colnames(portfolios_mktcap_ff_table) \u0026lt;- seq(1, 10, 1) rownames(portfolios_mktcap_ff_table) \u0026lt;- c(\u0026quot;Market Cap (in $B)\u0026quot;, \u0026quot;% of Total Market Cap\u0026quot;, \u0026quot;Number of Stocks\u0026quot;, \u0026quot;% NYSE Stocks\u0026quot; , \u0026quot;Beta\u0026quot;) # combine to table and print html rbind(portfolios_mktcap_table, portfolios_mktcap_ff_table) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Turan-Engle-Murray)\u0026quot;, 1, 5) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Fama-French)\u0026quot;, 6, 10)    1  2  3  4  5  6  7  8  9  10     Market Cap (Turan-Engle-Murray)    Market Cap (in $B)  25.35  99.91  181.20  287.58  437.49  654.42  996.57  1672.01  3340.98  16572.63    % of Total Market Cap  1.13  1.17  1.44  1.89  2.51  3.33  4.77  7.52  13.76  62.48    Number of Stocks  1417.91  387.87  266.52  217.58  186.88  162.59  150.40  141.68  134.12  129.67    % NYSE Stocks  43.31  56.33  64.19  69.66  74.94  81.16  85.12  88.54  92.15  94.56    Beta  0.76  0.99  1.02  1.02  1.02  1.01  1.02  1.01  1.00  1.01   Market Cap (Fama-French)    Market Cap (in $B)  26.20  101.63  181.51  287.22  433.62  645.69  980.72  1645.79  3291.40  16201.58    % of Total Market Cap  1.14  1.15  1.43  1.88  2.48  3.31  4.75  7.54  13.81  62.51    Number of Stocks  1360.22  363.88  255.50  209.14  180.02  158.73  146.55  139.21  131.69  127.42    % NYSE Stocks  44.23  57.62  65.21  70.72  76.06  81.82  85.80  88.73  92.39  94.79    Beta  0.76  1.00  1.02  1.03  1.03  1.01  1.02  1.01  1.00  1.01     The numbers are again very similar for both measures. The table illustrates the usage of NYSE breakpoints as the share of NYSE stocks increases across portfolios. The table also shows that the portfolio of largest firms takes up more than 60% of total market capitalization although it contains on average only about 130 firms.\nFor illustrative purposes, I also plot the cumulative value-weighted excess returns over time to get a better feeling for the data.\nportfolios_mktcap %\u0026gt;% group_by(portfolio) %\u0026gt;% select(portfolio, date, ret_vw) %\u0026gt;% arrange(date) %\u0026gt;% mutate(cum_ret = cumsum(ret_vw)) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(x = date, y = cum_ret, group = as.factor(portfolio))) + geom_line(aes(color = as.factor(portfolio), linetype = as.factor(portfolio))) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Log Return (in %)\u0026quot;, color = \u0026quot;Size Portfolio\u0026quot;, linetype = \u0026quot;Size Portfolio\u0026quot;) + scale_y_continuous(labels = scales::comma, breaks = scales::pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Given the portfolio returns, I want to evaluate whether a portfolio exhibits on average positive or negative excess returns. The following function estimates the mean excess return and CAPM alpha for each portfolio and computes corresponding Newey and West (1987) \\(t\\)-statistics (using six lags as common in the literature) testing the null hypothesis that the average portfolio excess return or CAPM alpha is zero. I just recycle the function I wrote in this post.\nestimate_portfolio_returns \u0026lt;- function(data, ret) { ## estimate average returns per portfolio average_ret \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(rlang::as_name(enquo(ret)),\u0026quot; ~ 1\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6)))) %\u0026gt;% broom::tidy(model) %\u0026gt;% ungroup() %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() ## estimate capm alpha per portfolio average_capm_alpha \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(rlang::as_name(enquo(ret)),\u0026quot; ~ 1 + ret_mkt\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6))[1])) %\u0026gt;% broom::tidy(model)%\u0026gt;% ungroup() %\u0026gt;% filter(term == \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() ## construct output table out \u0026lt;- rbind(average_ret, average_capm_alpha) colnames(out) \u0026lt;-c(as.character(seq(1, 10, 1)), \u0026quot;10-1\u0026quot;) rownames(out) \u0026lt;- c(\u0026quot;Excess Return\u0026quot;, \u0026quot;t-Stat\u0026quot; , \u0026quot;CAPM Alpha\u0026quot;, \u0026quot;t-Stat\u0026quot;) return(out) } rbind(estimate_portfolio_returns(portfolios_mktcap, ret_ew), estimate_portfolio_returns(portfolios_mktcap_ff, ret_ew), estimate_portfolio_returns(portfolios_mktcap, ret_vw), estimate_portfolio_returns(portfolios_mktcap_ff, ret_vw)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Turan-Engle-Murray) - Equal-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Fama-French) - Equal-Weighted Portfolio Returns\u0026quot;, 5, 8)%\u0026gt;% pack_rows(\u0026quot;Market Cap (Turan-Engle-Murray) - Value-Weighted Portfolio Returns\u0026quot;, 9, 12) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Fama-French) - Value-Weighted Portfolio Returns\u0026quot;, 13, 16)    1  2  3  4  5  6  7  8  9  10  10-1     Market Cap (Turan-Engle-Murray) - Equal-Weighted Portfolio Returns    Excess Return  1.52  0.92  0.86  0.83  0.79  0.76  0.70  0.72  0.63  0.51  -1.01    t-Stat  3.49  2.86  2.78  3.04  3.01  3.02  2.89  3.15  2.91  2.63  -3.23    CAPM Alpha  0.55  -0.02  -0.05  -0.03  -0.04  -0.04  -0.08  -0.03  -0.08  -0.14  -0.68    t-Stat  2.20  -0.09  -0.36  -0.21  -0.35  -0.36  -0.73  -0.27  -0.69  -1.61  -2.90   Market Cap (Fama-French) - Equal-Weighted Portfolio Returns    Excess Return  1.37  0.95  0.92  0.83  0.80  0.79  0.73  0.70  0.64  0.52  -0.85    t-Stat  3.44  2.82  3.01  2.97  3.02  3.08  2.99  3.03  2.96  2.63  -3.15    CAPM Alpha  0.43  0.01  0.02  -0.03  -0.04  -0.03  -0.05  -0.05  -0.08  -0.14  -0.57    t-Stat  1.88  0.04  0.11  -0.20  -0.34  -0.25  -0.44  -0.44  -0.72  -1.44  -2.66   Market Cap (Turan-Engle-Murray) - Value-Weighted Portfolio Returns    Excess Return  1.25  0.91  0.87  0.83  0.79  0.75  0.70  0.72  0.62  0.51  -0.74    t-Stat  3.00  2.86  2.81  3.05  3.01  3.01  2.91  3.19  2.91  2.69  -2.49    CAPM Alpha  0.26  -0.02  -0.04  -0.03  -0.04  -0.05  -0.07  -0.03  -0.08  -0.11  -0.38    t-Stat  1.17  -0.13  -0.28  -0.20  -0.35  -0.37  -0.68  -0.23  -0.67  -1.00  -1.74   Market Cap (Fama-French) - Value-Weighted Portfolio Returns    Excess Return  1.00  0.85  0.89  0.81  0.79  0.78  0.72  0.70  0.63  0.51  -0.49    t-Stat  2.79  2.69  2.99  3.01  3.04  3.11  3.02  3.11  2.95  2.70  -2.08    CAPM Alpha  0.05  -0.06  0.00  -0.02  -0.03  -0.02  -0.04  -0.03  -0.07  -0.11  -0.16    t-Stat  0.27  -0.39  0.03  -0.16  -0.28  -0.16  -0.33  -0.24  -0.65  -0.99  -0.90     The results show that all porfolios deliver positive excess returns, but the returns decrease as firm size increases. As a consequence, the portfolio that is long the largest firms and short the smallest firms yields statistically significant negative excess returns. Even though the CAPM alpha of each portfolio is statistically indistinguishable from zero, the long-short portfolio still yields negative alphas (except for the Fama-French value-weighted portfolios). Taken together, the results of the univariate portfolio analysis indicate a negative relation between firm market capitalization and future stock returns.\n Regression Analysis As a last step, I analyze the relation between firm size and stock returns using Fama and MacBeth (1973) regression analysis. Each month, I perform a cross-sectional regression of one-month-ahead excess stock returns on the given measure of firm size. Time-series averages over these cross-sectional regressions then provide the desired results.\nAgain, I essetnailly recylce functions that I developed in an earlier post. Note that I condition on all three explanatory variables being defined in the data, regardless of which measure I am using. I do this to ensure comparability across results, i.e. I want to run the same set of regressions across different specifications.\nwinsorize \u0026lt;- function(x, cut = 0.005){ cut_point_top \u0026lt;- quantile(x, 1 - cut, na.rm = TRUE) cut_point_bottom \u0026lt;- quantile(x, cut, na.rm = TRUE) i \u0026lt;- which(x \u0026gt;= cut_point_top) x[i] \u0026lt;- cut_point_top j \u0026lt;- which(x \u0026lt;= cut_point_bottom) x[j] \u0026lt;- cut_point_bottom return(x) } fama_macbeth_regression \u0026lt;- function(data, model, cut = 0.005) { # prepare and winsorize data data_nested \u0026lt;- data %\u0026gt;% filter(!is.na(ret_adj_f1) \u0026amp; !is.na(size) \u0026amp; !is.na(size_ff) \u0026amp; !is.na(beta)) %\u0026gt;% group_by(date) %\u0026gt;% mutate_at(vars(size, size_ff, beta), ~winsorize(., cut = cut)) %\u0026gt;% nest() # perform cross-sectional regressions for each month cross_sectional_regs \u0026lt;- data_nested %\u0026gt;% mutate(model = map(data, ~lm(enexpr(model), data = .x))) %\u0026gt;% mutate(tidy = map(model, broom::tidy), glance = map(model, broom::glance), n = map(model, stats::nobs)) # extract average coefficient estimates fama_macbeth_coefs \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% summarize(coefficient = mean(estimate)) # compute newey-west standard errors of average coefficient estimates newey_west_std_errors \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% arrange(date) %\u0026gt;% group_modify(~enframe(sqrt(diag(sandwich::NeweyWest(lm(estimate ~ 1, data = .x), lag = 6))))) %\u0026gt;% select(term, nw_std_error = value) # put coefficient estimates and standard errors together and compute t-statistics fama_macbeth_coefs \u0026lt;- fama_macbeth_coefs %\u0026gt;% left_join(newey_west_std_errors, by = \u0026quot;term\u0026quot;) %\u0026gt;% mutate(nw_t_stat = coefficient / nw_std_error) %\u0026gt;% select(term, coefficient, nw_t_stat) %\u0026gt;% pivot_longer(cols = c(coefficient, nw_t_stat), names_to = \u0026quot;statistic\u0026quot;) %\u0026gt;% mutate(statistic = paste(term, statistic, sep = \u0026quot; \u0026quot;)) %\u0026gt;% select(-term) # extract average r-squared and average number of observations fama_macbeth_stats \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(c(glance, n)) %\u0026gt;% ungroup() %\u0026gt;% summarize(adj_r_squared = mean(adj.r.squared), n = mean(n)) %\u0026gt;% pivot_longer(c(adj_r_squared, n), names_to = \u0026quot;statistic\u0026quot;) # combine desired output and return results out \u0026lt;- rbind(fama_macbeth_coefs, fama_macbeth_stats) return(out) } m1 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size) %\u0026gt;% rename(m1 = value) m2 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size + beta) %\u0026gt;% rename(m2 = value) m3 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size_ff) %\u0026gt;% rename(m3 = value) m4 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size_ff + beta) %\u0026gt;% rename(m4 = value) The following table provides the regression results for various specifications.\nregression_table \u0026lt;- m1 %\u0026gt;% full_join(m2, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% full_join(m3, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% full_join(m4, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% right_join(tibble(statistic = c(\u0026quot;(Intercept) coefficient\u0026quot;, \u0026quot;(Intercept) nw_t_stat\u0026quot;, \u0026quot;size coefficient\u0026quot;, \u0026quot;size nw_t_stat\u0026quot;, \u0026quot;size_ff coefficient\u0026quot;, \u0026quot;size_ff nw_t_stat\u0026quot;, \u0026quot;beta coefficient\u0026quot;, \u0026quot;beta nw_t_stat\u0026quot;, \u0026quot;adj_r_squared\u0026quot;, \u0026quot;n\u0026quot;)), by = \u0026quot;statistic\u0026quot;) colnames(regression_table) \u0026lt;- c(\u0026quot;statistic\u0026quot;, \u0026quot;m1\u0026quot;, \u0026quot;m2\u0026quot;, \u0026quot;m3\u0026quot;, \u0026quot;m4\u0026quot;) regression_table[, 1] \u0026lt;- c(\u0026quot;intercept\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;size\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;size_ff\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;adj_r_squared\u0026quot;, \u0026quot;n\u0026quot;) regression_table %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   statistic  m1  m2  m3  m4      intercept  1.63  1.66  1.54  1.60    t-stat  4.18  4.85  4.10  4.77    size  -0.17  -0.17      t-stat  -3.69  -3.16      size_ff    -0.15  -0.14    t-stat    -3.39  -2.84    beta   -0.09   -0.14    t-stat   -0.68   -1.06    adj_r_squared  0.02  0.05  0.02  0.04    n  2944.15  2944.15  2944.15  2944.15     The regression results show a statisticall significant negative relation between firm size and abnormal stock returns. The results also show that controling for market beta neither yields a statistically significant relation to stock returns, nor does it impact the coefficient estimate on firm size. This result contradicts the fundamental prediction of the CAPM that market beta is the only determinant of cross-sectional variation in expected returns. Also note that the explanatory power of size and beta is still very low.\n ","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582243200,"objectID":"7a00c6e36a3c165aa4ab709edb6cd6df","permalink":"/post/asset-pricing/size/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/post/asset-pricing/size/","section":"post","summary":"On the relation of firm size and future stock returns","tags":["Academic"],"title":"Tidy Asset Pricing - Part III: Size and Stock Returns","type":"post"},{"authors":["[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581552000,"objectID":"de2008daaf489ae7401ced461e486911","permalink":"/publication/demand-uncertainty/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/publication/demand-uncertainty/","section":"publication","summary":"Reward-based crowdfunding allows entrepreneurs to sell claims on future products to finance investments and at the same time generate demand information that benefits screening for viable projects. I characterize the profit-maximizing crowdfunding mechanism when the entrepreneur knows neither the number of consumers who positively value the product, nor their reservation prices. Entrepreneurs can finance all viable projects by committing to prices that decrease in the number of pledgers, which grants consumers with high reservation prices information rents. However, if information rents exceed a threshold, entrepreneurs prefer fixed high prices that lead to no financing in states with low reservation prices.","tags":null,"title":"Crowdfunding and Demand Uncertainty","type":"publication"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" This note proposes an estimation and evaluation procedure for market betas following Bali, Engle and Murray. I do not go into details about the foundations of market beta, but simply refer to any treatment of the Capital Asset Pricing Model (CAPM). This note has two objectives: the first one is to implement several approaches to estimate a stock’s beta in a tidy manner and to empirically examine these measures. The second objective is to analyze the cross-sectional relation between stock returns and market beta. The text below references an opinion and is for information purposes only. I do not intend to provide any investment advice.\nI mainly use the following packages throughout this note:\nlibrary(tidyverse) library(slider) # for rolling window operations (https://github.com/DavisVaughan/slider) library(kableExtra) # for nicer html tables First, I load the monthly return data that I prepared in an earlier post and compute market excess returns required for the estimation procedure.\ncrsp \u0026lt;- read_rds(\u0026quot;data/crsp.rds\u0026quot;) crsp \u0026lt;- crsp %\u0026gt;% mutate(ret_excess = ret_adj - rf_ff, mkt_ff_excess = mkt_ff - rf_ff) %\u0026gt;% select(permno, date_start, ret_excess, mkt_ff_excess) Estimation According to the CAPM, cross-sectional variation in expected asset returns should be a function of the covariance between the return of the asset and the return on the market portfolio – the beta. To estimate stock-specific betas, I have to regress excess stock returns on excess returns of the market portfolio. Throughout the note, I use the market excess return and risk-free rate provided in Ken French’s data library. The estimation procedure is based on a rolling window estimation where I may use either monthly or daily returns and different window lengths. I follow Bali et al. and examine nine different combinations of estimation periods and data frequencies. For monthly return data, I use excess return observations over the past 1, 2, 3 and 5 years, requiring at least 10, 20, 24 and 24 valid return observations, respectively. For daily data measures, I use period lengths of 1, 3, 6, 12 and 24 months and require 15, 50, 100, 200 and 450 days of valid return data. The function below implements these requirements for a given estimation window and data frequency and returns the estimated beta.\ncapm_regression \u0026lt;- function(x, window, freq) { # drop missing values x \u0026lt;- na.omit(x) # determine minimum number of observations depending on window size and data frequency if (freq == \u0026quot;monthly\u0026quot;) { if (window == 12) {check \u0026lt;- 10} if (window == 24) {check \u0026lt;- 20} if (window == 36) {check \u0026lt;- 24} if (window == 60) {check \u0026lt;- 24} } if (freq == \u0026quot;daily\u0026quot;) { if (window == 1) {check \u0026lt;- 15} if (window == 3) {check \u0026lt;- 50} if (window == 6) {check \u0026lt;- 100} if (window == 12) {check \u0026lt;- 200} if (window == 24) {check \u0026lt;- 450} } # check if minimum number of obervations is satisfied if (nrow(x) \u0026lt; check) { return(tibble(beta = NA)) } else { reg \u0026lt;- lm(ret ~ mkt, data = x) return(tibble(beta = reg$coefficients[2])) } } To perform the rolling estimation, I employ the new slider package of Davis Vaughan which provides a family of sliding window functions similar to purrr::map(). Most importantly, the slide_index function is able to handle months in its window input. I thus avoid using any time-series package (e.g., zoo) and converting the data to fit the package functions, but rather stay in the tibble world.\nrolling_capm_regression \u0026lt;- function(data, window, freq = \u0026quot;monthly\u0026quot;, col_name, ...) { # prepare data x \u0026lt;- data %\u0026gt;% select(date = date_start, ret = ret_excess, mkt = mkt_ff_excess) %\u0026gt;% arrange(date) # slide across dates betas \u0026lt;- slide_index(x, x$date, ~capm_regression(.x, window, freq), .before = months(window), .complete = FALSE) %\u0026gt;% bind_rows() # collect output col_name \u0026lt;- quo_name(col_name) mutate(data, !! col_name := betas$beta) } I use the above function to compute several different variants of the beta estimator. However, the estimation takes up a considerable amount of time (around 5 hours). In principle, I could considerably speed up the estimation procedure by parallelizing it across stocks. However, for the sake of exposition, I just provide the code for the joint estimation below.\nbetas_monthly \u0026lt;- crsp %\u0026gt;% group_by(permno) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 12, freq = \u0026quot;monthly\u0026quot;, col_name = \u0026quot;beta_1y\u0026quot;)) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 24, freq = \u0026quot;monthly\u0026quot;, col_name = \u0026quot;beta_2y\u0026quot;)) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 36, freq = \u0026quot;monthly\u0026quot;, col_name = \u0026quot;beta_3y\u0026quot;)) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 60, freq = \u0026quot;monthly\u0026quot;, col_name = \u0026quot;beta_5y\u0026quot;)) %\u0026gt;% select(-c(ret_excess, mkt_ff_excess)) Next, I use the same function to perform rolling window estimation on the daily return data. Note that this sample has about 3 GB and that the computation takes about 6 hours (but is again in principle parallelizable).\ncrsp_daily \u0026lt;- read_rds(\u0026quot;data/crsp_daily.rds\u0026quot;) crsp_daily \u0026lt;- crsp_daily %\u0026gt;% mutate(ret_excess = ret - rf_ff, mkt_ff_excess = mkt_ff - rf_ff) %\u0026gt;% select(permno, date_start, ret_excess, mkt_ff_excess) betas_daily \u0026lt;- crsp_daily %\u0026gt;% group_by(permno) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 1, freq = \u0026quot;daily\u0026quot;, col_name = \u0026quot;beta_1m\u0026quot;)) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 3, freq = \u0026quot;daily\u0026quot;, col_name = \u0026quot;beta_3m\u0026quot;)) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 6, freq = \u0026quot;daily\u0026quot;, col_name = \u0026quot;beta_6m\u0026quot;)) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 12, freq = \u0026quot;daily\u0026quot;, col_name = \u0026quot;beta_12m\u0026quot;)) %\u0026gt;% group_modify(~rolling_capm_regression(.x, window = 24, freq = \u0026quot;daily\u0026quot;, col_name = \u0026quot;beta_24m\u0026quot;)) %\u0026gt;% distinct(permno, date_start, beta_1m, beta_3m, beta_6m, beta_12m, beta_24m)  Summary Statistics Let us add the beta estimates to the monthly crsp sample. As a first evaluation, I compute average summary statistics across all months.\ncrsp \u0026lt;- crsp %\u0026gt;% left_join(betas_monthly, by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) %\u0026gt;% left_join(betas_daily, by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) crsp_ts \u0026lt;- crsp %\u0026gt;% select(date, beta_1y:beta_24m) %\u0026gt;% pivot_longer(cols = beta_1y:beta_24m, names_to = \u0026quot;measure\u0026quot;, values_drop_na = TRUE) %\u0026gt;% group_by(measure, date) %\u0026gt;% summarize(mean = mean(value), sd = sd(value), skew = moments::skewness(value), kurt = moments::kurtosis(value), min = min(value), q05 = quantile(value, 0.05), q25 = quantile(value, 0.25), q50 = quantile(value, 0.50), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95), max = max(value), n = n()) %\u0026gt;% ungroup() crsp_ts %\u0026gt;% select(-date) %\u0026gt;% group_by(measure) %\u0026gt;% summarize_all(list(mean)) %\u0026gt;% mutate(measure = factor(measure, levels = c(\u0026quot;beta_1y\u0026quot;, \u0026quot;beta_2y\u0026quot;, \u0026quot;beta_3y\u0026quot;, \u0026quot;beta_5y\u0026quot;, \u0026quot;beta_1m\u0026quot;, \u0026quot;beta_3m\u0026quot;, \u0026quot;beta_6m\u0026quot;, \u0026quot;beta_12m\u0026quot;, \u0026quot;beta_24m\u0026quot;))) %\u0026gt;% arrange(measure) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   measure  mean  sd  skew  kurt  min  q05  q25  q50  q75  q95  max  n      beta_1y  1.16  1.12  0.65  17.72  -7.10  -0.35  0.50  1.06  1.72  3.00  11.55  2969.62    beta_2y  1.16  0.82  0.58  9.14  -3.74  0.05  0.64  1.08  1.60  2.57  7.10  2776.02    beta_3y  1.17  0.72  0.55  7.12  -2.71  0.18  0.69  1.09  1.56  2.41  5.83  2713.06    beta_5y  1.17  0.65  0.43  8.56  -2.47  0.27  0.73  1.10  1.53  2.29  5.19  2733.32    beta_1m  0.87  0.95  0.35  22.23  -7.03  -0.40  0.31  0.79  1.37  2.42  9.06  3158.70    beta_3m  0.88  0.76  0.35  10.12  -4.03  -0.15  0.39  0.81  1.32  2.17  5.99  3120.56    beta_6m  0.89  0.66  0.40  6.23  -2.52  -0.02  0.43  0.83  1.29  2.05  4.58  3070.97    beta_12m  0.90  0.59  0.44  4.10  -1.53  0.07  0.47  0.84  1.27  1.95  3.58  2973.85    beta_24m  0.90  0.54  0.45  3.37  -0.88  0.14  0.50  0.85  1.25  1.87  3.01  2736.83     There appears to be more stability in the distribution of estimated betas as I extend the estimation period. This might indicate that longer estimation periods yield more precise beta estimates. However, the average number of observations mechanically decreases as the estimation windows increase. Moreover, estimates based on monthly observations are on average higher than estimates based on daily returns.\n Correlations The next table presents the time-series averages of the monthly cross-sectional (Pearson product-moment) correlations between different measures of market beta.\ncor_matrix \u0026lt;- crsp %\u0026gt;% select(contains(\u0026quot;beta\u0026quot;)) %\u0026gt;% cor(use = \u0026quot;complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) cor_matrix[upper.tri(cor_matrix, diag = FALSE)] \u0026lt;- NA cor_matrix[, ] %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))    beta_1y  beta_2y  beta_3y  beta_5y  beta_1m  beta_3m  beta_6m  beta_12m  beta_24m      beta_1y  1.00            beta_2y  0.71  1.00           beta_3y  0.61  0.85  1.00          beta_5y  0.53  0.75  0.87  1.00         beta_1m  0.21  0.25  0.26  0.27  1.00        beta_3m  0.28  0.33  0.35  0.35  0.78  1.00       beta_6m  0.34  0.39  0.41  0.42  0.67  0.86  1.00      beta_12m  0.38  0.45  0.47  0.48  0.58  0.75  0.88  1.0     beta_24m  0.35  0.49  0.53  0.54  0.51  0.67  0.78  0.9  1     Correlations range from as low as 0.21 between the one-year measure based on monthly data and the one-month measure based on daily data to as high as 0.9 between the one-year and two-year measure based on daily data. Again, correlations seem to increase as the length of the measurement period increases.\n Persistence If I want to use a measure of a stock’s beta in fruther analyses, I want the beta to be fairly stable over time. To examine whether this is the case, I calculate the persistence of each measure as the correlation between month \\(t\\) and month \\(t+\\tau\\). The table below presents the persistence measures for lags one, three, six, 12, 24, 36, 48, 60 and 120 months.\ncompute_persistence \u0026lt;- function(data, col, tau) { dates \u0026lt;- data %\u0026gt;% distinct(date) %\u0026gt;% arrange(date) %\u0026gt;% mutate(date_lag = lag(date, tau)) col \u0026lt;- enquo(col) correlation \u0026lt;- data %\u0026gt;% select(permno, date, !!col) %\u0026gt;% left_join(dates, by = \u0026quot;date\u0026quot;) %\u0026gt;% left_join(data %\u0026gt;% select(permno, date, !! col), by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_lag\u0026quot;=\u0026quot;date\u0026quot;)) %\u0026gt;% select(-c(permno, date, date_lag)) %\u0026gt;% cor(use = \u0026quot;pairwise.complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) return(correlation[1, 2]) } tau \u0026lt;- c(1, 3, 6, 12, 24, 36, 48, 60, 120) beta_1y \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_1y, .x)) beta_2y \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_2y, .x)) beta_3y \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_3y, .x)) beta_5y \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_5y, .x)) beta_1m \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_1m, .x)) beta_3m \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_3m, .x)) beta_6m \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_6m, .x)) beta_12m \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_12m, .x)) beta_24m \u0026lt;- map_dbl(tau, ~compute_persistence(crsp, beta_24m, .x)) tibble(tau = tau, beta_1y = beta_1y, beta_2y = beta_2y, beta_3y = beta_3y, beta_5y = beta_5y, beta_1m = beta_1m, beta_3m = beta_3m, beta_6m = beta_6m, beta_12m = beta_12m, beta_24m = beta_24m) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   tau  beta_1y  beta_2y  beta_3y  beta_5y  beta_1m  beta_3m  beta_6m  beta_12m  beta_24m      1  0.91  0.96  0.98  0.99  0.61  0.85  0.93  0.98  0.99    3  0.75  0.89  0.93  0.96  0.26  0.56  0.80  0.92  0.97    6  0.54  0.79  0.88  0.93  0.25  0.41  0.61  0.84  0.94    12  0.19  0.61  0.76  0.88  0.23  0.38  0.51  0.67  0.86    24  0.14  0.30  0.55  0.77  0.21  0.35  0.46  0.58  0.71    36  0.14  0.26  0.37  0.66  0.21  0.33  0.43  0.54  0.65    48  0.12  0.25  0.34  0.54  0.20  0.31  0.41  0.51  0.61    60  0.13  0.24  0.32  0.42  0.19  0.30  0.39  0.48  0.57    120  0.09  0.16  0.21  0.28  0.16  0.24  0.31  0.37  0.44     Of course, entries that correspond to lags for which the measurement periods overlap mechanically exhibit high persistence. The results indicate that the calculation of beta using short measurement periods is very noisy. Moreoever, as the length of the measurement window increases, so does the persistence of beta calculated from daily return data. The measures based on monthly return data exhibit similar persistence patterns. In summary, the results indicate that the use of longer measurement periods results in more accurate measures of beta and that beta seems to be more accurately measured using daily return data.\n Portfolio Analysis Next, I turn to the relation of beta and stock returns. The fundamental prediction of the CAPM is that there is a positive relation between market beta and expected stock returns where the slope of this relation is the market risk premium. We would expect to find a positive cross-sectional association between beta and future excess returns. However, as I show below, contrary to this prediction, analyses fail to detect any strong relation or even exhibit a negative relation. This result is typically viewed as one of the most persistent empirical anomalies in all of empirical asset pricing.\nTo perform the portfolio analysis, I sort stocks into 10 portfolios ranked by the value of the beta estimate. The breakpoints for each portfolio are based on deciles constructed using only NYSE stocks, while the portfolios are formed using stocks from all exchanges. The rationale behind this procedure is that firms with small market capitalization make up a large chunk of the investable universe by number, but only a small portion of the total market value. Since NYSE stocks exhibit on average larger market capitalization, sorting on all firms would basically separate NYSE stocks from all others. Moreover, the CRSP data covers NYSE stocks since 1926, while AMEX and NASDAQ stocks enter later. Another reason for using NYSE breakpoints is consistency over an extended period of time.\nIn addition to the 10 decile portfolios, the function below also adds the 10-1 portfolio.\nconstruct_portfolios \u0026lt;- function(data, variable) { variable \u0026lt;- enquo(variable) # determine breakpoints data_nyse \u0026lt;- data %\u0026gt;% filter(exchange == \u0026quot;NYSE\u0026quot;) %\u0026gt;% select(date, !!variable) percentiles \u0026lt;- seq(0.1, 0.9, 0.1) percentiles_names \u0026lt;- map_chr(percentiles, ~paste0(\u0026quot;q\u0026quot;, .x*100)) percentiles_funs \u0026lt;- map(percentiles, ~partial(quantile, probs = .x, na.rm = TRUE)) %\u0026gt;% set_names(nm = percentiles_names) quantiles \u0026lt;- data_nyse %\u0026gt;% group_by(date) %\u0026gt;% summarize_at(vars(!!variable), lst(!!!percentiles_funs)) # sort all stocks into decile portfolios portfolios \u0026lt;- data %\u0026gt;% left_join(quantiles, by = \u0026quot;date\u0026quot;) %\u0026gt;% mutate(portfolio = case_when(!!variable \u0026gt; 0 \u0026amp; !!variable \u0026lt;= q10 ~ 1L, !!variable \u0026gt; q10 \u0026amp; !!variable \u0026lt;= q20 ~ 2L, !!variable \u0026gt; q20 \u0026amp; !!variable \u0026lt;= q30 ~ 3L, !!variable \u0026gt; q30 \u0026amp; !!variable \u0026lt;= q40 ~ 4L, !!variable \u0026gt; q40 \u0026amp; !!variable \u0026lt;= q50 ~ 5L, !!variable \u0026gt; q50 \u0026amp; !!variable \u0026lt;= q60 ~ 6L, !!variable \u0026gt; q60 \u0026amp; !!variable \u0026lt;= q70 ~ 7L, !!variable \u0026gt; q70 \u0026amp; !!variable \u0026lt;= q80 ~ 8L, !!variable \u0026gt; q80 \u0026amp; !!variable \u0026lt;= q90 ~ 9L, !!variable \u0026gt; q90 ~ 10L)) portfolios_ts \u0026lt;- portfolios %\u0026gt;% mutate(portfolio = as.character(portfolio)) %\u0026gt;% group_by(portfolio, date) %\u0026gt;% summarize(ret_ew = mean(ret_adj_excess_f1, na.rm = TRUE), ret_vw = weighted.mean(ret_adj_excess_f1, mktcap, na.rm = TRUE), ret_mkt = mean(mkt_ff_excess_f1, na.rm = TRUE)) %\u0026gt;% na.omit() %\u0026gt;% ungroup() # 10-1 portfolio portfolios_ts_101 \u0026lt;- portfolios_ts %\u0026gt;% filter(portfolio %in% c(\u0026quot;1\u0026quot;, \u0026quot;10\u0026quot;)) %\u0026gt;% pivot_wider(names_from = portfolio, values_from = c(ret_ew, ret_vw)) %\u0026gt;% mutate(ret_ew = ret_ew_10 - ret_ew_1, ret_vw = ret_vw_10 - ret_vw_1, portfolio = \u0026quot;10-1\u0026quot;) %\u0026gt;% select(portfolio, date, ret_ew, ret_vw, ret_mkt) # combine everything out \u0026lt;- bind_rows(portfolios_ts, portfolios_ts_101) %\u0026gt;% mutate(portfolio = factor(portfolio, levels = c(as.character(seq(1, 10, 1)), \u0026quot;10-1\u0026quot;))) return(out) } portfolios_beta_1y \u0026lt;- construct_portfolios(crsp, beta_1y) portfolios_beta_2y \u0026lt;- construct_portfolios(crsp, beta_2y) portfolios_beta_3y \u0026lt;- construct_portfolios(crsp, beta_3y) portfolios_beta_5y \u0026lt;- construct_portfolios(crsp, beta_5y) portfolios_beta_1m \u0026lt;- construct_portfolios(crsp, beta_1m) portfolios_beta_3m \u0026lt;- construct_portfolios(crsp, beta_3m) portfolios_beta_6m \u0026lt;- construct_portfolios(crsp, beta_6m) portfolios_beta_12m \u0026lt;- construct_portfolios(crsp, beta_12m) portfolios_beta_24m \u0026lt;- construct_portfolios(crsp, beta_24m) For illustrative purposes, I plot the cumulative value-weighted excess returns over time to get a first feeling for the data.\nportfolios_beta_12m %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% na.omit() %\u0026gt;% mutate(cum_ret = cumsum(ret_vw)) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(x = date, y = cum_ret, group = as.factor(portfolio))) + geom_line(aes(color = as.factor(portfolio), linetype = as.factor(portfolio))) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Log Return (in %)\u0026quot;, color = \u0026quot;Portfolio\u0026quot;, linetype = \u0026quot;Portfolio\u0026quot;) + scale_y_continuous(labels = scales::comma, breaks = scales::pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Given the portfolio returns, I want to evaluate whether the portfolio exhibit on average positive or negative excess returns. The following function estimates the mean excess return and CAPM alpha for each portfolio and computes corresponding Newey and West (1987) \\(t\\)-statistics (using six lags as common in the literature) testing the null hypothesis that the average portfolio excess return or CAPM alpha is zero.\nestimate_portfolio_returns \u0026lt;- function(data, ret) { # estimate average returns per portfolio average_ret \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(ret,\u0026quot; ~ 1\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6)))) %\u0026gt;% broom::tidy(model) %\u0026gt;% ungroup() %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # estimate capm alpha per portfolio average_capm_alpha \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(ret,\u0026quot; ~ 1 + ret_mkt\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6))[1])) %\u0026gt;% broom::tidy(model)%\u0026gt;% ungroup() %\u0026gt;% filter(term == \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # construct output table out \u0026lt;- rbind(average_ret, average_capm_alpha) colnames(out) \u0026lt;-c(as.character(seq(1, 10, 1)), \u0026quot;10-1\u0026quot;) rownames(out) \u0026lt;- c(\u0026quot;Excess Return\u0026quot;, \u0026quot;t-Stat\u0026quot; , \u0026quot;CAPM Alpha\u0026quot;, \u0026quot;t-Stat\u0026quot;) return(out) } Let us first apply the function to equal-weighted porfolios.\nrbind(estimate_portfolio_returns(portfolios_beta_1y, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_2y, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_3y, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_5y, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_1m, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_3m, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_6m, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_12m, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_24m, \u0026quot;ret_ew\u0026quot;)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Year (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 2 Years (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 5, 8) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Years (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 9, 12) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 5 Years (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 13, 16) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Month (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 17, 20) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Months (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 21, 24) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 6 Months (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 25, 28) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 12 Months (Daily Data)- Equal-Weighted Portfolio Returns\u0026quot;, 29, 32) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 24 Months (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 33, 36)    1  2  3  4  5  6  7  8  9  10  10-1     Estimation Period: 1 Year (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  1.01  0.76  0.82  0.92  0.95  0.93  1.03  1.02  1.03  0.91  0.34    t-Stat  3.93  3.12  3.37  3.59  3.58  3.28  3.51  3.28  3.20  2.47  1.42    CAPM Alpha  0.34  0.16  0.11  0.20  0.17  0.10  0.13  0.02  0.03  -0.23  -0.36    t-Stat  2.92  1.06  0.82  1.41  1.19  0.75  0.87  0.14  0.18  -1.13  -1.84   Estimation Period: 2 Years (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  0.71  0.76  0.87  0.97  0.88  1.03  1.04  1.01  0.97  0.91  0.28    t-Stat  2.91  3.12  3.58  3.77  3.29  3.54  3.39  3.20  2.92  2.42  1.29    CAPM Alpha  0.19  0.21  0.18  0.22  0.16  0.20  0.13  0.09  -0.04  -0.23  -0.45    t-Stat  1.15  1.47  1.30  1.63  1.07  1.29  0.81  0.59  -0.21  -1.08  -2.52   Estimation Period: 3 Years (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  0.74  0.84  0.91  0.92  0.91  0.99  0.94  1.05  0.96  0.88  0.09    t-Stat  3.29  3.48  3.75  3.49  3.33  3.33  3.13  3.29  2.83  2.47  0.40    CAPM Alpha  0.22  0.27  0.26  0.19  0.15  0.20  0.05  0.11  -0.01  -0.20  -0.56    t-Stat  1.37  1.92  1.92  1.46  1.05  1.29  0.34  0.67  -0.06  -0.93  -3.22   Estimation Period: 5 Years (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  0.79  0.83  0.88  0.91  1.00  0.98  0.97  1.06  0.97  0.79  0.04    t-Stat  3.60  3.61  3.56  3.57  3.61  3.34  3.18  3.27  2.74  2.26  0.16    CAPM Alpha  0.30  0.27  0.24  0.23  0.24  0.16  0.07  0.11  -0.07  -0.21  -0.62    t-Stat  1.90  1.92  1.83  1.52  1.75  1.05  0.44  0.62  -0.38  -0.97  -3.51   Estimation Period: 1 Month (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  0.82  0.87  0.95  0.87  0.97  0.93  0.91  0.88  0.85  0.65  -0.19    t-Stat  2.93  3.47  3.84  3.45  3.69  3.39  3.16  3.01  2.69  1.77  -0.87    CAPM Alpha  0.21  0.25  0.25  0.14  0.18  0.10  0.03  -0.06  -0.18  -0.53  -0.91    t-Stat  1.10  1.64  1.84  0.99  1.26  0.64  0.18  -0.45  -1.13  -2.78  -5.56   Estimation Period: 3 Months (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  0.86  0.90  0.90  0.89  0.89  0.92  0.92  0.89  0.81  0.66  -0.19    t-Stat  3.15  3.51  3.59  3.55  3.42  3.38  3.19  3.01  2.55  1.76  -0.94    CAPM Alpha  0.27  0.28  0.22  0.17  0.10  0.09  0.02  -0.05  -0.23  -0.53  -0.86    t-Stat  1.72  2.00  1.56  1.19  0.70  0.63  0.16  -0.29  -1.44  -2.74  -5.71   Estimation Period: 6 Months (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  0.94  0.90  0.89  0.88  0.94  0.96  0.92  0.87  0.82  0.64  -0.27    t-Stat  3.65  3.60  3.47  3.38  3.56  3.43  3.20  2.86  2.59  1.77  -1.37    CAPM Alpha  0.34  0.28  0.21  0.15  0.13  0.11  0.02  -0.09  -0.23  -0.56  -0.95    t-Stat  2.17  2.07  1.42  1.04  0.89  0.71  0.14  -0.57  -1.40  -2.95  -5.90   Estimation Period: 12 Months (Daily Data)- Equal-Weighted Portfolio Returns    Excess Return  0.93  0.93  0.89  0.95  0.92  0.94  0.93  0.86  0.85  0.63  -0.29    t-Stat  3.38  3.60  3.62  3.50  3.34  3.33  3.23  2.88  2.60  1.73  -1.42    CAPM Alpha  0.38  0.30  0.23  0.19  0.12  0.09  0.03  -0.08  -0.20  -0.57  -0.96    t-Stat  2.47  2.06  1.56  1.26  0.79  0.61  0.22  -0.49  -1.18  -3.05  -5.94   Estimation Period: 24 Months (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  0.94  0.91  1.00  0.94  0.95  0.93  0.94  0.87  0.89  0.62  -0.32    t-Stat  3.54  3.59  3.84  3.47  3.37  3.29  3.32  2.90  2.74  1.72  -1.60    CAPM Alpha  0.42  0.31  0.34  0.20  0.16  0.10  0.07  -0.06  -0.12  -0.55  -0.97    t-Stat  2.66  2.06  2.23  1.30  1.08  0.69  0.44  -0.35  -0.73  -2.95  -5.89     There is neither a clear pattern across the portfolios in average excess returns, nor does the 10-1 portfolio yield statistically significant excess returns. However, the CAPM alphas exhibit a decreasing pattern across portfolios and a statistically significant negative coefficient on the 10-1 portfolio for all beta estimates. This result is actually not surprising since the risk-adjustment using the CAPM model adjusts returns for the sensitivity to the market factor which is the same factor used to calculate beta. Stocks in the highest decile hence mechanically exhibit a higher sensitivity than stocks in the low decile. As a result, the 10-1 portfolio has a high sensitivity to the market portfolio. Since the market factor generates positive returns in the long run and the 10-1 portfolio has a positive sensitivity, the effect of the risk-adjustment is negative.\nNext, let us repeat the analysis using value-weighted returns for each portfolio.\nrbind(estimate_portfolio_returns(portfolios_beta_1y, \u0026quot;ret_vw\u0026quot;), estimate_portfolio_returns(portfolios_beta_2y, \u0026quot;ret_vw\u0026quot;), estimate_portfolio_returns(portfolios_beta_3y, \u0026quot;ret_vw\u0026quot;), estimate_portfolio_returns(portfolios_beta_5y, \u0026quot;ret_vw\u0026quot;), estimate_portfolio_returns(portfolios_beta_1m, \u0026quot;ret_vw\u0026quot;), estimate_portfolio_returns(portfolios_beta_3m, \u0026quot;ret_ew\u0026quot;), estimate_portfolio_returns(portfolios_beta_6m, \u0026quot;ret_vw\u0026quot;), estimate_portfolio_returns(portfolios_beta_12m, \u0026quot;ret_vw\u0026quot;), estimate_portfolio_returns(portfolios_beta_24m, \u0026quot;ret_vw\u0026quot;)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Year (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 2 Years (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 5, 8) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Years (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 9, 12) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 5 Years (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 13, 16) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Month (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 17, 20) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Months (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 21, 24) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 6 Months (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 25, 28) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 12 Months (Daily Data)- Value-Weighted Portfolio Returns\u0026quot;, 29, 32) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 24 Months (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 33, 36)    1  2  3  4  5  6  7  8  9  10  10-1     Estimation Period: 1 Year (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.53  0.46  0.64  0.68  0.70  0.62  0.75  0.77  0.79  0.63  0.50    t-Stat  2.90  2.54  3.57  3.65  3.20  2.85  3.34  3.13  2.83  2.01  1.88    CAPM Alpha  0.00  -0.05  0.05  0.06  0.00  -0.12  -0.06  -0.14  -0.16  -0.48  -0.31    t-Stat  0.00  -0.37  0.36  0.49  0.01  -0.88  -0.50  -1.14  -1.11  -3.04  -1.49   Estimation Period: 2 Years (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.51  0.50  0.60  0.70  0.62  0.76  0.67  0.75  0.65  0.75  0.34    t-Stat  2.73  2.89  3.19  3.50  2.87  3.42  2.80  2.88  2.35  2.26  1.50    CAPM Alpha  0.05  0.04  0.02  0.04  -0.03  0.01  -0.16  -0.10  -0.30  -0.39  -0.46    t-Stat  0.35  0.29  0.12  0.36  -0.26  0.03  -1.17  -0.79  -2.26  -2.30  -2.57   Estimation Period: 3 Years (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.48  0.55  0.57  0.67  0.63  0.67  0.64  0.71  0.66  0.67  0.16    t-Stat  2.63  3.04  3.19  3.34  2.80  2.97  2.62  2.66  2.27  2.09  0.73    CAPM Alpha  0.05  0.05  0.02  0.04  -0.05  -0.04  -0.17  -0.16  -0.28  -0.40  -0.54    t-Stat  0.32  0.36  0.18  0.31  -0.37  -0.27  -1.29  -1.21  -1.95  -2.36  -3.05   Estimation Period: 5 Years (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.50  0.54  0.58  0.63  0.69  0.68  0.67  0.71  0.66  0.57  0.13    t-Stat  2.90  3.02  3.16  3.25  3.04  2.91  2.73  2.65  2.23  1.82  0.58    CAPM Alpha  0.09  0.05  0.02  0.03  0.01  -0.07  -0.14  -0.18  -0.32  -0.41  -0.57    t-Stat  0.61  0.44  0.20  0.22  0.08  -0.45  -1.09  -1.33  -2.08  -2.54  -3.22   Estimation Period: 1 Month (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.52  0.53  0.55  0.59  0.58  0.73  0.61  0.78  0.68  0.47  0.01    t-Stat  2.35  3.01  3.21  3.27  3.15  3.62  2.87  3.45  2.58  1.52  0.03    CAPM Alpha  0.03  0.10  0.05  0.03  -0.04  0.07  -0.11  -0.01  -0.21  -0.63  -0.73    t-Stat  0.20  0.73  0.45  0.26  -0.37  0.55  -0.82  -0.07  -1.56  -4.18  -3.91   Estimation Period: 3 Months (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.86  0.90  0.90  0.89  0.89  0.92  0.92  0.89  0.81  0.66  -0.19    t-Stat  3.15  3.51  3.59  3.55  3.42  3.38  3.19  3.01  2.55  1.76  -0.94    CAPM Alpha  0.27  0.28  0.22  0.17  0.10  0.09  0.02  -0.05  -0.23  -0.53  -0.86    t-Stat  1.72  2.00  1.56  1.19  0.70  0.63  0.16  -0.29  -1.44  -2.74  -5.71   Estimation Period: 6 Months (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.50  0.54  0.51  0.62  0.55  0.66  0.69  0.68  0.65  0.60  0.16    t-Stat  2.95  3.00  2.93  3.43  2.96  3.46  3.24  2.94  2.47  1.91  0.68    CAPM Alpha  0.12  0.09  0.03  0.09  -0.06  0.01  -0.02  -0.09  -0.26  -0.50  -0.64    t-Stat  0.89  0.77  0.22  0.69  -0.50  0.10  -0.13  -0.78  -1.91  -3.37  -3.89   Estimation Period: 12 Months (Daily Data)- Value-Weighted Portfolio Returns    Excess Return  0.49  0.53  0.59  0.61  0.56  0.70  0.68  0.69  0.62  0.56  0.09    t-Stat  2.79  3.04  3.40  3.29  3.04  3.46  3.22  2.97  2.33  1.79  0.42    CAPM Alpha  0.13  0.08  0.10  0.05  -0.02  0.06  -0.01  -0.08  -0.27  -0.53  -0.66    t-Stat  0.96  0.69  0.91  0.46  -0.19  0.49  -0.11  -0.61  -2.04  -3.59  -4.04   Estimation Period: 24 Months (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.49  0.55  0.59  0.62  0.59  0.62  0.70  0.64  0.58  0.52  0.03    t-Stat  2.80  3.11  3.34  3.26  3.03  3.12  3.30  2.70  2.19  1.64  0.14    CAPM Alpha  0.15  0.12  0.10  0.07  0.01  0.00  0.02  -0.11  -0.29  -0.54  -0.69    t-Stat  1.15  0.94  0.85  0.61  0.08  0.01  0.17  -0.79  -2.13  -3.44  -4.16     The results show that the negative relation between beta and future stock returns detected using equal-weighted portfolios is weaker using value-weighted portfolios. Taken together, the portfolio analyses provide evidence that contradicts the predictions of the CAPM. According to the CAPM, returns should increase across the decile portfolios and risk-adjusted returns should be statistically indistinguishable from zero.\n Regression Analysis As a last step, I analyze the relation between beta and stock returns using Fama and MacBeth (1973) regression analysis. Each month, I perform a cross-sectional regression of one-month-ahead excess stock returns on the given measure of beta. Time-series averages over these cross-sectional regressions then provide the desired results.\nFollowing the literature, I winsorize each measure of beta on a monthly basis to mitigate the impact of outliers or estimation errors. I define the following function to winsorize any given variable.\nwinsorize \u0026lt;- function(x, cut = 0.005){ cut_point_top \u0026lt;- quantile(x, 1 - cut, na.rm = TRUE) cut_point_bottom \u0026lt;- quantile(x, cut, na.rm = TRUE) i \u0026lt;- which(x \u0026gt;= cut_point_top) x[i] \u0026lt;- cut_point_top j \u0026lt;- which(x \u0026lt;= cut_point_bottom) x[j] \u0026lt;- cut_point_bottom return(x) } The next function performs Fama-MacBeth regressions for a given variable and returns the time-series average of monthly regression coefficients and corresponding \\(t\\)-statistics.\nfama_macbeth_regression \u0026lt;- function(data, variable, cut = 0.005) { variable \u0026lt;- enquo(variable) # prepare and winsorize data data_nested \u0026lt;- data %\u0026gt;% filter(!is.na(ret_adj_f1) \u0026amp; !is.na(!!variable)) %\u0026gt;% group_by(date) %\u0026gt;% mutate(beta = winsorize(!!variable, cut = cut)) %\u0026gt;% nest() # perform cross-sectional regressions for each month cross_sectional_regs \u0026lt;- data_nested %\u0026gt;% mutate(model = map(data, ~lm(ret_adj_f1 ~ beta, data = .x))) %\u0026gt;% mutate(tidy = map(model, broom::tidy), glance = map(model, broom::glance), n = map(model, stats::nobs)) # extract average coefficient estimates fama_macbeth_coefs \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% summarize(coefficient = mean(estimate)) # compute newey-west standard errors of average coefficient estimates newey_west_std_errors \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% arrange(date) %\u0026gt;% group_modify(~enframe(sqrt(diag(sandwich::NeweyWest(lm(estimate ~ 1, data = .x), lag = 6))))) %\u0026gt;% select(term, nw_std_error = value) # put coefficient estimates and standard errors together and compute t-statistics fama_macbeth_coefs \u0026lt;- fama_macbeth_coefs %\u0026gt;% left_join(newey_west_std_errors, by = \u0026quot;term\u0026quot;) %\u0026gt;% mutate(nw_t_stat = coefficient / nw_std_error) %\u0026gt;% select(term, coefficient, nw_t_stat) %\u0026gt;% pivot_longer(cols = c(coefficient, nw_t_stat), names_to = \u0026quot;statistic\u0026quot;) %\u0026gt;% mutate(statistic = paste(term, statistic, sep = \u0026quot; \u0026quot;)) %\u0026gt;% select(-term) # extract average r-squared and average number of observations fama_macbeth_stats \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(c(glance, n)) %\u0026gt;% ungroup() %\u0026gt;% summarize(adj_r_squared = mean(adj.r.squared), n = mean(n)) %\u0026gt;% pivot_longer(c(adj_r_squared, n), names_to = \u0026quot;statistic\u0026quot;) # combine desired output and return results out \u0026lt;- rbind(fama_macbeth_coefs, fama_macbeth_stats) return(out) } The following table provides the regression results for each beta measure.\nbeta_1y_reg \u0026lt;- fama_macbeth_regression(crsp, beta_1y) %\u0026gt;% rename(beta_1y = value) beta_2y_reg \u0026lt;- fama_macbeth_regression(crsp, beta_2y) %\u0026gt;% rename(beta_2y = value) beta_3y_reg \u0026lt;- fama_macbeth_regression(crsp, beta_3y) %\u0026gt;% rename(beta_3y = value) beta_5y_reg \u0026lt;- fama_macbeth_regression(crsp, beta_5y) %\u0026gt;% rename(beta_5y = value) beta_1m_reg \u0026lt;- fama_macbeth_regression(crsp, beta_1m) %\u0026gt;% rename(beta_1m = value) beta_3m_reg \u0026lt;- fama_macbeth_regression(crsp, beta_3m) %\u0026gt;% rename(beta_3m = value) beta_6m_reg \u0026lt;- fama_macbeth_regression(crsp, beta_6m) %\u0026gt;% rename(beta_6m = value) beta_12m_reg \u0026lt;- fama_macbeth_regression(crsp, beta_12m) %\u0026gt;% rename(beta_12m = value) beta_24m_reg \u0026lt;- fama_macbeth_regression(crsp, beta_24m) %\u0026gt;% rename(beta_24m = value) regression_table \u0026lt;- beta_1y_reg %\u0026gt;% left_join(beta_2y_reg, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% left_join(beta_3y_reg, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% left_join(beta_5y_reg, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% left_join(beta_1m_reg, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% left_join(beta_3m_reg, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% left_join(beta_6m_reg, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% left_join(beta_12m_reg, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% left_join(beta_24m_reg, by = \u0026quot;statistic\u0026quot;) regression_table$statistic \u0026lt;- c(\u0026quot;intercept\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;adj. R-squared\u0026quot;, \u0026quot;n\u0026quot;) regression_table %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   statistic  beta_1y  beta_2y  beta_3y  beta_5y  beta_1m  beta_3m  beta_6m  beta_12m  beta_24m      intercept  1.09  1.07  1.08  1.06  1.20  1.22  1.28  1.32  1.36    t-stat  4.43  4.67  4.90  5.01  4.61  4.73  4.95  5.13  5.30    beta  0.06  0.07  0.08  0.09  -0.10  -0.12  -0.17  -0.21  -0.23    t-stat  0.81  0.77  0.69  0.74  -1.57  -1.57  -1.82  -1.99  -2.00    adj. R-squared  0.02  0.02  0.02  0.03  0.02  0.02  0.03  0.03  0.03    n  2940.43  2747.58  2679.78  2687.25  3132.94  3095.20  3045.97  2949.61  2715.26     For all measures based on monthly data, I do not find any statistically significant relation between beta and stock returns, while the measures based on daily returns do exhibit a statistically significant relation for longer estimation periods. Moreover, the intercepts are positive and statistically significant across all beta measures, although, controlling for the effect of beta, the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM.\nFinally, it is worth noting that the average adjusted R-squared ranges only from 2 to 3 percent. Low values of R-squared are common in empirical asset pricing studies. The main reason put forward in the literature is that predicting stock returns is that realized stock returns are just a very noisy measure for expected stock returns (e.g., Elton, 1999).\n ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581552000,"objectID":"059dc45735d97d9d59b75c7129c49aa0","permalink":"/post/asset-pricing/beta/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/asset-pricing/beta/","section":"post","summary":"On the estimation on market betas and their relation to stock returns","tags":["Academic"],"title":"Tidy Asset Pricing - Part II: Beta and Stock Returns","type":"post"},{"authors":["[Francesco D'Acunto](http://www.francescodacunto.com/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Michael Weber](http://faculty.chicagobooth.edu/michael.weber/)"],"categories":null,"content":"","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"a7c6029d7166af6cc3091628ba3cdea0","permalink":"/publication/perceived-precautionary-savings/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/publication/perceived-precautionary-savings/","section":"publication","summary":"We study the spending response of first-time borrowers to an overdraft facility and elicit their preferences, beliefs, and motives through a FinTech application. Users increase their spending permanently, lower their savings rate, and reallocate spending from non-discretionary to discretionary goods. Interestingly, liquid users react more than others but do not tap into negative deposits. The credit line acts as a form of insurance. These results are not fully consistent with models of financial constraints, buffer stock models, or present-bias preferences. We label this channel perceived precautionary savings motives: Liquid users behave as if they faced strong precautionary savings motives even though no observables, including elicited preferences and beliefs, suggest they should.","tags":null,"title":"Perceived Precautionary Savings Motives: Evidence from FinTech","type":"publication"},{"authors":["[Nikolaus Hautsch](https://homepage.univie.ac.at/nikolaus.hautsch/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Stefan Voigt](https://voigtstefan.github.io/)"],"categories":null,"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"7a755c2d16372cc64eb49a7f21d349a3","permalink":"/publication/stochastic-latency/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/publication/stochastic-latency/","section":"publication","summary":"Distributed ledger technologies replace trusted clearing counterparties and security depositories with time-consuming consensus protocols to record the transfer of ownership. This settlement latency exposes cross-market arbitrageurs to price risk. We theoretically derive arbitrage bounds that increase with expected latency, latency uncertainty, volatility and risk aversion. Using Bitcoin orderbook and network data, we estimate arbitrage bounds of on average 121 basis points, explaining 91% of the observed cross-market price differences. Consistent with our theory, periods of high latency-implied price risk exhibit large price differences, while asset flows chase arbitrage opportunities. Blockchain-based settlement thus introduces a non-trivial friction that impedes arbitrage activity.","tags":null,"title":"Trust Takes Time: Limits to Arbitrage in Blockchain-Based Markets","type":"publication"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" Seasonalcharts.de claims that stock indices exhibit persistent seasonality that may be exploited through an appropriate trading strategy. As part of a job application, I had to replicate the seasonal pattern for the DAX and then test whether this pattern entails a profitable trading strategy. To sum up, I indeed find that a trading strategy that holds the index only over a specific season outperforms the market significantly, but these results might be driven by a few outliers. The text below references an opinion and is for information purposes only. I do not intend to provide any investment advice.\nThe code is structured in a way that allows for a straight-forward replication of the methodoloty for other indices. The document was knitted in RStudio using the following packages:\nlibrary(tidyverse) # for grammar library(scales) # for pretty breaks in figures library(tidyquant) # for data download library(knitr) # for html knitting library(kableExtra) # for nicer tables Data Preparation First, download data from yahoo finance using the tidyquant package. Note that the DAX was officially launched in July 1988, so this is where our sample starts.\ndax_raw \u0026lt;- tq_get(\u0026quot;^GDAXI\u0026quot;, get = \u0026quot;stock.prices\u0026quot;, from = \u0026quot;1988-07-01\u0026quot;, to = \u0026quot;2019-11-30\u0026quot;)  Then, select only date and the adjusted price (i.e., closing price after adjustments for all applicable splits and dividend distributions) as the relevant variables and compute summary statistics to check for missing or weird values. The results are virtually the same if I use unadjusted closing prices.\ndax \u0026lt;- dax_raw %\u0026gt;% select(date, price = adjusted) summary(dax) ## date price ## Min. :1988-07-01 Min. : 1150 ## 1st Qu.:1996-04-04 1st Qu.: 2512 ## Median :2004-01-08 Median : 5286 ## Mean :2004-02-06 Mean : 5657 ## 3rd Qu.:2011-12-03 3rd Qu.: 7445 ## Max. :2019-11-29 Max. :13560 ## NA\u0026#39;s :160 I replace the missing values by the last available index value.\ndax \u0026lt;- dax %\u0026gt;% arrange(date) %\u0026gt;% fill(price, .direction = \u0026quot;down\u0026quot;) As a last immediate plausibility check, I plot the dax over the whole sample period.\ndax %\u0026gt;% ggplot(aes(x = date, y = price)) + geom_line() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Adjusted Price\u0026quot;) + scale_x_date(expand = c(0, 0), breaks = \u0026quot;5 years\u0026quot;) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() The data seems to be good to go, so let us turn to some graphical evidence for seasonality.\n Graphical Evidence for Seasonality To replicate the average seasonal pattern that we can find here, I construct an index of cumulative returns that starts at 100 in each year of my sample. To do so, I nest the data by year (i.e., create a list-column of returns for each full year in the sample).\ndax_nested \u0026lt;- dax %\u0026gt;% filter(date \u0026gt;= \u0026quot;1988-01-01\u0026quot; \u0026amp; date \u0026lt;= \u0026quot;2018-12-31\u0026quot;) %\u0026gt;% mutate(year = year(date)) %\u0026gt;% group_by(year) %\u0026gt;% nest() Next, I define the function that creates the seasonal pattern, given the sample of returns of a specific year, apply the function to each year and compute summary statistics for each trading day across all years in my sample.\nget_seasonality \u0026lt;- function(data) { data %\u0026gt;% arrange(date) %\u0026gt;% mutate(trading_day = 1:n(), # use number of trading days as index ret = (log(price) - lag(log(price)))*100, ret = if_else(is.na(ret), 0, ret), cum_ret = 100 + cumsum(ret)) %\u0026gt;% select(trading_day, ret, cum_ret) } dax_seasonality \u0026lt;- dax_nested %\u0026gt;% mutate(seasonality = map(data, get_seasonality)) %\u0026gt;% select(year, seasonality) %\u0026gt;% unnest(cols = c(seasonality)) %\u0026gt;% ungroup() dax_seasonality_summary \u0026lt;- dax_seasonality %\u0026gt;% group_by(trading_day) %\u0026gt;% summarize(mean = mean(cum_ret), q05 = quantile(cum_ret, 0.05), q95 = quantile(cum_ret, 0.95)) The latter data frame contains the average cumulative return and corresponding quantiles for each trading day of a year in my sample. Let us now take a look at the average pattern across trading days.\ndax_seasonality_summary %\u0026gt;% ggplot(aes(x = trading_day)) + geom_line(aes(y = mean)) + labs(x = \u0026quot;Trading Days\u0026quot;, y = \u0026quot;Cumulative Returns (in %)\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() While it is unclear what exactly Seasonalcharts plots on their website, the above pattern seems to be fairly consistent with their claimed seasonality. However, even if the pattern seems to be on average there, let us add confidence intervals before we think about trading strategies.\ndax_seasonality_summary %\u0026gt;% ggplot(aes(x = trading_day)) + geom_ribbon(aes(ymin = q05, ymax = q95), alpha = 0.25) + geom_line(aes(y = mean)) + labs(x = \u0026quot;Trading Days\u0026quot;, y = \u0026quot;Cumulative Returns (in %)\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() Naturally, these confidence intervals mechanically increase over the course of a year as each index starts at 100. Even with confidence intervals, the pattern is still visible. Given the graphical evidence, let us now turn to the analysis of a trading strategy that aims to exploit this seasonal pattern.\n Trading Strategy The main idea of Seasonalcharts is to implement the strategy proposed by Jacobsen and Bouman (2002) and Jacobsen and Zhan (2018) which they label ‘The Halloween Indicator’ (or ‘Sell in May Effect’). The main finding of these papers is that stock indices returns seem significantly lower duing the May-October period than during the remainder of the year. The corresponding trading strategy holds an index during the months November-April, but holds the risk-free asset in the May-October period.\nTo replicate their approach (and avoid noise in the daily data), I focus on monthly returns from now on.\ndax_monthly \u0026lt;- dax %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% group_by(year, month) %\u0026gt;% slice(which.max(date)) %\u0026gt;% ungroup() %\u0026gt;% arrange(date) %\u0026gt;% mutate(ret = (log(price) - lag(log(price))) * 100) %\u0026gt;% na.omit() nrow(dax_monthly) ## [1] 376 As usual in empirical asset pricing, we do not care about raw returns, but returns in excess of the risk-free asset. I simply add the European risk free rate from the Fama-French data library as the corresponding reference point. Of course, one could use other measures for the risk-free rate, but the impact on the results won’t be substantial.\ntemp \u0026lt;- tempfile() download.file(\u0026quot;https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Europe_3_Factors_CSV.zip\u0026quot;, temp) unzip(temp, \u0026quot;Europe_3_Factors.csv\u0026quot;) rf_raw \u0026lt;- read_csv(\u0026quot;Europe_3_Factors.csv\u0026quot;, skip = 3) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## `Mkt-RF` = col_character(), ## SMB = col_character(), ## HML = col_character(), ## RF = col_character() ## ) rf \u0026lt;- rf_raw %\u0026gt;% mutate(date = ymd(paste0(X1, \u0026quot;01\u0026quot;)), year = year(date), month = month(date), rf = as.numeric(RF)) %\u0026gt;% filter(date \u0026lt;= \u0026quot;2019-09-01\u0026quot;) %\u0026gt;% select(year, month, rf) dax_monthly \u0026lt;- dax_monthly %\u0026gt;% left_join(rf, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% mutate(excess_ret = ret - rf) %\u0026gt;% na.omit() nrow(dax_monthly) # lose a few obs b/c ff data starts in july 1990 ## [1] 351 Next, let us examine the average excess returns per month in our sample. Before we do that, I define a new (factor) column that ensures that R sorts months correctly in the following analyses.\ndax_monthly$month_factor \u0026lt;- factor(x = months(dax_monthly$date), levels = c(\u0026quot;January\u0026quot;, \u0026quot;February\u0026quot;, \u0026quot;March\u0026quot;, \u0026quot;April\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;June\u0026quot;, \u0026quot;July\u0026quot;, \u0026quot;August\u0026quot;, \u0026quot;September\u0026quot;, \u0026quot;October\u0026quot;, \u0026quot;November\u0026quot;, \u0026quot;December\u0026quot;)) Let us now take a look at the average excess returns per month. I also add the standard deviation, 5% and 95% quantiles, and t-statistic of a t-test of the null hypothesis that average returns are zero in a given month.\ndax_monthly %\u0026gt;% group_by(`Month` = month_factor) %\u0026gt;% summarize(Mean = mean(excess_ret), SD = sd(excess_ret), Q05 = quantile(excess_ret, 0.05), Q95 = quantile(excess_ret, 0.95), `t-Statistic` = sqrt(n()) * mean(excess_ret) / sd(excess_ret) ) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   Month  Mean  SD  Q05  Q95  t-Statistic      January  0.50  5.93  -9.88  8.46  0.45    February  0.63  5.49  -8.72  8.16  0.62    March  0.38  4.46  -6.05  7.41  0.46    April  2.78  5.68  -4.24  13.02  2.63    May  0.11  4.20  -5.90  5.73  0.14    June  -0.69  4.55  -8.13  5.48  -0.82    July  0.95  6.38  -7.59  9.23  0.82    August  -3.07  7.05  -19.05  2.87  -2.38    September  -2.94  8.46  -19.82  5.66  -1.90    October  2.18  6.49  -9.12  11.57  1.80    November  1.79  4.27  -4.86  6.70  2.25    December  1.43  5.43  -6.27  8.53  1.41     August and September seem to usually exhibit negative excess returns with an average of about -3% (statistically significant) over all years, while April and November are the only months that tend to exhibit statistically significant positive excess returns. For a graphical illustration of the above table, I complement it with boxplots for each month. The takeaway is essentially the same, but we can see that August and September exhibit a couple of outliers that might considerably drive the results.\ndax_monthly %\u0026gt;% ggplot(aes(x = month_factor, y = excess_ret)) + geom_boxplot() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Monthly Excess Return (in %)\u0026quot;) + theme_classic() Let us proceed to test for the presence of statistically significant excess returns due to seasonal patterns. In the above table, I only test for significance for each month seperately. To test for positive returns in a joint model, I regress the monthly excess returns on month indicators. Note that I always adjust the standard errors to be heteroskedasticity robust.\nsummary(lm(excess_ret ~ month_factor, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ month_factor, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.5324 -3.0783 0.7171 3.7981 16.4934 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.4970 1.0852 0.458 0.6473 ## month_factorFebruary 0.1380 1.5347 0.090 0.9284 ## month_factorMarch -0.1179 1.5347 -0.077 0.9388 ## month_factorApril 2.2835 1.5347 1.488 0.1377 ## month_factorMay -0.3909 1.5347 -0.255 0.7991 ## month_factorJune -1.1862 1.5347 -0.773 0.4401 ## month_factorJuly 0.4564 1.5218 0.300 0.7645 ## month_factorAugust -3.5626 1.5218 -2.341 0.0198 * ## month_factorSeptember -3.4373 1.5218 -2.259 0.0245 * ## month_factorOctober 1.6789 1.5347 1.094 0.2747 ## month_factorNovember 1.2913 1.5347 0.841 0.4007 ## month_factorDecember 0.9298 1.5347 0.606 0.5450 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.844 on 339 degrees of freedom ## Multiple R-squared: 0.08555, Adjusted R-squared: 0.05588 ## F-statistic: 2.883 on 11 and 339 DF, p-value: 0.001222 Seems like August and September have on average indeed lower returns than January (which is the omitted reference point in this regression). Note that the size of the coefficients from the regression are the same as in the table above (i.e., constant plus coefficient). Next, I follow Jacobsen and Bouman (2002) and simply regression excess returns on dummies that indicate specific seasons, i.e., I estimate the model \\[ y_t=\\alpha + \\beta D_t + \\epsilon_t, \\] where \\(D_t\\) is a dummy variable equal to one for the months in a specific season and zero otherwise. I consider both the ‘Halloween’ season (where the dummy is one for November-April) and a `Seasonality’ season which only excludes July-September (and the dummy is one for October-June). If \\(D_t\\) is statistically significant and positive for the corresponding season, then I take this as evidence for the presence of seasonality effects.\nhalloween_months \u0026lt;- c(11, 12, 1, 2, 3, 4) seasonality_months \u0026lt;- c(10, 11, 12, 1, 2, 3, 4, 5, 6) dax_monthly \u0026lt;- dax_monthly %\u0026gt;% mutate(halloween = if_else(month %in% halloween_months, 1L, 0L), seasonality = if_else(month %in% seasonality_months, 1L, 0L)) The first model considers the `Halloween’ effect:\nsummary(lm(excess_ret ~ halloween, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ halloween, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.8772 -2.9011 0.5344 3.8357 18.0227 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.5954 0.4473 -1.331 0.18402 ## halloween 1.8465 0.6353 2.906 0.00389 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.951 on 349 degrees of freedom ## Multiple R-squared: 0.02363, Adjusted R-squared: 0.02083 ## F-statistic: 8.447 on 1 and 349 DF, p-value: 0.00389 I indeed find evidence that excess returns are higher during the months November-April relative to the remaining months. Let us take this spiel even further by adding even more months:\nsummary(lm(excess_ret ~ seasonality, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ seasonality, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.7885 -3.0178 0.5807 4.0105 18.2628 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.6842 0.6226 -2.705 0.007159 ** ## seasonality 2.6952 0.7220 3.733 0.000221 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.906 on 349 degrees of freedom ## Multiple R-squared: 0.0384, Adjusted R-squared: 0.03564 ## F-statistic: 13.94 on 1 and 349 DF, p-value: 0.0002207 The effect seems to be even stronger if I also include October, May and June.\nAs a last step, let us compare five different strategies: (i) buy and hold the index over the full year, (ii) go long in the index outside of the Halloween season and otherwise hold the risk-free asset, (iii) go long in the index outside of the Halloween season and otherwise short the index, (iv) buy the index outside of the extended seasonality period and otherwise invest in the risk-free asset, and (v) go long in the index outside of the extended seasonality period and short the index otherwise. Below I compare the returns of the three different strategies on an annual basis:\ndax_monthly \u0026lt;- dax_monthly %\u0026gt;% mutate(excess_ret_halloween = if_else(halloween == 1, ret, rf), excess_ret_halloween_short = if_else(halloween == 1, ret, -ret), excess_ret_seasonality = if_else(seasonality == 1, ret, rf), excess_ret_seasonality_short = if_else(seasonality == 1, ret, -ret)) dax_monthly %\u0026gt;% group_by(year) %\u0026gt;% summarize(`Buy and Hold` = sum(excess_ret), `Seasonality` = sum(excess_ret_seasonality), `Seasonality-Short` = sum(excess_ret_seasonality_short), `Halloween` = sum(excess_ret_halloween), `Halloween-Short` = sum(excess_ret_halloween_short)) %\u0026gt;% pivot_longer(-year, names_to = \u0026quot;strategy\u0026quot;, values_to = \u0026quot;excess_ret\u0026quot;) %\u0026gt;% ggplot(aes(x = year, group = strategy)) + geom_col(aes(y = excess_ret, fill = strategy), position = \u0026quot;dodge\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Annual Excess Return (in %)\u0026quot;, fill = \u0026quot;Strategy\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + theme_classic() The ‘Halloween’ and ‘Seasonality’ strategies seem to outperform the ‘Buy and Hold’ strategy in most of the years with the ‘Seasonality’ typically outperforming ‘Halloween’. The strategies that short the index rather than holding the risk free assets also outperform their counterparts. Plotting the overall cumulative excess return of the five strategies confirms these conjectures.\ndax_monthly %\u0026gt;% arrange(date) %\u0026gt;% mutate(`Buy and Hold` = 100 + cumsum(excess_ret), `Seasonality` = 100 + cumsum(excess_ret_seasonality), `Seasonality-Short` = 100 + cumsum(excess_ret_seasonality_short), `Halloween` = 100 + cumsum(excess_ret_halloween), `Halloween-Short` = 100 + cumsum(excess_ret_halloween_short)) %\u0026gt;% select(date, `Buy and Hold`, `Seasonality`, `Seasonality-Short`, `Halloween`, `Halloween-Short`) %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;strategy\u0026quot;, values_to = \u0026quot;cum_excess_ret\u0026quot;) %\u0026gt;% ggplot(aes(x = date)) + geom_line(aes(y = cum_excess_ret, color = strategy)) + scale_x_date(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Excess Return (in %)\u0026quot;, color = \u0026quot;Strategy\u0026quot;) + theme_classic() Which of these strategies might constitute a better investment opportunity? For a very simple assessment, let us compute the corresponding Sharpe ratios. Note that I annualize Sharpe ratios by multiplying them with \\(\\sqrt{12}\\) which strictly speaking only works under IID distributed returns (which is typically unlikely to be the case), but which suffices for the purpose of this note.\nsharpe_ratio \u0026lt;- function(x) { sqrt(12) * mean(x) / sd(x) } dax_monthly %\u0026gt;% arrange(date) %\u0026gt;% summarize(`Buy and Hold` = sharpe_ratio(excess_ret), `Seasonality` = sharpe_ratio(excess_ret_seasonality), `Seasonality-Short` = sharpe_ratio(excess_ret_seasonality_short), `Halloween` = sharpe_ratio(excess_ret_halloween), `Halloween-Short` = sharpe_ratio(excess_ret_halloween_short)) ## # A tibble: 1 x 5 ## `Buy and Hold` Seasonality `Seasonality-Short` Halloween `Halloween-Shor~ ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.184 0.737 0.754 0.774 0.530 The ‘Seasonality-Short’ strategy delivers the highest cumulative excess return in my sample period, but the ‘Halloween’ strategy exhibits a slightly higher Sharpe ratio than the others and thus constitutes a better investment opportunity.\n ","date":1580428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580428800,"objectID":"aa6c77ee45e495b5b8c47c28f78082e9","permalink":"/post/asset-pricing/dax-seasonality/","publishdate":"2020-01-31T00:00:00Z","relpermalink":"/post/asset-pricing/dax-seasonality/","section":"post","summary":"A quick evaluation of seasonal patterns in an equity index","tags":["Academic"],"title":"Seasonality in the DAX","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" As a newbie to the empirical asset pricing literature, I found it quite hard to fully grasp how results come about solely relying on the data descriptions in the corresponding papers. Fortunately, Bali, Engle and Murray provide an excellent textbook that not only provides a comprehensive overview of the empirical research on the cross-section of expected returns, but also offers detailed description of common practices in sample construction and methodology. In this series of notes, I replicate the main results of their textbook in R. Moreover, I try to implement all analyses in a tidy manner using mainly packages beloning to the tidyverse family since it’s fun and fairly transparent.\nIn this first note I start out with the raw monthly CRSP data, compute some descriptive statistics and construct the main sample I use in later analyses. In the next note, I’ll use the constructed sample to investigate the size effect. So let us start with the list of packages that I use throughout the note.\nlibrary(tidyverse) # for grammar library(lubridate) # for working with dates library(scales) # for nicer axes of figures I just downloaded the full monthly CRSP sample from WRDS. The full monthly CRSP sample from December 1925 until December 2018 has about 1.18GB. After reading in the data, I convert all column names to lower case out of a personal preference.\ncrspa_msf \u0026lt;- read_csv(\u0026quot;raw/crspa_msf.csv\u0026quot;) colnames(crspa_msf) \u0026lt;- tolower(colnames(crspa_msf)) nrow(crspa_msf) ## [1] 4568082 Next, I make sure that all relevant variables are correctly parsed.\ncrsp \u0026lt;- crspa_msf %\u0026gt;% transmute(permno = as.integer(permno), # security identifier date = ymd(date), # month identifier ret = as.numeric(ret) * 100, # return (convert to percent) shrout = as.numeric(shrout), # shares outstanding (in thousands) altprc = as.numeric(altprc), # last traded price in a month exchcd = as.integer(exchcd), # exchange code shrcd = as.integer(shrcd), # share code siccd = as.integer(siccd), # industry code dlret = as.numeric(dlret) * 100, # delisting return (converted to percent) dlstcd = as.integer(dlstcd) # delisting code ) nrow(crsp) ## [1] 4568082 As common in the literature, I focus on US-based common stocks in the CRSP database. US-based common stocks are identified with share codes 10 and 11, where the first digit pins down ordinary common shares and the second digit indicates that the secudity has not been further defined (0) or does not need to be further defined (1). I refer to the original CRSP documentation on a full description of the meaning of this distinction.\ncrsp \u0026lt;- crsp %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% select(-shrcd) nrow(crsp) ## [1] 3630251 I apply the following two checks to all data sets I work with. I make sure to keep only distinct observations and I check that there is only one observation by date for each security. The main reason is that I want to work with a nice panel of unique security-date combinations where no pair appears more than once in the data.\ncrsp \u0026lt;- crsp %\u0026gt;% distinct() nrow(crsp) ## [1] 3601076 crsp %\u0026gt;% group_by(permno, date) %\u0026gt;% filter(n() \u0026gt; 1) %\u0026gt;% nrow() == 0 ## [1] TRUE If the last part of the code returns FALSE, I usually investigate where the failure comes from (typically related to reporting errors). Fortunately, everything looks good in the CRSP data.\nFollowing Bali, Engle and Murray, I compute the market capitalization as the absolute value of the product of shares outstanding and the price of the stock as of the end of the last trading day of a given month. I have to take the absolute value since altprc is the negative of average of bid and ask from last traded pricefor which the data is available if there is no last traded price. Since the shares outstanding are reported in thousands of shares, I divide by 1000 such that the resulting measure is in millions of dollars.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(mktcap = abs(shrout * altprc) / 1000, # in millions of dollars mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap))  As a first glance at the data, let us look at the stock exchange composition of the CRSP sample. Stocks listed on NYSDE, AMEX, and NASDAQ are indicated with values of 1 or 31, 2 or 32, and 3 or 33, respectively, in the exchange code filed. I collect stocks traded on all other exchange (e.g., Arca Stock Market, Boston Stock Exchange, Chicago Stock Exchange, etc.) in a separate category.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(exchange = case_when(exchcd %in% c(1, 31) ~ \u0026quot;NYSE\u0026quot;, exchcd %in% c(2, 32) ~ \u0026quot;AMEX\u0026quot;, exchcd %in% c(3, 33) ~ \u0026quot;NASDAQ\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot;)) The figure below plots the number of stocks per exchange. NYSE has the longest history in the data, but NASDAQ exhibits a considerable large number of stocks. Interestingly, the number of stocks on AMEX is decreasing steadily over the last couple of decades. By the end of 2018, there are 2,221 stocks on NASDAQ, 1,274 on NYSE, 167 on AMEX and 3 belonging to the other category.\ncrsp %\u0026gt;% count(exchange, date) %\u0026gt;% ggplot(aes(x = date, y = n, group = exchange)) + geom_line(aes(color = exchange, linetype = exchange)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Securities\u0026quot;, color = \u0026quot;Exchange\u0026quot;, linetype = \u0026quot;Exchange\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Next, let us look at the market capitalization per exchange. To do so, I adjust the market capitalization values for inflation using the using the consumer price index from the US Bureau of Labor Statistics. All values are in end of 2018 dollars for intertemporal comparability. NYSE has by far the largest market capilization, followed by NASDAQ. At the end of 2018, total market value on NYSE was 15,649 billion and 9,386 billion on NASDAQ. AMEX only plays a minor role by now with a total market capitalization of 39 billion at the end of 2018, while other exchanges only make up 15 billion.\ncpi_raw \u0026lt;- read_csv(\u0026quot;raw/CPIAUCNS.csv\u0026quot;) cpi \u0026lt;- cpi_raw %\u0026gt;% filter(DATE \u0026lt;= max(crsp$date)) %\u0026gt;% transmute(year = year(DATE), month = month(DATE), cpi = CPIAUCNS) cpi$cpi \u0026lt;- cpi$cpi / cpi$cpi[nrow(cpi)] crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(cpi, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) crsp %\u0026gt;% group_by(exchange, date) %\u0026gt;% summarize(mktcap = sum(mktcap / cpi, na.rm = TRUE) / 1000) %\u0026gt;% ggplot(aes(x = date, y = mktcap, group = exchange)) + geom_line(aes(color = exchange, linetype = exchange)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Total Market Value (Billions of Dec 2018 Dollars)\u0026quot;, color = \u0026quot;Exchange\u0026quot;, linetype = \u0026quot;Exchange\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Now let us turn to the industry composition of the CRSP sample. I follow the below convention on defining industries using the standard industry classification (SIC) codes.\n## define industry labels crsp \u0026lt;- crsp %\u0026gt;% mutate(industry = case_when(siccd \u0026gt;= 1 \u0026amp; siccd \u0026lt;= 999 ~ \u0026quot;Agriculture\u0026quot;, siccd \u0026gt;= 1000 \u0026amp; siccd \u0026lt;= 1499 ~ \u0026quot;Mining\u0026quot;, siccd \u0026gt;= 1500 \u0026amp; siccd \u0026lt;= 1799 ~ \u0026quot;Construction\u0026quot;, siccd \u0026gt;= 2000 \u0026amp; siccd \u0026lt;= 3999 ~ \u0026quot;Manufacturing\u0026quot;, siccd \u0026gt;= 4000 \u0026amp; siccd \u0026lt;= 4999 ~ \u0026quot;Transportation\u0026quot;, siccd \u0026gt;= 5000 \u0026amp; siccd \u0026lt;= 5199 ~ \u0026quot;Wholesale\u0026quot;, siccd \u0026gt;= 5200 \u0026amp; siccd \u0026lt;= 5999 ~ \u0026quot;Retail\u0026quot;, siccd \u0026gt;= 6000 \u0026amp; siccd \u0026lt;= 6799 ~ \u0026quot;Finance\u0026quot;, siccd \u0026gt;= 7000 \u0026amp; siccd \u0026lt;= 8999 ~ \u0026quot;Services\u0026quot;, siccd \u0026gt;= 9000 \u0026amp; siccd \u0026lt;= 9999 ~ \u0026quot;Public\u0026quot;, TRUE ~ \u0026quot;Missing\u0026quot;)) The figure below plots the number of stocks in the sample in each of the SIC industries. Most of the stocks are apparently in Manufacturing albeit the number peaked somewhere in the 90ies. The number of public administration stocks seems to the the only category on the rise in recent years.\n## plot number of stocks by industry crsp %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% count(industry, date) %\u0026gt;% ggplot(aes(x = date, y = n, group = industry)) + geom_line(aes(color = industry, linetype = industry)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Securities\u0026quot;, color = \u0026quot;Industry\u0026quot;, linetype = \u0026quot;Industry\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Let us repeat the excercise by computing the market value of all stocks belonging to the respective industries. All values are again in terms of billions of end of 2018 dollars. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Towards the end of the sample financial firms and services begin to make up a substantial portion of the market value.\ncrsp %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% group_by(industry, date) %\u0026gt;% summarize(mktcap = sum(mktcap / cpi, na.rm = TRUE) / 1000) %\u0026gt;% ggplot(aes(x = date, y = mktcap, group = industry)) + geom_line(aes(color = industry, linetype = industry)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Total Market Value (Billions of Dec 2018 Dollars)\u0026quot;, color = \u0026quot;Industry\u0026quot;, linetype = \u0026quot;Industry\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Next, I turn to the calculation of returns. Monthly stock returns are in the ret field (i.e., the return realized by holding the stock from its last trade in the previous month to its last trade in the current month). However, we have to adjust for delistings which are fortunately recorded by CRSP. The adjustment procedure below follows Shumway (1997). Apparently, the adjustment kicks in on both tails of the return distribution as we can see from the summary statistics.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(ret_adj = case_when(!is.na(dlret) ~ dlret, is.na(dlret) \u0026amp; !is.na(dlstcd) ~ -100, is.na(dlret) \u0026amp; (dlstcd %in% c(500, 520, 580, 584) | (dlstcd \u0026gt;= 551 \u0026amp; dlstcd \u0026lt;= 574)) ~ -30, TRUE ~ ret)) %\u0026gt;% select(-c(dlret, dlstcd)) summary(crsp %\u0026gt;% select(ret, ret_adj))  ## ret ret_adj ## Min. : -99.36 Min. :-100.00 ## 1st Qu.: -6.41 1st Qu.: -6.43 ## Median : 0.00 Median : 0.00 ## Mean : 1.17 Mean : 1.03 ## 3rd Qu.: 6.98 3rd Qu.: 6.94 ## Max. :2400.00 Max. :4700.00 ## NA\u0026#39;s :121850 NA\u0026#39;s :103306 As a main reference point for each stock return, we want to consider the return on the market. According to the Capital Asset Pricing Model (CAPM), cross-sectional variation in expected asset returns should be a function of the covariance between the return of the asset and the return on the market portfolio. The value-weighted portfolio of all US-based common stocks in the CRSP database is one of the main proxies used in the empirical asset pricing literature which I also get from WRDS.\ncrspa_si \u0026lt;- read_csv(\u0026quot;raw/crspa_si.csv\u0026quot;) colnames(crspa_si) \u0026lt;- tolower(colnames(crspa_si)) crsp_market \u0026lt;- crspa_si %\u0026gt;% transmute(year = year(ymd(date)), month = month(ymd(date)), vwretd = vwretd * 100, ewretd = ewretd * 100) crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(crsp_market, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) In most cases, it makes sense to use stock returns in excess of the risk-free security over the same period. I take the monthly risk-free rate from Ken French’s data library and also add the other Fama-French factors while we are already at it. In particular, we also add another measure for the return on the market portfolio provided by Fama and French. The main difference to the CRSP measure lies in the fact that Fama and French exclude firms that are not based in the US, closed-end funds and other securities that are not common stocks. I’ll discuss the other factors in later notes where I’ll try to replicate them.\nfactors_ff \u0026lt;- read_csv(\u0026quot;raw/F-F_Research_Data_Factors.csv\u0026quot;, skip = 3) factors_ff \u0026lt;- factors_ff %\u0026gt;% transmute(date = ymd(paste0(X1, \u0026quot;01\u0026quot;)), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF), smb_ff = as.numeric(SMB), hml_ff = as.numeric(HML),) %\u0026gt;% filter(date \u0026lt;= max(crsp$date)) %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% select(-date) crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(factors_ff, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) As a final descriptive statistic, I plot the cumulative returns of our market measures (i.e., equal-weighted, value-weighted and the Fama-French market factor). Clearly, the value-weighted CRSP index and the Fama-French market factor are highly correlated, while the equal-weighted CRSP index stands out due its high returns apparently driven by smaller stocks.\nmarket \u0026lt;- crsp %\u0026gt;% distinct(date, vwretd, ewretd, mkt_ff) %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;portfolio\u0026quot;, values_to = \u0026quot;return\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% mutate(cum_return = cumsum(return)) %\u0026gt;% ungroup() market %\u0026gt;% mutate(portfolio = case_when(portfolio == \u0026quot;vwretd\u0026quot; ~ \u0026quot;CRSP (Value-Weighted)\u0026quot;, portfolio == \u0026quot;ewretd\u0026quot; ~ \u0026quot;CRSP (Equal-Weighted)\u0026quot;, portfolio == \u0026quot;mkt_ff\u0026quot; ~ \u0026quot;MKT (Fama-French)\u0026quot;)) %\u0026gt;% ggplot(aes(x = date, y = cum_return, group = portfolio)) + geom_line(aes(color = portfolio, linetype = portfolio)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Log Return (in %)\u0026quot;, color = \u0026quot;Portfolio\u0026quot;, linetype = \u0026quot;Portfolio\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() The focus of empirical asset pricing is to examine the ability of different stock characteristics to predict the cross section of future stock returns. This is why I add the (excess) returns for the subsequent month of each stock to the sample. Note that I use beginning of month dates to ensure correct matching of dates.\ncrsp_f1 \u0026lt;- crsp %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;) %m-% months(1), ret_excess = ret - rf_ff, ret_adj_excess = ret_adj - rf_ff, mkt_ff_excess = mkt_ff - rf_ff) %\u0026gt;% select(permno, date_start, ret_f1 = ret, ret_adj_f1 = ret_adj, ret_excess_f1 = ret_excess, ret_adj_excess_f1 = ret_adj_excess, mkt_ff_excess_f1 = mkt_ff_excess) crsp \u0026lt;- crsp %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;)) %\u0026gt;% left_join(crsp_f1, by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) To wrap up this data set, I arrange everything by security and date before saving the sample and provide some summary statistics of all the variables that were used above. I end up with a monthly CRSP file that has about 700 MB.\ncrsp \u0026lt;- crsp %\u0026gt;% arrange(permno, date) write_rds(crsp, \u0026quot;data/crsp.rds\u0026quot;) summary(crsp) ## permno date ret shrout ## Min. :10000 Min. :1925-12-31 Min. : -99.36 Min. : 0 ## 1st Qu.:22103 1st Qu.:1977-01-31 1st Qu.: -6.41 1st Qu.: 2141 ## Median :49403 Median :1990-08-31 Median : 0.00 Median : 6615 ## Mean :50216 Mean :1987-10-29 Mean : 1.17 Mean : 39672 ## 3rd Qu.:78394 3rd Qu.:2001-10-31 3rd Qu.: 6.98 3rd Qu.: 22510 ## Max. :93436 Max. :2018-12-31 Max. :2400.00 Max. :29206400 ## NA\u0026#39;s :121850 NA\u0026#39;s :3707 ## altprc exchcd siccd mktcap ## Min. : -1832.5 Min. :-2.000 Min. : 0 Min. : 0.0 ## 1st Qu.: 1.6 1st Qu.: 1.000 1st Qu.:3079 1st Qu.: 15.6 ## Median : 10.7 Median : 2.000 Median :3841 Median : 65.2 ## Mean : 25.7 Mean : 2.106 Mean :4493 Mean : 1409.7 ## 3rd Qu.: 25.0 3rd Qu.: 3.000 3rd Qu.:6030 3rd Qu.: 345.2 ## Max. :326000.0 Max. :33.000 Max. :9999 Max. :1099436.1 ## NA\u0026#39;s :71504 NA\u0026#39;s :2380 NA\u0026#39;s :71641 ## exchange cpi industry ret_adj ## Length:3601076 Min. :0.05015 Length:3601076 Min. :-100.00 ## Class :character 1st Qu.:0.23285 Class :character 1st Qu.: -6.43 ## Mode :character Median :0.52382 Mode :character Median : 0.00 ## Mean :0.50056 Mean : 1.03 ## 3rd Qu.:0.70731 3rd Qu.: 6.94 ## Max. :1.00658 Max. :4700.00 ## NA\u0026#39;s :103306 ## vwretd ewretd rf_ff mkt_ff ## Min. :-29.173 Min. :-31.275 Min. :-0.060 Min. :-29.100 ## 1st Qu.: -1.761 1st Qu.: -1.987 1st Qu.: 0.150 1st Qu.: -1.750 ## Median : 1.264 Median : 1.402 Median : 0.390 Median : 1.270 ## Mean : 0.915 Mean : 1.116 Mean : 0.372 Mean : 0.933 ## 3rd Qu.: 3.893 3rd Qu.: 4.231 3rd Qu.: 0.510 3rd Qu.: 3.920 ## Max. : 39.414 Max. : 66.594 Max. : 1.350 Max. : 38.950 ## NA\u0026#39;s :497 NA\u0026#39;s :497 NA\u0026#39;s :3627 NA\u0026#39;s :3627 ## smb_ff hml_ff date_start ## Min. :-16.860 Min. :-13.280 Min. :1925-12-01 ## 1st Qu.: -1.660 1st Qu.: -1.230 1st Qu.:1977-01-01 ## Median : 0.040 Median : 0.260 Median :1990-08-01 ## Mean : 0.146 Mean : 0.369 Mean :1987-09-30 ## 3rd Qu.: 1.860 3rd Qu.: 1.770 3rd Qu.:2001-10-01 ## Max. : 36.700 Max. : 35.460 Max. :2018-12-01 ## NA\u0026#39;s :3627 NA\u0026#39;s :3627 ## ret_f1 ret_adj_f1 ret_excess_f1 ret_adj_excess_f1 ## Min. : -99.36 Min. :-100.00 Min. : -99.52 Min. :-101.31 ## 1st Qu.: -6.41 1st Qu.: -6.43 1st Qu.: -6.79 1st Qu.: -6.81 ## Median : 0.00 Median : 0.00 Median : -0.37 Median : -0.37 ## Mean : 1.17 Mean : 1.03 Mean : 0.80 Mean : 0.66 ## 3rd Qu.: 6.98 3rd Qu.: 6.94 3rd Qu.: 6.64 3rd Qu.: 6.61 ## Max. :2400.00 Max. :4700.00 Max. :2399.66 Max. :4699.24 ## NA\u0026#39;s :122004 NA\u0026#39;s :103476 NA\u0026#39;s :125006 NA\u0026#39;s :106484 ## mkt_ff_excess_f1 ## Min. :-29.130 ## 1st Qu.: -2.040 ## Median : 0.930 ## Mean : 0.558 ## 3rd Qu.: 3.520 ## Max. : 38.850 ## NA\u0026#39;s :28949 Finally, here is the code chunk for setting up the daily return data from CRSP. Note that the raw daily CRSP sample has about 30GB, whereas the final sample is about 3 GB. I just use a scientific cluster with plenty of memory to run this part.\n## read in data crspa_dsf \u0026lt;- read_csv(\u0026quot;raw/crspa_dsf.csv\u0026quot;) colnames(crspa_dsf) \u0026lt;- tolower(colnames(crspa_dsf)) nrow(crspa_dsf) # keep only common stocks and relevant columns crsp_daily \u0026lt;- crspa_dsf %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% transmute(permno = as.integer(permno), date = ymd(date), ret = as.numeric(ret) * 100 ) # keep only distinct observations crsp_daily \u0026lt;- crsp_daily %\u0026gt;% distinct() # add fama french factors (https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) factors_ff \u0026lt;- read_csv(\u0026quot;raw/F-F_Research_Data_Factors_daily.csv\u0026quot;, skip = 3) factors_ff \u0026lt;- factors_ff %\u0026gt;% transmute(date = ymd(X1), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF)) %\u0026gt;% filter(date \u0026lt;= max(crsp_daily$date)) crsp_daily \u0026lt;- crsp_daily %\u0026gt;% left_join(factors_ff, by = \u0026quot;date\u0026quot;) # drop rows with missing values crsp_daily \u0026lt;- crsp_daily %\u0026gt;% na.omit() # set reference date crsp_daily \u0026lt;- crsp_daily %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;)) # export sample write_rds(crsp_daily, \u0026quot;data/crsp_daily.rds\u0026quot;)  ","date":1579824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579824000,"objectID":"26840e564e4a98d43b53d838e4ba623d","permalink":"/post/asset-pricing/crsp-sample/","publishdate":"2020-01-24T00:00:00Z","relpermalink":"/post/asset-pricing/crsp-sample/","section":"post","summary":"On the preparation of monthly return data for empirical research on the cross-section of expected returns","tags":["Academic"],"title":"Tidy Asset Pricing - Part I: The CRSP Sample in R","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":"In Hautsch, et al. (2019), we focus on the market for Bitcoin (BTC) against US Dollar (USD) and estimate the corresponding spotvolatilities for 16 different exchanges. In this note, I replicate the estimation procedure for the spotvolatilities of other cryptocurrencies, namely Ether (ETH), Litecoin (LTC), and Ripple (XRP) against USD.\nBefore I proceed to the results, I briefly sketch the estimation procedure. To estimate the spot volatility, I follow the approach of Kristensen (2010). For each market $i$, asset $j$, and minute $t$, I estimate $\\sigma_{i,j,t}^2$ by $$\\widehat{{\\sigma}_{i,j,t}}^2(h_T) = \\sum\\limits_{l=1}^\\infty K\\left(l - t, h_T\\right)\\left(b_{i,j,l} - b_{i,j,l-1}\\right)^2,$$ where $K\\left(l - t, h_T\\right)$ is a one-sided Gaussian kernel smoother with bandwidth $h_T$ and $b_{i,j,l}$ corresponds to the quoted bid price on market $i$ for asset $j$ at time $l$. The choice of the bandwidth $h_T$ involves a trade-off between the variance and the bias of the estimator. Using too many observations introduces a bias if the volatility is time-varying, whereas shrinking the estimation window through a lower bandwidth increases the variance of the estimator. Kristensen (2010) hence proposes to choose $h_T$ such that information on day $T-1$ is used for the estimation on day $T$, i.e., the bandwith on any day is the result of minimizing the integrated squared error of estimates on the previous day.\nI employ the data that I collected together with my colleague Stefan Voigt. Since January 2018, we gather the first 25 levels of all open buy and sell orders of a couple of exchanges on a minute level using our R package. The spotvolatility estimation procedure from above, however, only rests on the first level of bids. To ensure comparability across asset pairs, I focus on exchanges in our sample that feature trading of all four asset pairs (Binance, Bbitfinex, Bitstamp, Cex, Gate, Kraken, Lykke, Poloniex, xBTCe).\nNow, let us take a look at the resulting volatility estimates for all asset pairs. The app below shows the average daily spotvolatility estimate across all exchanges (solid lines) and corresponding range of average daily spotvolatility estimates (shaded areas). Overall, all four asset pairs exhibit a strong correlation over the last two years. Feel free to play around with the app by comparing asset pairs seperately or focussing on subperiods.\n Shiny App Iframe    \n","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"fa8ef2201f05612cb5e54b1b492fbc4f","permalink":"/post/settlement-latency/spotvolas/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/post/settlement-latency/spotvolas/","section":"post","summary":"An application of the spotvolatility estimator from [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) to other cryptocurrencies","tags":["Academic"],"title":"Volatility in Cryptocurrency Markets","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":" In its essence, the distributed ledger technology (DLT) is a digital record-keeping system that allows for the verification, updating and storage of transfers of ownership without the need for a designated third party. It relies on a single ledger that is distributed among many different parties who are incentivized to ensure a truthful representation of the transaction history. Nakamoto first popularized the idea of DLTs in a financial context with the Bitcoin protocol and the underlying concept of blockchain. Nowadays, however, Bitcoin is just one of several hundred applications that use the blockchain technology, while other forms of DLTs, in particular directed acyclic graphs (DAG), are actively explored as well. In the following, we first describe the building blocks of DLT before we turn to a more detailed discussion of blockchains and DAGs.\nFundamentals of Distributed Ledgers DLT solves the fundamental problems that arise in the context of digital transfer of ownership. Transactions are pieces of information that agents authorize to be sent to other agents. A record-keeping system has to ensure that transactions are signed and recorded in the correct order. In principle, a single authority could verify signatures and consistency of transactions, but it would be prone to failures. As a result, it might be desirable to distribute this type of information to a system with multiple machines that can sustain the failure of single units. A fault-tolerant design would enable a system to continue its operation, even when one or several units stop working.\nTo achieve this goal, DLT essentially combines two fundamental concepts. First, a distributed ledger is based on asymmetrical cryptography that enables digital signatures of transactions. On the one hand, the sender of a transaction wants to be the sole owner of the signature that allows the transfer of assets from her private wealth. On the other hand, a record-keeping system requires information about the identities of the parties involved. Cryptographic algorithms ensure that any private keys are only known to their owners, while public keys may be disseminated widely. That is, everybody can check whether a private key is valid, but nobody can back out the private key from public information.1\nSecond, a distributed ledger is conceptually a distributed system which checks whether transactions should be in the system and in which order. In a distributed system, many machines are connected through a network and ensure that the system keeps operating even when some machines fail or try to mess up the system. For instance, if the sender of a transaction is also a potential validator, then she has an incentive for dishonest behavior, such as double-spending or revoking transactions. Individual machines have to reach some form of consensus about actual transaction histories. This consensus can be achieved through different network structures such as blockchain or DAG.\nBlockchain In the context of blockchain, the typical solution to the consensus problem involves competition among potential validators for the right to append information to the ledger.2 The most common consensus protocol, Proof-of-Work (PoW), involves solving a computationally expensive problem where the winner gets the right to update the ledger and typically receives a reward. This particular form of DLT is called blockchain since transactions are not verified individually, but rather appended to the ledger in blocks. Validators bundle transactions that wait for verification and try to solve the problem. However, the system\u0026rsquo;s protocol limits the number of transactions that can be included in a single block. This limit leads to a queue of unconfirmed transactions and validators are free to choose the transactions they try to append to the blockchain. Average verification times thus not only depend on the number of unconfirmed transactions, but also on the fee associated with a transaction, as validators find it more attractive to include transactions with high fees in their blocks.\nThe computationally difficult problem typically relies on cryptographic hash functions, which map an input of arbitrary size to output of fixed size and cannot be inverted. In the Bitcoin network, validators bundle the information of several transactions and a reference to the current state of the blockchain and plug the data into a hash function. The hash function converts this input into a sequence of characters and numbers of certain length. The system\u0026rsquo;s protocol then requires that the output starts with a certain number of zeros. The probability of calculating a hash that starts with many zeros is very low and to generate a new hash, validators include a random number called nonce that can lead to a very different output. The difficulty of the problem is then determined by the number of leading zeros validators have to find. Depending on total available computational power, the system regularly adjusts the target to achieve an average of 10 minutes between two consecutive blocks.\nWhile validators in a PoW system utilize substantial computational resources to win the competition for block generation, validators might also be randomly chosen based on their wealth. In the so called Proof-of-Stake (PoS) protocol, validators stake their tokens to be able to create blocks. The higher a validator\u0026rsquo;s stake, the higher are the chances of creating the next block. After successfully appending a new block, the validator receives transaction fees just as in the case of PoW. If the validator submits an incorrect block or is offline during a staking period, then she is penalized and (at least partly) loses her stake. The penalty might either arise explicitly through a deduction of funds from the stake or implicitly as dishonest behavior creates a feedback on the value of the stake. In particular, if a validator appends to the blockchain in a way that perpetuates disagreement, then she imposes a cost upon all users of the particular blockchain. Such behavior lowers the value of the whole network and is also reflected in a lower valuation of the misbehaving validator\u0026rsquo;s stake. The endogenous feedback between validators\u0026rsquo; behavior and the value of their stakes incentives them to eventually reach consensus.\nOther consensus protocols combine features of PoW and PoS. For instance, delegated Proof-of-Stake (DPoS) relies both on stakeholders, who elect validators and have voting rights proportional to their stake, and validators, who exert effort to append information to the ledger. The reputation of validators determines their chance for reelection, while stakeholders have incentives to select truthful validators.\nDirected Acyclic Graphs While blockchains record transactions in blocks, DAGs store information in single transactions. More specifically, any transaction represents a node in a graph (i.e., a set of vertices connected by edges) and each new transaction confirms at least one previous transaction, depending on the configuration of the underlying protocol. The longer the branch on which a transaction is based, the more certain is its validity. Intuitively, only once a transaction is broadcasted sufficiently throughout the network, it is verified. DAGs thus hinge on a steady flow of new transactions that enter the network to verify and reference old transactions. The connections between transactions are directed (i.e., the edges in the form of confirmations are one way) and the whole graph is acyclic (i.e., it is impossible to traverse the entire graph starting from a single edge). Given a high number of transactions, DAG ledgers scale better and can achieve consensus faster than blockchains which rely on fixed block sizes and limited verification rates.\nSettlement Latency in Distributed Systems A distributed system features settlement latency in transaction verification, as it is ex-ante unclear how long it takes until validators achieve consensus or to broadcast that consensus through the network. For PoW, latency depends on the time it takes for validators to find a solution to the computationally expensive problem. In the Bitcoin protocol, for instance, validators append a new block on average every 10 minutes, while the process takes about 20 seconds in the Ethereum protocol. For PoS, latency depends on how long disagreement on the correct order of transactions persists. For both blockchain-based protocols, the information still needs to be distributed to all other nodes in the network, possibly facing technological limitations that prevent instant percolation. Technological limits are also particularly relevant for DAGs which rely on a large number of nodes that verify transactions and distribute information through the network. Overall, any distributed system that refrains from using designated third-parties bearing the counterparty risk associated with transactions thus features settlement latency.\n The most simple illustrative example for asymmetric cryptography is the multiplication of prime numbers. One can easily multiply two prime numbers (private key) to get a large number (public key), but it can be difficult to infer the initial set of numbers from the product.\r^ The problem is more severe in a permissionless blockchain where anybody can access and potentially update the blockchain. Other variants, where only few institutions or individuals are entitled to direct access to the blockchain, so-called permissioned blockchains, limit the problem to few players.\r^   ","date":1575331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575331200,"objectID":"385edcc2abfadfe0677ae2eb7a7d1acd","permalink":"/post/settlement-latency/fundamentals/","publishdate":"2019-12-03T00:00:00Z","relpermalink":"/post/settlement-latency/fundamentals/","section":"post","summary":"A companion piece to [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) on how latency enters blockchain-based settlement","tags":["Academic"],"title":"Distributed Ledger Technology and Settlement Latency","type":"post"},{"authors":["[Alexander Mürmann](https://www.wu.ac.at/finance/people/faculty/muermann/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1510790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510790400,"objectID":"56c84d7d745a1cc19c70899dab3e5554","permalink":"/publication/fishing-with-pearls/","publishdate":"2017-11-16T00:00:00Z","relpermalink":"/publication/fishing-with-pearls/","section":"publication","summary":"We provide novel evidence of banks establishing lending relationships with prestigious firms to signal their quality and attract future business. Using survey data on firm-level prestige, we show that lenders compete more intensely for prestigious borrowers and offer lower upfront fees to initiate lending relationships with prestigious firms. We also find that banks expand their lending after winning prestigious clients. Prestigious firms benefit from these relations as they face lower costs of borrowing even though prestige has no predictive power for credit risk. Our results are robust to matched sample analyses and a regression discontinuity design.","tags":null,"title":"Fishing with Pearls: The Value of Lending Relationships with Prestigious Firms","type":"publication"}]