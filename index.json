[{"authors":["admin"],"categories":null,"content":"I am the Head of Business Intelligence and Data Science at the social trading platform wikifolio.\nBefore joining wikifolio, I graduated from the Vienna Graduate School of Finance where my research focused on the economics of technological innovations in the financial sector. One of my papers shows how blockchain-based settlement introduces limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the introduction of overdraft facilities through a mobile banking app. In my solo-authored paper, I investigate the potential of crowdfunding mechanisms to elicit demand information and improve the screening of viable projects under different valuation structures.\nIn my spare time, I replicate asset pricing approaches or build visualizations for arbitrary economic stories which all eventually end up on this page.\n","date":1619481600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1619481600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://christophscheuch.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am the Head of Business Intelligence and Data Science at the social trading platform wikifolio.\nBefore joining wikifolio, I graduated from the Vienna Graduate School of Finance where my research focused on the economics of technological innovations in the financial sector. One of my papers shows how blockchain-based settlement introduces limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the introduction of overdraft facilities through a mobile banking app.","tags":null,"title":"Christoph Scheuch","type":"authors"},{"authors":["Christoph Scheuch"],"categories":["R"],"content":" library(tidyverse) library(lubridate) library(glue) library(scales) library(tidyquant) library(rvest) library(readxl) I recently had an interesting discussion about the impact of the gold standard on food price stability. The gold standard has existed in the US since 1792 in some form or another, with only a few interruptions. The US government nationalized gold in 1934 and kept the value of the US dollar tied to gold reserves. The Bretton Woods system effectively maintained the gold standard until 1971. Afterwards, gold prices fluctuated freely and were not tied to the US dollar anymore.\nSo how did the end of Bretton woods impact food price volatility? Moreover, how would have food prices have evolved in a hypothetical world where we had to pay in ounces of gold rather than fiat currency? These questions call for a simple visualization that compares the evolution of food price indexes with different numeraires. In this post, I consider 3 different types of numeraires: fiat currency (US dollar), ounces of gold, and stocks (units of the S\u0026amp;P 500 total return index) - where the latter serves as a benchmark asset that was not literally tied to the other two. I focus on the US simply because of data availability.\nGold prices First, I need to get annual gold prices in US dollar. Fortunately, FRED offers daily gold fixing prices in London Bullion Markets since 1968 where I can easily get the last price for each year.\ntbl.GoldFRED \u0026lt;- tq_get(\u0026quot;GOLDPMGBD228NLBM\u0026quot;, get=\u0026quot;economic.data\u0026quot;, from = \u0026quot;1968-01-01\u0026quot;,to = \u0026quot;2021-02-01\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(Year = year(date)) %\u0026gt;% arrange(desc(date)) %\u0026gt;% filter(row_number()==1) %\u0026gt;% ungroup() %\u0026gt;% select(Year, GoldUSD = price) %\u0026gt;% arrange(Year) Gold prices before 1968 can be scrapped from onlygold.com. The website contains annual gold prices since 1792, but unfortunately the information is contained in a horrible table format. The code below extracts the annual gold prices and arranges them in a desireable long format.\ntbl.GoldScrapped \u0026lt;- read_html(\u0026quot;https://onlygold.com/gold-prices/historical-gold-prices/\u0026quot;) %\u0026gt;% html_table(fill = TRUE) %\u0026gt;% .[[1]] %\u0026gt;% as_tibble() tbl.GoldFREDPre \u0026lt;- bind_rows( tbl.GoldScrapped %\u0026gt;% transmute(Year = as.integer(X1), GoldUSD = parse_number(X2)), tbl.GoldScrapped %\u0026gt;% transmute(Year = as.integer(X4), GoldUSD = parse_number(X5)), tbl.GoldScrapped[, c(7, 8)] %\u0026gt;% transmute(Year = as.integer(X7), GoldUSD = parse_number(X8))) %\u0026gt;% na.omit() %\u0026gt;% arrange(Year) %\u0026gt;% filter(Year \u0026gt;= 1928) So let’s put the two data sources together and move on to the next time series.\ntbl.GoldAnnual \u0026lt;- bind_rows( tbl.GoldFRED, filter(tbl.GoldFREDPre, Year \u0026lt; min(tbl.GoldFRED$Year)) ) %\u0026gt;% arrange(Year)  S\u0026amp;P total return index As the second alternative numeraire, I want to have the S\u0026amp;P 500 total return index which I fortunately already reconstructed historically in this blog post. Importantly, the total return index tracks both capital gains and cash distributions and represents the overall performance of the S\u0026amp;P 500 to shareholders.\ntbl.SP500Monthly \u0026lt;- read_rds(\u0026quot;data/SP500Monthly.rds\u0026quot;) tbl.SP500Annual \u0026lt;- tbl.SP500Monthly %\u0026gt;% group_by(Year = year(Month)) %\u0026gt;% arrange(Month) %\u0026gt;% summarize(SP500PerformanceUSD = last(TotalReturnIndex)) %\u0026gt;% filter(Year \u0026gt;= 1928)   Food prices Lastly, I get the US-dollar-denominated annual food price index based on consumer price for all urban consumers from FRED.\ntbl.FoodAnnual \u0026lt;- tq_get(\u0026quot;CPIUFDNS\u0026quot;, get = \u0026quot;economic.data\u0026quot;, from = \u0026quot;1928-01-01\u0026quot;,to = \u0026quot;2020-12-31\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(Year = year(date)) %\u0026gt;% arrange(desc(date)) %\u0026gt;% filter(row_number()==1) %\u0026gt;% ungroup() %\u0026gt;% select(Year, FoodIndexUSD = price) %\u0026gt;% arrange(Year)  During and after Bretton Woods Now, I combine the different data into a single table and compute food indexes with gold and S\u0026amp;P total return index as numeraires, respectively. To ensure comparability I normalize each time series by its Value in the year 1928. I hence get different time series that all start in 1928.\ntbl.DataAnnual \u0026lt;- tbl.FoodAnnual %\u0026gt;% full_join(tbl.SP500Annual, by = \u0026quot;Year\u0026quot;) %\u0026gt;% full_join(tbl.GoldAnnual, by = \u0026quot;Year\u0026quot;) %\u0026gt;% mutate(FoodIndexGold = FoodIndexUSD / GoldUSD, FoodIndexSP500 = FoodIndexUSD / SP500PerformanceUSD) %\u0026gt;% arrange(Year) %\u0026gt;% pivot_longer(cols = -Year, names_to = \u0026quot;Type\u0026quot;, values_to = \u0026quot;Value\u0026quot;) %\u0026gt;% group_by(Type) %\u0026gt;% arrange(Year) %\u0026gt;% mutate(Value = Value / first(Value) * 100, Change = Value / lag(Value) -1) %\u0026gt;% ungroup()  Let us look the evolution of the different annual indexes with US dollar as the denominator. I use log scale, otherwise I wouldn’t be able to see anything other than the astounding cumulative performance of the S\u0026amp;P 500 total return index. The figure also shows that gold price volatility increased substantially after Bretton Woods, while food prices grew rather linearly.\nfig.IndexesEvolution \u0026lt;- tbl.DataAnnual %\u0026gt;% filter(Type %in% c(\u0026quot;FoodIndexUSD\u0026quot;, \u0026quot;SP500PerformanceUSD\u0026quot;, \u0026quot;GoldUSD\u0026quot;)) %\u0026gt;% mutate(Type = case_when(Type == \u0026quot;FoodIndexUSD\u0026quot; ~ \u0026quot;Food\u0026quot;, Type == \u0026quot;GoldUSD\u0026quot; ~ \u0026quot;Gold\u0026quot;, Type == \u0026quot;SP500PerformanceUSD\u0026quot; ~ \u0026quot;S\u0026amp;P 500 Total Return\u0026quot;)) %\u0026gt;% ggplot(aes(x = Year, y = Value, color = Type)) + geom_line() + scale_y_log10(labels = comma) + geom_vline(xintercept = 1971, linetype = \u0026quot;dashed\u0026quot;) + geom_text(aes(x = 1971, y = 250000, label = \u0026quot;End of Bretton Woods system\u0026quot;), color = \u0026quot;black\u0026quot;, hjust = -0.1) + labs(x = NULL, y = NULL, color = \u0026quot;Index\u0026quot;, title = \u0026quot;Evolution of different annual indexes in US dollar (1928 = 100)\u0026quot;)+ theme_bw() fig.IndexesEvolution Next, I look at the evolution of food price indexes using US dollar, ounces of gold or units of the S\u0026amp;P 500 total return index as numeraires. The figure shows that food became increasingly expensive in US dollar terms over time. Not surprisingly, the raise in gold prices would have rendered food substantially less expensive in gold terms. However, the price of food in S\u0026amp;P total return index units nearly approached zero.\nfig.NumerairesEvolution \u0026lt;- tbl.DataAnnual %\u0026gt;% filter(Type %in% c(\u0026quot;FoodIndexUSD\u0026quot;, \u0026quot;FoodIndexGold\u0026quot;, \u0026quot;FoodIndexSP500\u0026quot;)) %\u0026gt;% mutate(Type = case_when(Type == \u0026quot;FoodIndexUSD\u0026quot; ~ \u0026quot;US Dollar\u0026quot;, Type == \u0026quot;FoodIndexGold\u0026quot; ~ \u0026quot;Gold\u0026quot;, Type == \u0026quot;FoodIndexSP500\u0026quot; ~ \u0026quot;S\u0026amp;P 500 Total Return\u0026quot;), Type = factor(Type, levels = c(\u0026quot;US Dollar\u0026quot;, \u0026quot;Gold\u0026quot;, \u0026quot;S\u0026amp;P 500 Total Return\u0026quot;))) %\u0026gt;% ggplot(aes(x = Year, y = Value, color = Type)) + geom_line() + scale_y_log10(labels = comma) + geom_vline(xintercept = 1971, linetype = \u0026quot;dashed\u0026quot;) + geom_text(aes(x = 1971, y = 10000, label = \u0026quot;End of Bretton Woods system\u0026quot;), color = \u0026quot;black\u0026quot;, hjust = -0.1) + labs(x = NULL, y = NULL, color = \u0026quot;Numeraire\u0026quot;, title = \u0026quot;Evolution of food price indexes with different numeraires (1928 = 100)\u0026quot;)+ theme_bw() fig.NumerairesEvolution In the following visualization, I want to plot the average annual food prices changes in different periods: during the Bretton woods system, the 10 years after Bretton woods and since 1981. I compute the geometric mean which is appropriate in situations where values are meant to be multiplied together or that are exponential in nature. I just tock the function from here:\nfun.geometric_mean = function(x, na.rm = TRUE){ x \u0026lt;- 1+x out \u0026lt;- exp(sum(log(x[x \u0026gt; 0]), na.rm = na.rm) / length(x)) - 1 return(out) } I first construct a summary table by period before I plot the statistics.\ntbl.DataAnnualSummarized \u0026lt;- tbl.DataAnnual %\u0026gt;% filter(Type %in% c(\u0026quot;FoodIndexUSD\u0026quot;, \u0026quot;FoodIndexGold\u0026quot;, \u0026quot;FoodIndexSP500\u0026quot;)) %\u0026gt;% mutate(Period = cut(Year, c(-Inf, 1970, 1980, Inf), c(\u0026quot;Bretton Woods system\u0026quot;, \u0026quot;1971-1980\u0026quot;, \u0026quot;Since 1981\u0026quot;))) %\u0026gt;% mutate(Type = case_when(Type == \u0026quot;FoodIndexUSD\u0026quot; ~ \u0026quot;US Dollar\u0026quot;, Type == \u0026quot;FoodIndexGold\u0026quot; ~ \u0026quot;Gold\u0026quot;, Type == \u0026quot;FoodIndexSP500\u0026quot; ~ \u0026quot;S\u0026amp;P 500 Total Return\u0026quot;), Type = factor(Type, levels = c(\u0026quot;US Dollar\u0026quot;, \u0026quot;Gold\u0026quot;, \u0026quot;S\u0026amp;P 500 Total Return\u0026quot;))) %\u0026gt;% na.omit() %\u0026gt;% group_by(Period, Type) %\u0026gt;% summarize(N = n(), ChangeMean = fun.geometric_mean(Change), ChangeSD = sd(Change)) The figure below now shows the (geometric) average food index changes with different numeraires. It demonstrates that the price of food in US dollar terms increased strongly in the first 10 years after Bretton Woods, but reached similar growth levels to the Bretton Woods system afterwards. The deflationary effect of gold seems to be concentrated in the first 10 years after Bretton Woods, afterwards it is relatively weak. The S\u0026amp;P total return index would have lead to food price deflation in all periods, but was particularly weak in the first 10 years after Bretton Woods.\nfig.ComparisonMean \u0026lt;- tbl.DataAnnualSummarized %\u0026gt;% ggplot(aes(y = fct_rev(Type), x = ChangeMean, fill = fct_rev(Period))) + geom_col(position = \u0026quot;dodge\u0026quot;) + theme_bw() + scale_x_continuous(labels = percent) + labs(x = NULL, y = NULL, fill = \u0026quot;Period\u0026quot;, title = \u0026quot;Geometric mean of annual food index changes with different numeraires\u0026quot;) + guides(fill = guide_legend(reverse = TRUE)) fig.ComparisonMean The last figure shows the standard deviation of annual food index changes with different numeraires. It illustrates how the price of food became less volatile in US dollar terms after Bretton Woods, whereas it would have been much more volatile in gold or S\u0026amp;P 500 units in the same time period. The volatility of food in ounces of gold was particularly voaltile in the first 10 years after the end of Bretton Woods.\nfig.ComparisonSD \u0026lt;- tbl.DataAnnualSummarized %\u0026gt;% ggplot(aes(y = fct_rev(Type), x = ChangeSD, fill = fct_rev(Period))) + geom_col(position = \u0026quot;dodge\u0026quot;) + theme_bw() + scale_x_continuous(labels = percent) + labs(x = NULL, y = NULL, fill = \u0026quot;Period\u0026quot;, title = \u0026quot;Standard deviation of annual food index changes with different numeraires\u0026quot;) + guides(fill = guide_legend(reverse = TRUE)) fig.ComparisonSD Of course the visualizations above suggest some counter-factual scenarios that are far from reality. Probably the world would have looked very differently if US citizens had to pay for food in ounces of gold or shares of the S\u0026amp;P 500 total return index. Nonetheless, simple visualizations are able to tell interesting stories.\n ","date":1619481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"06d30492e22ecf3dca6bd1c8ccd1203b","permalink":"https://christophscheuch.github.io/post/r-stuff/historical-gold/","publishdate":"2021-04-27T00:00:00Z","relpermalink":"/post/r-stuff/historical-gold/","section":"post","summary":"How did food prices evolve during and after the Gold Standard using different numeraires?","tags":["R"],"title":"Visualizing Historical Food Prices in Units of Fiat, Gold, or Stock","type":"post"},{"authors":["Christoph Scheuch"],"categories":["R"],"content":" I recently wanted to simulate simple equity savings plans over long time horizons and many different starting periods. The good thing is that the S\u0026amp;P 500 index provides a great starting point as it is easily available since 1928 via Yahoo Finance. However, I wanted my savings plans to be accumulating, i.e., all cash distributions are reinvested in the savings plan. The S\u0026amp;P index is inadequate for this situation as it is a price index that only tracks its components’ price movements. The S\u0026amp;P 500 Total Return Index tracks the overall performance of the S\u0026amp;P 500 and would be the solution to my problem, but it is only available since 1988.\nFortunately, I came up with a solution using data provided by Robert Shiller and provide the complete code below for future reference. If you spot any errors or have better suggestions, please feel free to drop your comments below.\nThis is the set of packages I use throughout this note.\nlibrary(tidyverse) # for overall grammar library(lubridate) # to parse dates library(tidyquant) # to download data from yahoo finance library(glue) # to automatically construct figure captions library(scales) # for nicer axis labels library(readxl) # to read Shiller\u0026#39;s data  First, let us download the S\u0026amp;P 500 Total Return Index from Yahoo Finance. I only consider the closing prices of the last day of each month because my savings plans only transfer funds once a month. In principle, you could also approximate the daily time series, but I believe it will be noiser because Shiller only provides monthly data.\ntbl.SP500Recent \u0026lt;- tq_get(\u0026quot;^SP500TR\u0026quot;, get = \u0026quot;stock.prices\u0026quot;, from = \u0026quot;1988-01-04\u0026quot;, to = \u0026quot;2020-10-31\u0026quot;) %\u0026gt;% transmute(Date = date, TotalReturnIndex = close) %\u0026gt;% na.omit() %\u0026gt;% group_by(Month = ceiling_date(Date, \u0026quot;month\u0026quot;)-1) %\u0026gt;% arrange(Date) %\u0026gt;% filter(Date == max(Date)) %\u0026gt;% ungroup() %\u0026gt;% select(Month, TotalReturnIndex) Next, I download data from Robert Shiller’s website used in his great book Irrational Excuberance into a temporary file and read the relevant sheet. In particular, the data contains monthly S\u0026amp;P 500 price and dividend data. The original file has a bit of annoying date format that I have to correct before parsing.\ntemp \u0026lt;- tempfile(fileext = \u0026quot;.xls\u0026quot;) download.file(url = \u0026quot;http://www.econ.yale.edu/~shiller/data/ie_data.xls\u0026quot;, destfile = temp, mode=\u0026#39;wb\u0026#39;) tbl.ShillerHistorical \u0026lt;- read_excel(temp, sheet = \u0026quot;Data\u0026quot;, skip = 7) %\u0026gt;% transmute(Month = ceiling_date(ymd(str_replace(str_c(Date, \u0026quot;.01\u0026quot;), \u0026quot;\\\\.1\\\\.\u0026quot;, \u0026quot;\\\\.10\\\\.\u0026quot;)), \u0026quot;month\u0026quot;)-1, Price = as.numeric(P), Dividend = as.numeric(D))  To construct the total return index, I need a return figure that includes dividends. In the next code chunk, I compute monthly total returns of the S\u0026amp;P 500 index by incorporating the monthly dividend paid on the index in the corresponding month. Note that Shiller’s data contains the 12-month moving sum of monthly dividends, hence the division by 12. Admittedly, this is a brute force approximation, but I couldn’t come up with a better solution ad hoc.\ntbl.ShillerHistorical \u0026lt;- tbl.ShillerHistorical %\u0026gt;% arrange(Month) %\u0026gt;% mutate(Ret = (Price + Dividend / 12) / lag(Price) - 1) Before I go back in time, let us check whether the total return computed above is able to match the actual total return since 1988. I start with the first total return index number that is available and use the cumulative product of returns from above to construct the check time series.\ntbl.Check \u0026lt;- tbl.ShillerHistorical %\u0026gt;% full_join(tbl.SP500Recent, by = \u0026quot;Month\u0026quot;) %\u0026gt;% filter(!is.na(TotalReturnIndex)) %\u0026gt;% arrange(Month) %\u0026gt;% mutate(Ret = if_else(row_number() == 1, 0, Ret), # ignore first month return TotalReturnCheck = TotalReturnIndex[1] * cumprod(1 + Ret)) %\u0026gt;% na.omit() The correlation between the actual time series and the check is remarkably high which gives me confidence in the method I propose here.\ncor(tbl.Check$TotalReturnIndex, tbl.Check$TotalReturnCheck) ## [1] 0.9992971 In addition, the visual inspection of the two time series corroborates my confidence. Note that both the actual and the simulated total return indexes start at the same index value.\nfig.Check \u0026lt;- tbl.Check %\u0026gt;% select(Month, Actual = TotalReturnIndex, Simulated = TotalReturnCheck) %\u0026gt;% pivot_longer(cols = -Month, names_to = \u0026quot;Type\u0026quot;, values_to = \u0026quot;Value\u0026quot;) %\u0026gt;% ggplot(aes(x = Month, y = Value, color = Type)) + geom_line() + theme_bw() + scale_y_continuous(labels = comma)+ labs(x = NULL, y = NULL, title = \u0026quot;Actual and Simulated S\u0026amp;P 500 Total Return Index\u0026quot;, subtitle = glue(\u0026quot;Both Indexes start at {min(tbl.Check$Month)}\u0026quot;)) fig.Check Now, let us use the same logic to construct the total return index for the time before 1988. Note that I just sort the months in descending order and divide by the cumulative product of the total return from Shiller’s data.\ntbl.SP500Historical \u0026lt;- tbl.SP500Recent %\u0026gt;% filter(Month == min(Month)) %\u0026gt;% full_join(tbl.ShillerHistorical %\u0026gt;% filter(Month \u0026lt;= min(tbl.SP500Recent$Month)), by = \u0026quot;Month\u0026quot;) %\u0026gt;% arrange(desc(Month)) %\u0026gt;% mutate(Ret = if_else(row_number() == 1, 0, Ret), # ignore first month return TotalReturnIndex = TotalReturnIndex[1] / cumprod(1 + Ret)) Before we take a look at the results, I also add the S\u0026amp;P price index from Yahoo Finance for comparison.\ntbl.SP500Index \u0026lt;- tq_get(\u0026quot;^GSPC\u0026quot;, get = \u0026quot;stock.prices\u0026quot;, from = \u0026quot;1928-01-01\u0026quot;, to = \u0026quot;2020-10-31\u0026quot;) %\u0026gt;% transmute(Date = date, Index = close) %\u0026gt;% na.omit() %\u0026gt;% group_by(Month = ceiling_date(Date, \u0026quot;month\u0026quot;) - 1) %\u0026gt;% arrange(Date) %\u0026gt;% filter(Date == max(Date)) %\u0026gt;% ungroup() %\u0026gt;% select(Month, Index) Finally, let us combine (i) the actual S\u0026amp;P 500 Total Return Index from 1988 until 2020, (ii) the simulated S\u0026amp;P 500 total return index before 1988, and (iii) the S\u0026amp;P 500 price index from 1928 until 2020.\ntbl.SP500Monthly \u0026lt;- tbl.SP500Recent%\u0026gt;% bind_rows(tbl.SP500Historical %\u0026gt;% filter(Month \u0026lt; min(tbl.SP500Recent$Month)) %\u0026gt;% select(Month, TotalReturnIndex)) %\u0026gt;% full_join(tbl.SP500Index %\u0026gt;% select(Month, Index), by = \u0026quot;Month\u0026quot;) %\u0026gt;% filter(Month \u0026gt;= \u0026quot;1928-01-01\u0026quot;) %\u0026gt;% arrange(Month) tbl.SP500Monthly ## # A tibble: 1,114 x 3 ## Month TotalReturnIndex Index ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1928-01-31 1.20 17.6 ## 2 1928-02-29 1.21 17.3 ## 3 1928-03-31 1.20 19.3 ## 4 1928-04-30 1.26 19.8 ## 5 1928-05-31 1.35 20 ## 6 1928-06-30 1.39 19.1 ## 7 1928-07-31 1.33 19.4 ## 8 1928-08-31 1.35 20.9 ## 9 1928-09-30 1.39 21.1 ## 10 1928-10-31 1.50 21.7 ## # ... with 1,104 more rows The plot below shows you the dramatic differences in cumulative returns if you only consider price changes, as the S\u0026amp;P 500 Index does, versus total returns with reinvested capital gains. Note that I plot the indexes in log scale, otherwise everything until the last couple of decades would look like a flat line. I believe it is also important to keep the differences between price and performance indexes in mind whenever you compare equity indexes across countries. For instance, the DAX is a performance index by default and should never be compared with the S\u0026amp;P 500 price index.\nfig.Historical \u0026lt;- tbl.SP500Monthly %\u0026gt;% select(Month, Index, `Total Return` = TotalReturnIndex) %\u0026gt;% pivot_longer(cols = -Month, names_to = \u0026quot;Type\u0026quot;, values_to = \u0026quot;Value\u0026quot;) %\u0026gt;% group_by(Type) %\u0026gt;% arrange(Month) %\u0026gt;% mutate(Value = Value / Value[1] * 100) %\u0026gt;% ggplot(aes(x = Month, y = Value, color = Type)) + geom_line() + theme_bw() + scale_y_log10(labels = comma) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + labs(x = NULL, y = NULL, title = \u0026quot;S\u0026amp;P 500 Index and Total Return Index Since 1928\u0026quot;, subtitle = glue(\u0026quot;Both Indexes are Normalized to 100 at {min(tbl.SP500Monthly$Month)}\u0026quot;)) fig.Historical  ","date":1605398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605398400,"objectID":"ea7545ac407644165b279cfeff3ef545","permalink":"https://christophscheuch.github.io/post/r-stuff/historical-total-return/","publishdate":"2020-11-15T00:00:00Z","relpermalink":"/post/r-stuff/historical-total-return/","section":"post","summary":"How to construct a S\u0026P 500 total return index since 1928 using R","tags":["R"],"title":"Construction of a Historical S\u0026P 500 Total Return Index","type":"post"},{"authors":["Christoph Scheuch"],"categories":["R"],"content":" In this post, I provide a recollection of my effort to scrap Environmental, Social and Governance (ESG) information from Yahoo Finance (e.g., Apple). In particular, Yahoo Finance provides total ESG scores, environment, social and governance risk scores, as well as controversy levels, all compiled by Sustainalytics which is by now owned by Morningstar. My code builds on the walk-through by Kyle Ruden, which I adapted to the current page structure of Yahoo Finance and my own coding style. In addition, I added a few steps that I, as web scraping newbie, had to look up while going through his guide.\nTo begin with, I want to urge you to read at least the legal and ethical considerations put forward by Kyle. Most importantly, I want to mention that, when performing web scraping tasks, it is both good practice and often required to set a custom user agent request header to identify yourself, as well as sending requests at a modest rate to ‘smell like a human’. I consider both of these key aspects in my code below.\nThroughout this note, I rely on the following packages:\nlibrary(tidyverse) # overall grammar library(tidytext) # only for reorder_within function library(scales) # only for scales function library(httr) # http verbs library(rvest) # wrapper around xlm2 and httr library(robotstxt) # only for paths_allowed Get Symbols First, we want to get some companies for which we want to scrap ESG information from Yahoo Finance. Let us get a table of symbols and industry information of the S\u0026amp;P 500 constituents from Wikipedia. The function read_html normalizes the page to a valid XML document. html_nodes then allows us to point exactly to the table we can find on the website using the name of the CSS node. html_table then parses the HTML table into a data frame. Note that, as one of the last steps, we need to replace all dots in the symbols with dashes to get the symbols used by Yahoo Finance.\ntbl.Symbols \u0026lt;- read_html(\u0026quot;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\u0026quot;) %\u0026gt;% html_nodes(css = \u0026quot;table[id=\u0026#39;constituents\u0026#39;]\u0026quot;) %\u0026gt;% html_table() %\u0026gt;% data.frame() %\u0026gt;% as_tibble() %\u0026gt;% select(Symbol, Company = Security, Sector = GICS.Sector, Industry = GICS.Sub.Industry) %\u0026gt;% mutate(Symbol = str_replace(Symbol, \u0026quot;[.]\u0026quot;, \u0026quot;-\u0026quot;)) %\u0026gt;% arrange(Symbol) The following chunk prints what we got from Wikipedia. We will use the sector information in the last section of this post where we take a quick look at the scraped data.\ntbl.Symbols ## # A tibble: 505 x 4 ## Symbol Company Sector Industry ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 A Agilent Technologi~ Health Care Health Care Equipment ## 2 AAL American Airlines ~ Industrials Airlines ## 3 AAP Advance Auto Parts Consumer Discreti~ Automotive Retail ## 4 AAPL Apple Inc. Information Techn~ Technology Hardware, Storage \u0026amp;~ ## 5 ABBV AbbVie Inc. Health Care Pharmaceuticals ## 6 ABC AmerisourceBergen ~ Health Care Health Care Distributors ## 7 ABMD ABIOMED Inc Health Care Health Care Equipment ## 8 ABT Abbott Laboratories Health Care Health Care Equipment ## 9 ACN Accenture plc Information Techn~ IT Consulting \u0026amp; Other Services ## 10 ADBE Adobe Inc. Information Techn~ Application Software ## # ... with 495 more rows  Locate ESG Information The point where I struggled when I tried to replicate other guides was the search for the exact location of the information that I want to scrap (and the fact that the old locations seemed to have changed). After some trial and error, it turns out that it is really easy. Once you download a web page, you can in principle either use CSS nodes or XML paths to extract information using html_nodes as above. However, the CSS nodes on Yahoo Finance have a weird structure that is apparently not straight-forward to use in this function. Fortunately, XML paths work perfectly! Google will explain to you what these terms mean, I only demonstrate how you find the relevant paths which we use in the scraping function below.\nLet us stick to Apple as our main example and go to the sustainability tab on Yahoo Finance. If we right-click on the ESG score (e.g., using Google Chrome), we can see the the option to ‘Inspect’.\nOnce you click on it, a tab to the right opens where you see the underlying code. What is even more useful is the fact that the browser highlights the corresponding elements on the website as you hover over the code. This way, it is really easy to locate the information we are after. So we click on the relevant element and we copy the XML path.\nSo the location of the total ESG score on the page is:\n\u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[1]/div/div[1]/div/div[2]/div[1]\u0026#39; We can just point-and-click on all the items we want to scrap and collect the relevant XML paths. Once we downloaded a page, we just tell html_node where to look for the information we want (and afterwards how to parse it).\n Define Scraping Function My function to scrap ESG data takes two inputs: the stock symbol and your user agent. We got the symbols from Wikipedia, but we need to define our own user agent. For instance, I use an agent that looks like this:\nagent \u0026lt;- \u0026quot;Your Name (your@email.com). Doing personal research.\u0026quot; The main function then proceeds as follows:\nConstruct the link of the page we want to download. Check if scraping is allowed. Store the time stamp of our scraping effort. Download the page. Store individual information using the XML paths we manually extracted following the point-and-click procedure from above. Collect all information in a table.  Note that the XML paths might be different at a later change, e.g., because of new website designs of Yahoo Finance. So you might have to update those manually yourself in case you try to use my code.\nscrap_esg_data \u0026lt;- function(symbol, agent) { # construct link link \u0026lt;- str_c(\u0026quot;https://finance.yahoo.com/quote/\u0026quot;, symbol, \u0026quot;/sustainability?p=\u0026quot;, symbol) # check if scraping is allowed check \u0026lt;- suppressMessages(paths_allowed(link)) if (check == TRUE) { # store scrap date scrap_date \u0026lt;- Sys.time() # download page page \u0026lt;- GET(link, user_agent(agent)) %\u0026gt;% read_html() # store total ESG score total_esg_score \u0026lt;- page %\u0026gt;% html_node(xpath = \u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[1]/div/div[1]/div/div[2]/div[1]\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% parse_number() # store total ESG percentile total_esg_percentile \u0026lt;- page %\u0026gt;% html_node(xpath = \u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[1]/div/div[1]/div/div[2]/div[2]/span\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% parse_number() # store environment risk score environment_risk_score \u0026lt;- page %\u0026gt;% html_node(xpath = \u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[1]/div/div[2]/div/div[2]/div[1]\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% parse_number() # store social risk score social_risk_score \u0026lt;- page %\u0026gt;% html_node(xpath = \u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[1]/div/div[3]/div/div[2]/div[1]\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% parse_number() # store governance risk score governance_risk_score \u0026lt;- page %\u0026gt;% html_node(xpath = \u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[1]/div/div[4]/div/div[2]/div[1]\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% parse_number() # store controversy level controversy_level \u0026lt;- page %\u0026gt;% html_node(xpath = \u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[2]/div[2]/div/div/div/div[1]/div\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% parse_number() # store last update date (currently in %m/%Y format) last_update_date \u0026lt;- page %\u0026gt;% html_node(xpath = \u0026#39;//*[@id=\u0026quot;Col1-0-Sustainability-Proxy\u0026quot;]/section/div[3]/span[2]/span\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% str_remove(., \u0026quot;Last updated on \u0026quot;) # collect output in tibble output \u0026lt;- tibble( Symbol = symbol, Link = link, ScrapDate = scrap_date, TotalESGScore = total_esg_score, EnvironmentRiskScore = environment_risk_score, SocialRiskScore = social_risk_score, GovernanceRiskScore = governance_risk_score, ControversyLevel = controversy_level, LastUpdateDate = last_update_date ) return(output) } else { # if scraping is not allowed, throw an error stop(str_c(\u0026quot;No bots allowed on page \u0026#39;\u0026quot;, link ,\u0026quot;\u0026#39;!\u0026quot;)) } }  Scrap ESG Data For output data of this size, I prefer to initialize an empty list that I sequentially fill using a simple loop and then bind the rows again to a table once the loop is done. Moreover, we have to incorporate (random) waiting times before we try to retrieve information from Yahoo Finance such that our scraping resembles the normal clicking behavior of a human. In case we get a timeout, there is no ESG information, so we tell the loop to try again, but wait a little longer. After 10 attempts, the loop gives up and assumes that there is no ESG information for the particular symbol, which is in fact the case most of the time.\n# initialize output list lst.ESGInfo \u0026lt;- vector(\u0026quot;list\u0026quot;, nrow(tbl.Symbols)) # loop over symbols for (j in 1:nrow(tbl.Symbols)) { # extract ESG information tbl.Results \u0026lt;- NULL attempt \u0026lt;- 1 # possibly use multiple attempts to fetch data (with max 10) while (is.null(tbl.Results) \u0026amp; attempt \u0026lt;= 10) { # wait random amount of time to avoid timeouts with increasing waiting time Sys.sleep(runif(1, attempt*5, attempt*10)) attempt \u0026lt;- attempt + 1 # try to extract ESG information tbl.Results \u0026lt;- try( scrap_esg_data(tbl.Symbols$Symbol[j], agent) ) # ESG score might be missing b/c we were blocked so try again just in case if (is.na(tbl.Results$TotalESGScore)) { tbl.Results \u0026lt;- NULL } } # put results into list once done trying to get the data lst.ESGInfo[[j]] \u0026lt;- tbl.Results # print progress cat(as.character(Sys.time()), tbl.Symbols$Symbol[j], \u0026quot;done!\\n\u0026quot;) } tbl.ESGInfo \u0026lt;- tbl.Symbols %\u0026gt;% left_join(bind_rows(lst.ESGInfo), by = c(\u0026quot;Symbol\u0026quot;)) The loop from above takes a couple of hours in the current specification because of the increasing waiting times. I am sure that there are better ways to solve the timeout problem, so feel free to drop a comment below. The whole table then looks like this and also includes our initial example Apple:\ntbl.ESGInfo ## # A tibble: 505 x 12 ## Symbol Company Sector Industry Link ScrapDate TotalESGScore ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A Agilen~ Healt~ Health ~ http~ 2020-08-16 09:35:22 17 ## 2 AAL Americ~ Indus~ Airlines http~ 2020-08-16 09:35:33 31 ## 3 AAP Advanc~ Consu~ Automot~ http~ 2020-08-16 09:35:39 12 ## 4 AAPL Apple ~ Infor~ Technol~ http~ 2020-08-16 09:35:47 24 ## 5 ABBV AbbVie~ Healt~ Pharmac~ http~ 2020-08-16 09:35:55 30 ## 6 ABC Ameris~ Healt~ Health ~ http~ 2020-08-16 09:36:06 18 ## 7 ABMD ABIOME~ Healt~ Health ~ \u0026lt;NA\u0026gt; NA NA ## 8 ABT Abbott~ Healt~ Health ~ http~ 2020-08-16 09:42:52 30 ## 9 ACN Accent~ Infor~ IT Cons~ http~ 2020-08-16 09:42:58 11 ## 10 ADBE Adobe ~ Infor~ Applica~ http~ 2020-08-16 09:43:06 14 ## # ... with 495 more rows, and 5 more variables: EnvironmentRiskScore \u0026lt;dbl\u0026gt;, ## # SocialRiskScore \u0026lt;dbl\u0026gt;, GovernanceRiskScore \u0026lt;dbl\u0026gt;, ControversyLevel \u0026lt;dbl\u0026gt;, ## # LastUpdateDate \u0026lt;chr\u0026gt;  Quick Evaluation of ESG Scores Let us take a quick look at the data we collected. First, let us check the overall coverage of our sample:\npercent(nrow(na.omit(tbl.ESGInfo)) / nrow(tbl.ESGInfo)) ## [1] \u0026quot;93%\u0026quot; This is not too bad. I believe that for most of the companies without ESG scores in my sample, Yahoo Finance does not provide any data. Admittedly, I should check manually at some point, but for the purpose of this note, this is definitely a success. To analyze sector-level breakdowns, I construct a summary table which I use as the main source for the following figures.\ntab.ESGScoresBySectors \u0026lt;- tbl.ESGInfo %\u0026gt;% group_by(Sector) %\u0026gt;% summarize(Companies = n(), Coverage = sum(!is.na(TotalESGScore)) / n(), `Total ESG Score` = mean(TotalESGScore, na.rm = TRUE), `Environment Risk Score` = mean(EnvironmentRiskScore, na.rm = TRUE), `Social Risk Score` = mean(SocialRiskScore, na.rm = TRUE), `Governance Risk Score` = mean(GovernanceRiskScore, na.rm = TRUE), `Controversy Level` = mean(ControversyLevel, na.rm = TRUE)) %\u0026gt;% arrange(-Coverage) The first figure gives us the coverage per sector. All real estate companies have ESG scores, while only a bit more than three quarters of communication services feature this information.\ntab.ESGScoresBySectors %\u0026gt;% mutate(Labels = str_c(Companies*Coverage, \u0026quot; out of \u0026quot;, Companies)) %\u0026gt;% ggplot(aes(y = reorder(Sector, Coverage), x = Coverage, fill = factor(round(Coverage, 0)))) + geom_col(show.legend = FALSE) + theme_minimal() + geom_text(aes(label = Labels), hjust = 1.1, color = \u0026quot;white\u0026quot;) + coord_cartesian(xlim = c(0, 1)) + scale_x_continuous(labels = percent) + labs(x = NULL, y = NULL, title = \u0026quot;How many companies have ESG scores per sector?\u0026quot;, subtitle = \u0026quot;Based on Yahoo Finance and S\u0026amp;P 500 data as of August 2020\u0026quot;) Next, I want to look at average ESG scores by sector (s/o to the amazing Julia Silge for pointing out the reorder_within function which makes facet grids even more fun). For instance, the real estate sector has the lowest total ESG score, indicating the lowest degree to which a sector’s business value is at risk driven by environmental, social and governance risks. Financials exhibit the the lowest environmental risk, while the energy sector (at least the part included in the S\u0026amp;P 500) has the highest exposure to environmental risks.\ntab.ESGScoresBySectors %\u0026gt;% pivot_longer(cols = c(`Total ESG Score`, `Environment Risk Score`, `Social Risk Score`, `Governance Risk Score`), names_to = \u0026quot;Types\u0026quot;, values_to = \u0026quot;Scores\u0026quot;) %\u0026gt;% mutate(Types = factor(Types, levels = c(\u0026quot;Total ESG Score\u0026quot;, \u0026quot;Environment Risk Score\u0026quot;, \u0026quot;Social Risk Score\u0026quot;, \u0026quot;Governance Risk Score\u0026quot;)), Sector = reorder_within(Sector, -Scores, Types)) %\u0026gt;% ggplot(aes(y = Sector, x = Scores, fill = Types)) + geom_col(show.legend = FALSE) + facet_wrap(~Types, scales = \u0026quot;free_y\u0026quot;) + theme_minimal() + scale_y_reordered() + geom_text(aes(label = round(Scores, 0)), hjust = 1.1, color = \u0026quot;white\u0026quot;) + labs(y = NULL, x = NULL, title = \u0026quot;What are the average ESG scores per sector?\u0026quot;, subtitle = \u0026quot;Based on Yahoo Finance and S\u0026amp;P 500 data as of August 2020\u0026quot;) Finally, I am also interested in the average controversy level which measures to which degree companies are involved in incidents and events that may negatively impact stakeholders, the environment or their operations. I decided to plot the controversy of each sector relative to the average overall controversy. Real estate and information technology seem to be far less controverse than consumer staples and communication services.\ntab.ESGScoresBySectors %\u0026gt;% mutate(RelativeControversy = `Controversy Level` - mean(`Controversy Level`)) %\u0026gt;% ggplot(aes(y = reorder(Sector, -RelativeControversy), x = RelativeControversy, fill = (RelativeControversy \u0026lt; 0))) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;) + theme_minimal() + theme(legend.position = \u0026quot;none\u0026quot;) + coord_cartesian(xlim = c(-1.5, 1.5)) + labs(y = NULL, x = NULL, title = \u0026quot;What is the average sector-level controversy relative to overall controversy?\u0026quot;, subtitle = \u0026quot;Based on Yahoo Finance and S\u0026amp;P 500 data as of August 2020\u0026quot;) I think there is a lot more interesting stuff to uncover using the ESG scores, but for now I’ll leave it at that. Please feel free to share any suggestions in the comments below on how to improve my proposed scrapping procedure. I am nonetheless surprised, how easy scraping information from websites is using these amazing packages.\n ","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597622400,"objectID":"a2614dfc31c069ace3831b7c24490dda","permalink":"https://christophscheuch.github.io/post/r-stuff/scraping-esg-data/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/post/r-stuff/scraping-esg-data/","section":"post","summary":"How to scrap environmental, social and governance (ESG) risk scores from Yahoo Finance using R","tags":["R"],"title":"Scraping ESG Data from Yahoo Finance with R","type":"post"},{"authors":["[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1587081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587081600,"objectID":"de2008daaf489ae7401ced461e486911","permalink":"https://christophscheuch.github.io/publication/demand-uncertainty/","publishdate":"2020-04-17T00:00:00Z","relpermalink":"/publication/demand-uncertainty/","section":"publication","summary":"Reward-based crowdfunding allows entrepreneurs to sell claims on future products to finance investments and, at the same time, to generate demand information that benefits screening for viable projects. I characterize the profit-maximizing crowdfunding mechanism when the entrepreneur knows neither the number of consumers who positively value the product, nor their reservation prices. The entrepreneur can finance all viable projects by committing to prices that decrease in the number of pledgers, which grants consumers with high reservation prices information rents. However, if these information rents are large, then the entrepreneur prefers fixed high prices that lead to underinvestment.Presentations: $2^{nd}$ Toronto FinTech Conference, VGSF Conference 2018 \u0026 2019","tags":null,"title":"Crowdfunding and Demand Uncertainty","type":"publication"},{"authors":["[Nikolaus Hautsch](https://homepage.univie.ac.at/nikolaus.hautsch/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Stefan Voigt](https://voigtstefan.github.io/)"],"categories":null,"content":"","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585008000,"objectID":"90e40fecf32e9f38117f8cf6bb3ee4f8","permalink":"https://christophscheuch.github.io/publication/settlement-latency/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/publication/settlement-latency/","section":"publication","summary":"Distributed ledger technologies replace trusted clearing counterparties and security depositories with time-consuming consensus protocols to record the transfer of ownership. This settlement latency exposes cross-market arbitrageurs to price risk. We theoretically derive arbitrage bounds that increase with expected latency, latency uncertainty, volatility and risk aversion. Using Bitcoin orderbook and network data, we estimate arbitrage bounds of on average 121 basis points, explaining 91% of the observed cross-market price differences. Consistent with our theory, periods of high latency-implied price risk exhibit large price differences, while asset flows chase arbitrage opportunities. Blockchain-based settlement thus introduces a non-trivial friction that impedes arbitrage activity. Presentations: QFFE 2018, $1^{st}$ International Conference on Data Science in Finance with R, $4^{th}$ Konstanz-Lancaster Workshop on Finance and Econometrics, Crypto Valley Blockchain Conference 2018, HFFE 2018, CFE 2018, CUNEF, University of Heidelberg, University of Vienna, University of Graz, $2^{nd}$ Toronto FinTech Conference, $4^{th}$ Vienna Workshop on High-Dimensional Time Series 2019, Conference on Market Microstructure and High Frequency Data 2019, FIRS Conference 2019, $12^{th}$ Annual SoFiE Conference, IMS at the National University of Singapore, $3^{rd}$ SAFE Microstructure Conference, EFA Annual Meeting 2019, Vienna Congress on Mathematical Finance, International Conference on Fintech \u0026 Financial Data Science 2019, $4^{th}$ International Workshop in Financial Econometrics, CFM-Imperial Workshop 2019, WFA 2020","tags":null,"title":"Building Trust Takes Time: Limits to Arbitrage in Blockchain-Based Markets","type":"publication"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" This note is an effort to replicate the famous Fama-French three-factor model, in particular the three factors that enter its equation: the market factor, the size factor (small firms outperform larger firms), and the value factor (firms with high book-to-market equity ratios outperform firms with low ratios). Throughout this note, I build on earlier posts that explored empirical asset pricing approaches in a similar direction. Namely, I constructed the CRSP stock return sample, I estimated stock-specific market betas, and I analyzed the relation between stock returns and size and I replicated the value premium. I keep the description of individual steps very short and refer to these notes for more details.\nAt the end of this post, I demonstrate that my effort yields a decent result: my version of the mkt factor exhibits a 99% correlation with the market factor provided on Kenneth French’s website. Similarly, my smb and hml factors show a 97% and 95% correlation, respectively, with the original data. Wayne Chang provides an R script on his website that supposedly achieves higher correlations. However, I have to admit that I have not yet tried to run his code because I find it very hard to grasp what is going on by just glancing over it. Maybe I’ll devote some more time to his code at a later stage.\nThe usual disclaimer applies, so the text below references an opinion and is for information purposes only and I do not intend to provide any investment advice.\nI mainly use the following packages throughout this note:\nlibrary(tidyverse) # for grammar library(scales) # for nicer axes of figures library(viridis) # for nicer colors in figures library(vroom) # for fast csv reading library(lubridate) # for working with dates library(moments) # for skewness and kurtosis library(kableExtra) # for nicer html tables Prepare CRSP Data This note is self-contained in the sense that everything starts from the raw data downloaded from WRDS. Note that your replication efforts might end up differently if you use another data source! First, I need to construct the sample of monthly stock returns. The code chunk below exhibits two differences to the earlier post: I drop observations before 1962 as most papers in empirical asset pricing start in 1963 due to data availability; I drop stocks that are listed on exchanges other than NYSE, AMEX or NASDAQ since Fama and French stress that they focus their seminal paper on these stocks.\n# Read raw monthly crsp file tbl.crsp_msf \u0026lt;- vroom(\u0026quot;raw/crsp_msf.csv\u0026quot;, col_types = cols(.default = col_character())) colnames(tbl.crsp_msf) \u0026lt;- str_to_lower(colnames(tbl.crsp_msf)) # Parse only relevant variables tbl.crsp \u0026lt;- tbl.crsp_msf %\u0026gt;% transmute(permno = as.integer(permno), # security identifier date = ymd(date), # month identifier ret = as.numeric(ret) * 100, # return (converted to percent) shrout = as.numeric(shrout), # shares outstanding (in thousands) altprc = as.numeric(altprc), # last traded price in a month exchcd = as.integer(exchcd), # exchange code shrcd = as.integer(shrcd), # share code siccd = as.integer(siccd), # industry code dlret = as.numeric(dlret) * 100, # delisting return (converted to percent) dlstcd = as.integer(dlstcd) # delisting code ) # Analysis of fama french (1993) starts in 1963 so I only need data after 1962 tbl.crsp \u0026lt;- tbl.crsp %\u0026gt;% filter(date \u0026gt;= \u0026quot;1962-01-01\u0026quot;) # Keep only US-based common stocks (10 and 11) tbl.crsp \u0026lt;- tbl.crsp %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% select(-shrcd) # Keep only distinct observations to avoid multiple counting tbl.crsp \u0026lt;- tbl.crsp %\u0026gt;% distinct(permno, date, .keep_all = TRUE) # remove duplicates # Compute market cap # Note: altprc is the negative of average of bid and ask from last traded price # for which the data is available if there is no last traded price tbl.crsp \u0026lt;- tbl.crsp %\u0026gt;% mutate(mktcap = abs(shrout * altprc) / 1000, # in millions of dollars mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap)) # Define exchange labels and keep only NYSE, AMEX and NASDAQ stocks tbl.crsp \u0026lt;- tbl.crsp %\u0026gt;% mutate(exchange = case_when(exchcd %in% c(1, 31) ~ \u0026quot;NYSE\u0026quot;, exchcd %in% c(2, 32) ~ \u0026quot;AMEX\u0026quot;, exchcd %in% c(3, 33) ~ \u0026quot;NASDAQ\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot;)) %\u0026gt;% filter(exchange != \u0026quot;Other\u0026quot;) # Adjust delisting returns (see Shumway, 1997) tbl.crsp \u0026lt;- tbl.crsp %\u0026gt;% mutate(ret_adj = case_when(is.na(dlstcd) ~ ret, !is.na(dlstcd) \u0026amp; !is.na(dlret) ~ dlret, dlstcd %in% c(500, 520, 580, 584) | (dlstcd \u0026gt;= 551 \u0026amp; dlstcd \u0026lt;= 574) ~ -30, TRUE ~ -100)) %\u0026gt;% select(-c(dlret, dlstcd))  Prepare CRSP/Compustat Merged Data The next code chunk is explained in this post with two additions: I do not include firms until they have appeared in Compustat for two years, just as in Fama and French (1993). However, this extra filter only has a negligible impact on the overall results. Moreover, the construction of book equity is now based on the instructions provided on Kenneth French’s website as opposed to the slightly simpler implementation in Bali, Engle and Murray (2016).\n# Read raw CRSP/Compustat merged file tbl.ccmfund \u0026lt;- vroom(\u0026quot;raw/ccmfund.csv\u0026quot;, col_types = cols(.default = col_character())) colnames(tbl.ccmfund) \u0026lt;- str_to_lower(colnames(tbl.ccmfund)) # Select and parse relevant variables tbl.compustat \u0026lt;- tbl.ccmfund %\u0026gt;% transmute( gvkey = as.integer(gvkey), # firm identifier permno = as.integer(lpermno), # stock identifier datadate = ymd(datadate), # date of report linktype = as.character(linktype), # link type linkenddt = ymd(linkenddt), # date when link ends to be valid seq = as.numeric(seq), # stockholders\u0026#39; equity ceq = as.numeric(ceq), # total common/ordinary equity at = as.numeric(at), # total assets lt = as.numeric(lt), # total liabilities txditc = as.numeric(txditc), # deferred taxes and investment tax credit txdb = as.numeric(txdb), # deferred taxes itcb = as.numeric(itcb), # investment tax credit pstkrv = as.numeric(pstkrv), # preferred stock redemption value pstkl = as.numeric(pstkl), # preferred stock liquidating value pstk = as.numeric(pstk), # preferred stock par value indfmt = as.character(indfmt), # industry format datafmt = as.character(datafmt) # data format ) # Make sure that only correct industry and data format is used tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% filter(indfmt == \u0026quot;INDL\u0026quot; \u0026amp; datafmt == \u0026quot;STD\u0026quot;) %\u0026gt;% select(-c(indfmt, datafmt)) # Check that only valid links are used tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% filter(linktype %in% c(\u0026quot;LU\u0026quot;, \u0026quot;LC\u0026quot;)) # Check that links are still active at time of information release tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% filter(datadate \u0026lt;= linkenddt | is.na(linkenddt)) # Keep only distinct observations tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% distinct() # Calculate book value of preferred stock and equity tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% mutate(be = coalesce(seq, ceq + pstk, at - lt) + coalesce(txditc, txdb + itcb, 0) - coalesce(pstkrv, pstkl, pstk, 0), be = if_else(be \u0026lt; 0, as.numeric(NA), be)) %\u0026gt;% select(gvkey, permno, datadate, be) # Determine year for matching and keep only last observation per year tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% mutate(year = year(datadate)) %\u0026gt;% group_by(permno, year) %\u0026gt;% filter(datadate == max(datadate)) %\u0026gt;% ungroup() # Keep only observations once a firm has been included for two years tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% group_by(gvkey) %\u0026gt;% mutate(first_year = min(year)) %\u0026gt;% ungroup() %\u0026gt;% filter(year \u0026gt; first_year + 1) %\u0026gt;% select(-c(gvkey, first_year)) # Kick out unnecessary rows with missing values tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% na.omit() # Determine reference date for matching (i.e. June of next calendar year) tbl.compustat \u0026lt;- tbl.compustat %\u0026gt;% mutate(reference_date = ymd(paste0(year + 1, \u0026quot;-06-01\u0026quot;))) %\u0026gt;% select(-year)  Construct Stock Sample The stock sample used for the construction of the factors also builds on this post. Note that the relevant returns for the factor construction are the adjusted returns in the current month. In the end, only stocks that have valid book equity data from year \\(y-1\\), market equity data at the end of year \\(y-1\\) and market capitalization in June of year \\(y\\) enter the factor construction procedure. Moreover, the market capitalization of June of year \\(y\\) is used to construct value-weighted returns until May of year \\(y+1\\). I also restrict the sample to start in 1963, consistent with Fama and French.\n# Select relevant variables tbl.stocks \u0026lt;- tbl.crsp %\u0026gt;% select(permno, date, exchange, ret = ret_adj, mktcap) %\u0026gt;% na.omit() # Define reference date for each stock (i.e. new sorting starts in June of year y) tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% mutate(reference_date = ymd(if_else(month(date) \u0026lt; 6, paste0(year(date) - 1, \u0026quot;-06-01\u0026quot;), paste0(year(date), \u0026quot;-06-01\u0026quot;)))) # Add book equity data for year y-1 which is used starting in June of year y tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% left_join(tbl.compustat, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) # Add market equity data from the end of year y-1 which is used for bm ratio in June of year y tbl.stocks_me \u0026lt;- tbl.stocks %\u0026gt;% filter(month(date) == 12) %\u0026gt;% mutate(reference_date = ymd(paste0(year(date) + 1, \u0026quot;-06-01\u0026quot;))) %\u0026gt;% select(permno, reference_date, me = mktcap) tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% left_join(tbl.stocks_me, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) # Compute book-to-market ratio tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% mutate(bm = be / me) # Add market cap of June of year y which is used for value-weighted returns tbl.stocks_weight \u0026lt;- tbl.stocks %\u0026gt;% filter(month(date) == 6) %\u0026gt;% select(permno, reference_date, mktcap_weight = mktcap) tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% left_join(tbl.stocks_weight, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) # Only keep stocks that have all the necessary data # (i.e. market equity data for December y-1 and June y, and book equity data for y-1) tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% na.omit() %\u0026gt;% filter(date \u0026gt;= \u0026quot;1963-01-01\u0026quot;) # Fama-French paper starts here  Size Sorts To construct the size portfolios, Fama and French independently sort all stocks into two portfolios according to the median of all NYSE stocks in the June of each year. Portfolios are then kept in these portfolios until June of the following year. This is exactly what happens in the code chunk below.\n# In June of each year, all NYSE stocks are ranked on size to get the median tbl.size_breakpoints \u0026lt;- tbl.stocks %\u0026gt;% filter(month(date) == 6 \u0026amp; exchange == \u0026quot;NYSE\u0026quot;) %\u0026gt;% select(reference_date, mktcap) %\u0026gt;% group_by(reference_date) %\u0026gt;% summarize(size_median = median(mktcap)) # Also in June, all stocks are sorted into 2 portfolios tbl.size_sorts \u0026lt;- tbl.stocks %\u0026gt;% filter(month(date) == 6) %\u0026gt;% left_join(tbl.size_breakpoints, by = \u0026quot;reference_date\u0026quot;) %\u0026gt;% mutate(size_portfolio = case_when(mktcap \u0026gt; size_median ~ \u0026quot;B\u0026quot;, mktcap \u0026lt;= size_median ~ \u0026quot;S\u0026quot;, TRUE ~ as.character(NA))) %\u0026gt;% select(permno, reference_date, size_portfolio) # Add size portfolio assignment back to stock data tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% left_join(tbl.size_sorts, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) Now each stock in my sample received an assignment to a size portfolio of either big \u0026quot;B\u0026quot; or small \u0026quot;S\u0026quot;.\n Value Sorts Fama and French determine the value portfolios based on 30% and 70% breakpoints again using NYSE stocks only in June of each year. Each stock is then independently sorted into either high BM \u0026quot;H\u0026quot;, medium bm \u0026quot;M\u0026quot; or low bm \u0026quot;L\u0026quot;.\n# Calculate value breakpoints using NYSE stocks tbl.value_breakpoints \u0026lt;- tbl.stocks %\u0026gt;% filter(month(date) == 6 \u0026amp; exchange == \u0026quot;NYSE\u0026quot;) %\u0026gt;% select(reference_date, bm) %\u0026gt;% group_by(reference_date) %\u0026gt;% summarize(value_q30 = quantile(bm, 0.3), value_q70 = quantile(bm, 0.7)) # Also in June, all stocks are sorted into 3 portfolios tbl.value_sorts \u0026lt;- tbl.stocks %\u0026gt;% filter(month(date) == 6) %\u0026gt;% left_join(tbl.value_breakpoints, by = \u0026quot;reference_date\u0026quot;) %\u0026gt;% mutate(value_portfolio = case_when(bm \u0026gt; value_q70 ~ \u0026quot;H\u0026quot;, bm \u0026lt;= value_q70 \u0026amp; bm \u0026gt; value_q30 ~ \u0026quot;M\u0026quot;, bm \u0026lt;= value_q30 ~ \u0026quot;L\u0026quot;, TRUE ~ as.character(NA))) %\u0026gt;% select(permno, reference_date, value_portfolio) # Add value portfolio assignment back to stock data tbl.stocks \u0026lt;- tbl.stocks %\u0026gt;% left_join(tbl.value_sorts, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;))  Construct Factor Portfolios To construct the smb and hml portfolios, I simply compute value-weighted returns for all combinations of value and size portfolios and then take the difference of the resulting portfolios as described by Fama and French.\ntbl.portfolios \u0026lt;- tbl.stocks %\u0026gt;% group_by(date, size_portfolio, value_portfolio) %\u0026gt;% summarize(ret_vw = weighted.mean(ret, mktcap_weight)) %\u0026gt;% ungroup() %\u0026gt;% mutate(portfolio = paste0(size_portfolio, \u0026quot;/\u0026quot;, value_portfolio)) tbl.factors \u0026lt;- tbl.portfolios %\u0026gt;% group_by(date) %\u0026gt;% summarize(smb = mean(ret_vw[portfolio %in% c(\u0026quot;S/H\u0026quot;, \u0026quot;S/M\u0026quot;, \u0026quot;S/L\u0026quot;)]) - mean(ret_vw[portfolio %in% c(\u0026quot;B/H\u0026quot;, \u0026quot;B/M\u0026quot;, \u0026quot;B/L\u0026quot;)]), hml = mean(ret_vw[portfolio %in% c(\u0026quot;S/H\u0026quot;, \u0026quot;B/H\u0026quot;)]) - mean(ret_vw[portfolio %in% c(\u0026quot;S/L\u0026quot;, \u0026quot;B/L\u0026quot;)])) Next, I also add the mkt factor as the monthly weighted average return across all stocks. I kick out the last month in my sample as it exhibits about a close to -100% return for whatever reason.\ntbl.factors \u0026lt;- tbl.factors %\u0026gt;% left_join(tbl.stocks %\u0026gt;% group_by(date) %\u0026gt;% summarize(mkt = weighted.mean(ret, mktcap_weight)), by = \u0026quot;date\u0026quot;) %\u0026gt;% select(date, mkt, smb, hml) %\u0026gt;% # rearrange columns filter(date \u0026lt; \u0026quot;2019-12-01\u0026quot;) # something weird is going on with returns in the last month Finally, I also add the corresponding factors from Ken French’s website.\ntbl.factors_ff \u0026lt;- read_csv(\u0026quot;raw/F-F_Research_Data_Factors.csv\u0026quot;, skip = 3) tbl.factors_ff \u0026lt;- tbl.factors_ff %\u0026gt;% transmute(date = ymd(paste0(X1, \u0026quot;01\u0026quot;)), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF), smb_ff = as.numeric(SMB), hml_ff = as.numeric(HML)) %\u0026gt;% filter(date \u0026lt;= max(tbl.crsp$date)) %\u0026gt;% mutate(date = floor_date(date, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-rf_ff) tbl.factors \u0026lt;- tbl.factors %\u0026gt;% mutate(date = floor_date(date, \u0026quot;month\u0026quot;)) %\u0026gt;% left_join(tbl.factors_ff, by = \u0026quot;date\u0026quot;)  Replication Results In the last section, I analyze how well I am able to replicate the original Fama-French factors. First, let us look at summary statistics across all 678 months.\ntab.summary \u0026lt;- tbl.factors %\u0026gt;% select(-date) %\u0026gt;% pivot_longer(cols = everything(), names_to = \u0026quot;measure\u0026quot;) %\u0026gt;% group_by(measure) %\u0026gt;% summarize(mean = mean(value), sd = sd(value), skew = skewness(value), kurt = kurtosis(value), min = min(value), q05 = quantile(value, 0.05), q25 = quantile(value, 0.25), q50 = quantile(value, 0.50), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95), max = max(value), n = n()) %\u0026gt;% ungroup() %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) tab.summary   measure  mean  sd  skew  kurt  min  q05  q25  q50  q75  q95  max  n      hml  0.25  2.84  -0.09  6.54  -15.10  -3.88  -1.29  0.21  1.65  4.78  14.94  678    hml_ff  0.31  2.81  0.19  4.90  -11.06  -4.02  -1.31  0.27  1.71  5.18  12.60  678    mkt  0.99  4.39  -0.41  4.98  -21.86  -6.52  -1.38  1.22  3.76  7.58  17.10  678    mkt_ff  0.91  4.37  -0.52  4.97  -22.64  -6.68  -1.65  1.25  3.74  7.43  16.61  678    smb  0.23  2.93  0.52  5.76  -12.23  -4.06  -1.55  0.10  1.98  4.66  16.40  678    smb_ff  0.18  3.04  0.44  7.88  -16.72  -4.22  -1.58  0.11  2.00  4.89  21.13  678     The distributions of my factors and the original ones seem to be quite similar for all three factors, but still far from perfect. Kolmogorov-Smirnov tests confirm that there is no statistically significant difference in the distributions.\n# Kolmogorov-Smirnov test ks.test(tbl.factors$mkt, tbl.factors$mkt_ff) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: tbl.factors$mkt and tbl.factors$mkt_ff ## D = 0.022124, p-value = 0.9964 ## alternative hypothesis: two-sided ks.test(tbl.factors$smb, tbl.factors$smb_ff) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: tbl.factors$smb and tbl.factors$smb_ff ## D = 0.020649, p-value = 0.9987 ## alternative hypothesis: two-sided ks.test(tbl.factors$hml, tbl.factors$hml_ff) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: tbl.factors$hml and tbl.factors$hml_ff ## D = 0.025074, p-value = 0.9834 ## alternative hypothesis: two-sided I am even more interested in the correlations of the factors, which I provide in the next table.\nfun.compute_correlations \u0026lt;- function(data, ..., upper = TRUE) { cor_matrix \u0026lt;- data %\u0026gt;% select(...) %\u0026gt;% cor(use = \u0026quot;complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) if (upper == TRUE) { cor_matrix[upper.tri(cor_matrix, diag = FALSE)] \u0026lt;- NA } return(cor_matrix) } tab.correlations \u0026lt;- fun.compute_correlations(tbl.factors, mkt, mkt_ff, smb, smb_ff, hml, hml_ff) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) tab.correlations    mkt  mkt_ff  smb  smb_ff  hml  hml_ff      mkt  1.00         mkt_ff  0.99  1.00        smb  0.27  0.31  1.00       smb_ff  0.25  0.30  0.97  1.00      hml  -0.23  -0.25  -0.14  -0.19  1.00     hml_ff  -0.22  -0.25  -0.12  -0.19  0.95  1     The mkt factor exhibits a correlation of 99% which is a pretty good fit. The smb factor also shows a satisfying fit with 97%. The hml factor only has a correlation of 95% with its original counterpart which might be driven by different sample construction procedures that I am not yet aware of or some ex-post changes in the CRSP or Compustat data that I capture, but Fama-French do not include.\nAs a last step, I present the time series of cumulative returns over the full sample period.\nfig.factors \u0026lt;- tbl.factors %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;portfolio\u0026quot;, values_to = \u0026quot;return\u0026quot;) %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% mutate(cum_return = cumsum(log(1+return/100))) %\u0026gt;% ungroup() %\u0026gt;% mutate(portfolio = str_to_upper(portfolio), portfolio = str_replace(portfolio, \u0026quot;_FF\u0026quot;, \u0026quot; (Fama-French)\u0026quot;)) %\u0026gt;% ggplot(aes(x = date, y = cum_return, color = portfolio)) + geom_line(aes()) + labs(x = NULL, y = NULL, color = NULL, title = \u0026quot;Cumulative log returns of original and replicated Fama-French factors\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = percent, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()   There seems to be a considerable divergence between my factors and the original data over time which is less severe for the smb factor. Please feel free to share any suggestions in the comments below on how to improve the fit. Overall, I am nonetheless somewhat surprised how quickly one can set up a decent replication of the three famous Fama-French factors.\n ","date":1582848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"da9ee212b81a32d2cce61ea8ddfd761f","permalink":"https://christophscheuch.github.io/post/asset-pricing/fama-french-3-factors/","publishdate":"2020-02-28T00:00:00Z","relpermalink":"/post/asset-pricing/fama-french-3-factors/","section":"post","summary":"A replication effort of the famous Fama-French factors","tags":["Asset Pricing"],"title":"Tidy Asset Pricing - Part V: The Fama-French 3-Factor Model","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" In this note, I explore the positive relation between the book-to-market (BM) ratio and expected stock returns – called the value premium – following Bali, Engle and Murray. In three earlier notes, I first prepared the CRSP sample in R, estimated and analyzed market betas and analyzed the relation between firm size and stock returns using the same data. I do not go into the theoretical foundations of the value premium, but rather focus on its estimation in a tidy manner. The usual disclaimer applies, so the text below references an opinion and is for information purposes only and I do not intend to provide any investment advice. The code below replicates most of the results of Bali et al. up to a few basis points if I restrict my data to their sample period. If you spot any mistakes or want to share any suggestions for better implementation, just drop a comment below.\nI mainly use the following packages throughout this note:\nlibrary(tidyverse) library(lubridate) # for working with dates library(broom) # for tidying up estimation results library(kableExtra) # for nicer html tables Calculating the Book-to-Market Ratio The BM ratio is defined as the book value of a firm’s common equity (BE) divided by the market value of the firm’s equity (ME), where the book value comes from the firm’s balance sheet and the market value is equal to the market capitalization of the firm as provided in the CRSP data. To get the BE data, I download the CRSP/Compustat Merged (CCM) data from WRDS. Importantly, this data already contains links of balance sheet information to the stock identifiers of CRSP.\nFirst, I extract the following variables from the CCM sample:\nccmfunda \u0026lt;- read_csv(\u0026quot;raw/ccmfunda.csv\u0026quot;) colnames(ccmfunda) \u0026lt;- tolower(colnames(ccmfunda)) # select and parse relevant variables compustat \u0026lt;- ccmfunda %\u0026gt;% transmute( permno = as.integer(lpermno), # stock identifier datadate = ymd(datadate), # date of report linktype = as.character(linktype), # link type linkenddt = ymd(linkenddt), # date when link ends to be valid seq = as.numeric(seq), # stockholders\u0026#39; equity txdb = as.numeric(txdb), # deferred taxes itcb = as.numeric(itcb), # investment tax credit pstkrv = as.numeric(pstkrv), # preferred stock redemption value pstkl = as.numeric(pstkl), # preferred stock liquidating value pstk = as.numeric(pstk), # preferred stock par value indfmt = as.character(indfmt), # industry format datafmt = as.character(datafmt) # data format )  Then, I apply a number of consistency checks and filters to the data. First, I need to make sure that only industry formats equal to \u0026quot;INDL\u0026quot; and data formats equal to \u0026quot;STD\u0026quot; are used.\ncompustat \u0026lt;- compustat %\u0026gt;% filter(indfmt == \u0026quot;INDL\u0026quot; \u0026amp; datafmt == \u0026quot;STD\u0026quot;) %\u0026gt;% select(-c(indfmt, datafmt)) Second, I should only use links that are established by comparing CUSIP values in the Compustat and CRSP database (\u0026quot;LU\u0026quot;) or links that have been researched and verified (\u0026quot;LC\u0026quot;).\ncompustat \u0026lt;- compustat %\u0026gt;% filter(linktype %in% c(\u0026quot;LU\u0026quot;, \u0026quot;LC\u0026quot;)) Third, I verify that links are still active at the time a firm’s accounting information is computed.\ncompustat \u0026lt;- compustat %\u0026gt;% filter(datadate \u0026lt;= linkenddt | is.na(linkenddt)) Fourth, I make sure that there are no duplicate observations.\ncompustat \u0026lt;- compustat %\u0026gt;% distinct() Fifth, I check that there is only one observation for each stock-date pair.\ncompustat %\u0026gt;% group_by(permno, datadate) %\u0026gt;% filter(n() \u0026gt; 1) %\u0026gt;% nrow() == 0 After these checks, I can proceed to calculate BE. The calculation is described in Bali et al. and translates to the following code where bvps refers to the book value of preferred stock.\ncompustat \u0026lt;- compustat %\u0026gt;% mutate(bvps = case_when(!is.na(pstkrv) ~ pstkrv, is.na(pstkrv) \u0026amp; !is.na(pstkl) ~ pstkl, is.na(pstkrv) \u0026amp; is.na(pstkl) \u0026amp; !is.na(pstk) ~ pstk, TRUE ~ as.numeric(0)), be = seq + txdb + itcb - bvps, be = if_else(be \u0026lt; 0, as.numeric(NA), be)) %\u0026gt;% select(permno, datadate, be)  To match the BE variable to the CRSP data and compute the BM ratio, I follow the procedure of Fama and French (1992), which is the most standard approach to doing this. Fama and French compute BM at the end of each calendar year and use that information starting in June of the next calendar year until the following May.\nFirms have different fiscal year ends that indicate the timing of the accounting information as reported in the column datadate. However, this date does not indicate when the accounting information was publicly available. Moreoever, in some cases, firms change the month in which their fiscal year ends, resulting in multiple entries in the Compustat data per calendar year \\(y\\). The Fama-French procedure first ensures that there is only one entry per calendar year \\(y\\) by taking the last available information.\ncompustat \u0026lt;- compustat %\u0026gt;% mutate(year = year(datadate)) %\u0026gt;% group_by(permno, year) %\u0026gt;% filter(datadate == max(datadate)) %\u0026gt;% ungroup() At this stage, I also kick out unnecessary rows with missing values in the BE column. These observations would lead to missing values in the upcoming matching procedure anyway.\ncompustat \u0026lt;- compustat %\u0026gt;% na.omit() nrow(compustat) Fama and French assume that data from the calendar year \\(y\\) is not known until the end of June of year \\(y+1\\). I hence define the corresponding reference date for each observation.\ncompustat \u0026lt;- compustat %\u0026gt;% mutate(reference_date = ymd(paste0(year + 1, \u0026quot;-06-01\u0026quot;))) %\u0026gt;% select(-year) For each return observation in year \\(y+1\\), the relevant accounting information is either still from the end of year \\(y-1\\) until May, or computed at the end of year \\(y\\) starting from June. I match the BE variable following this rationale in the next code chunk.\ncrsp \u0026lt;- read_rds(\u0026quot;data/crsp.rds\u0026quot;) # add book equity data crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date), reference_date = ymd(if_else(month \u0026lt; 6, paste0(year - 1, \u0026quot;-06-01\u0026quot;), paste0(year, \u0026quot;-06-01\u0026quot;)))) crsp \u0026lt;- crsp %\u0026gt;% left_join(compustat, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) As mentioned above, the ME variable is determined each year at the end of a calendar year, but it is used to calculate BM starting in June of the following year.\ncrsp_me \u0026lt;- crsp %\u0026gt;% filter(month(date_start) == 12) %\u0026gt;% mutate(reference_date = ymd(paste0(year(date_start) + 1, \u0026quot;-06-01\u0026quot;))) %\u0026gt;% select(permno, reference_date, me = mktcap) crsp \u0026lt;- crsp %\u0026gt;% left_join(crsp_me, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) Next, I also add an estimate for a stock’s beta which I calculated in this note. I use the beta based on daily data using a 12 month estimation window, as common in the literature according to Bali et al. betas \u0026lt;- read_rds(\u0026quot;data/betas_daily.rds\u0026quot;) crsp \u0026lt;- crsp %\u0026gt;% left_join(betas %\u0026gt;% select(permno, date_start, beta = beta_12m), by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) Finally, I can calculate the BM ratio and define additional variables that later enter the regression analysis.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(bm = be / me, log_bm = if_else(bm == 0, as.numeric(NA), log(bm)), size = log(mktcap)) The earliest date in my Compustat sample is in October 1960, while CRSP already starts in 1926. It might be thus interesting to look at the coverage of the variables of interest over the full sample period.\ncrsp %\u0026gt;% group_by(date) %\u0026gt;% summarize(share_with_me = sum(!is.na(me)) / n() * 100, share_with_be = sum(!is.na(be)) / n() * 100, share_with_bm = sum(!is.na(bm)) / n() * 100) %\u0026gt;% ggplot(aes(x = date)) + geom_line(aes(y = share_with_me, color = \u0026quot;Market Equity\u0026quot;)) + geom_line(aes(y = share_with_be, color = \u0026quot;Book Equity\u0026quot;)) + geom_line(aes(y = share_with_bm, color = \u0026quot;Book-to-Market\u0026quot;)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Share of Stocks with Information (in %)\u0026quot;, color = \u0026quot;Variable\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() There seem to be a few periods in which market and book equity data is temporarily not available anymore for a huge chunk of firms. However, I currently have no idea why this is the case.\nI follow the convention outlined in Bali et al. and start the sample in June 1963. Apart from the lack of accounting information, Bali et al. also mention that AMEX stocks start to be included in CRSP in July 1962 as a further reason (see this post).\ncrsp \u0026lt;- crsp %\u0026gt;% filter(date \u0026gt;= \u0026quot;1963-06-01\u0026quot;)  Summary Statistics I proceed to present summary statistics for the different measures I use in the following analyses. I first compute corresponding summary statistics for each month and before I average each summary statistics across all months. Note that I call the moments package to compute skewness and kurtosis. Since I repeatedly use a similar procedure, I define the following function which uses the dot-dot-dot argument and the curly-curly operator to compute summary statistics for aribtrary columns grouped by some variable.\nsummary_statistics \u0026lt;- function(data, ..., by) { data %\u0026gt;% select({{ by }}, ...) %\u0026gt;% pivot_longer(cols = -{{ by }}, names_to = \u0026quot;measure\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(measure, {{ by }}) %\u0026gt;% summarize(mean = mean(value), sd = sd(value), skew = moments::skewness(value), kurt = moments::kurtosis(value), min = min(value), q05 = quantile(value, 0.05), q25 = quantile(value, 0.25), q50 = quantile(value, 0.50), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95), max = max(value), n = n()) %\u0026gt;% ungroup() } The following table provides the time series averages of summary statistics for all observations with a non-missing BM.\ncrsp_ts \u0026lt;- crsp %\u0026gt;% filter(!is.na(bm)) %\u0026gt;% summary_statistics(bm, log_bm, be, me, mktcap, by = date) crsp_ts %\u0026gt;% select(-date) %\u0026gt;% group_by(measure) %\u0026gt;% summarize_all(list(mean)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   measure  mean  sd  skew  kurt  min  q05  q25  q50  q75  q95  max  n      be  856.15  4097.58  16.99  453.04  0.10  4.94  27.75  98.09  384.36  3227.64  99261.72  3779.89    bm  0.93  1.30  13.48  476.48  0.01  0.15  0.41  0.72  1.14  2.25  45.29  3779.89    log_bm  -0.48  0.87  -0.73  5.72  -6.41  -2.00  -0.95  -0.38  0.07  0.75  3.30  3779.84    me  1801.58  8290.59  15.38  349.23  0.88  8.62  46.20  187.48  781.51  6892.07  212293.66  3779.89    mktcap  1941.59  8921.13  15.16  338.47  0.71  8.21  46.79  198.64  853.28  7512.70  229728.20  3766.14     BM, BE and ME all exhibit a high positive skewness which justifies the log transformation in the regression analysis later. The average and median of BM are below one which indicates that the book equity values tend to be smaller than market equity valuations.\n Correlations I now proceed to examine the cross-sectional correlations between the variables of interest. To do so, I define the following function which takes an arbitrary number of columns as input.\ncompute_correlations \u0026lt;- function(data, ..., upper = TRUE) { cor_matrix \u0026lt;- data %\u0026gt;% select(...) %\u0026gt;% cor(use = \u0026quot;complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) if (upper == TRUE) { cor_matrix[upper.tri(cor_matrix, diag = FALSE)] \u0026lt;- NA } return(cor_matrix) } The table below provides the results.\ncorrelations \u0026lt;- crsp %\u0026gt;% filter(!is.na(bm)) %\u0026gt;% compute_correlations(bm, log_bm, be, me, mktcap, size, beta) correlations %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))    bm  log_bm  be  me  mktcap  size  beta      bm  1.00          log_bm  0.51  1.00         be  0.00  0.01  1.00        me  -0.04  -0.10  0.74  1.00       mktcap  -0.04  -0.09  0.72  0.96  1.00      size  -0.18  -0.29  0.31  0.35  0.35  1.00     beta  -0.09  -0.23  0.05  0.06  0.06  0.35  1     As expected, the cross-sectional correlation between BM and its log transformation is quite high. Moreover, BM is negatively correlated with each of market beta and size. The negative correlation between size and BM is somewhat mechnical als market capitalization is the denominator of BM.\n Persistence In this section, I exmanie the persistence of BM and its log transformation over time via the following function.\ncompute_persistence \u0026lt;- function(data, var, tau) { dates \u0026lt;- data %\u0026gt;% distinct(date) %\u0026gt;% arrange(date) %\u0026gt;% mutate(date_lag = lag(date, tau)) correlation \u0026lt;- data %\u0026gt;% select(permno, date, {{ var }}) %\u0026gt;% left_join(dates, by = \u0026quot;date\u0026quot;) %\u0026gt;% left_join(data %\u0026gt;% select(permno, date, {{ var }}) %\u0026gt;% rename(\u0026quot;{{ var }}_lag\u0026quot; := {{ var }}), by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_lag\u0026quot;=\u0026quot;date\u0026quot;)) %\u0026gt;% select(contains(rlang::quo_text(enquo(var)))) %\u0026gt;% cor(use = \u0026quot;pairwise.complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) return(correlation[1, 2]) } persistence \u0026lt;- tribble(~tau, ~bm, ~log_bm, 12, compute_persistence(crsp, bm, 12), compute_persistence(crsp, log_bm, 12), 24, compute_persistence(crsp, bm, 24), compute_persistence(crsp, log_bm, 24), 36, compute_persistence(crsp, bm, 36), compute_persistence(crsp, log_bm, 36), 48, compute_persistence(crsp, bm, 48), compute_persistence(crsp, log_bm, 48), 60, compute_persistence(crsp, bm, 60), compute_persistence(crsp, log_bm, 60), 120, compute_persistence(crsp, bm, 120), compute_persistence(crsp, log_bm, 120)) The next table provides the persistence results.\npersistence %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   tau  bm  log_bm      12  0.61  0.79    24  0.51  0.66    36  0.49  0.59    48  0.48  0.53    60  0.33  0.48    120  0.24  0.38     The table indicates thath both BM and log(BM) exhibit substantial persistence, but with decaying intensity over time. Regardless whether BM measures factor sensitivity or some sort of mispricing, it seems to persist for an extended period of time for the average stock.\n Portfolio Analysis Now, I turn to the cross-sectional relation between the book-to-market ratio and future stock returns. To conduct the univariate portfolio analysis, I need a couple of functions that I introduced in an earlier post.\nweighted_mean \u0026lt;- function(x, w, ..., na.rm = FALSE){ if(na.rm){ x1 \u0026lt;- x[!is.na(x) \u0026amp; !is.na(w)] w \u0026lt;- w[!is.na(x) \u0026amp; !is.na(w)] x \u0026lt;- x1 } weighted.mean(x, w, ..., na.rm = FALSE) } get_breakpoints_funs \u0026lt;- function(var, n_portfolios = 10) { # get relevant percentiles percentiles \u0026lt;- seq(0, 1, length.out = (n_portfolios + 1)) percentiles \u0026lt;- percentiles[percentiles \u0026gt; 0 \u0026amp; percentiles \u0026lt; 1] # construct set of named quantile functions percentiles_names \u0026lt;- map_chr(percentiles, ~paste0(rlang::quo_text(enquo(var)), \u0026quot;_q\u0026quot;, .x*100)) percentiles_funs \u0026lt;- map(percentiles, ~partial(quantile, probs = .x, na.rm = TRUE)) %\u0026gt;% set_names(nm = percentiles_names) return(percentiles_funs) } get_portfolio \u0026lt;- function(x, breakpoints) { portfolio \u0026lt;- as.integer(1 + findInterval(x, unlist(breakpoints))) return(portfolio) } The next function is an adapted version from the size post as it features the additional option of whether to use only NYSE stocks or all stocks to determine the breakpoints for portfolio sorts.\nconstruct_univariate_portfolios \u0026lt;- function(data, var, n_portfolios = 10, nyse_breakpoints = TRUE) { data \u0026lt;- data %\u0026gt;% filter(!is.na({{ var }})) # determine breakpoints based on NYSE stocks only or on all stocks if (nyse_breakpoints == TRUE) { data_quantiles \u0026lt;- data %\u0026gt;% filter(exchange == \u0026quot;NYSE\u0026quot;) %\u0026gt;% select(date, {{ var }}) } else { data_quantiles \u0026lt;- data %\u0026gt;% select(date, {{ var }}) } var_funs \u0026lt;- get_breakpoints_funs({{ var }}, n_portfolios) quantiles \u0026lt;- data_quantiles %\u0026gt;% group_by(date) %\u0026gt;% summarize_at(vars({{ var }}), lst(!!! var_funs)) %\u0026gt;% group_by(date) %\u0026gt;% nest() %\u0026gt;% rename(quantiles = data) # independently sort all stocks into portfolios based on breakpoints portfolios \u0026lt;- data %\u0026gt;% left_join(quantiles, by = \u0026quot;date\u0026quot;) %\u0026gt;% mutate(portfolio = map2_dbl({{ var }}, quantiles, get_portfolio)) %\u0026gt;% select(-quantiles) # compute average portfolio characteristics portfolios_ts \u0026lt;- portfolios %\u0026gt;% group_by(portfolio, date) %\u0026gt;% summarize(ret_ew = mean(ret_adj_excess_f1, na.rm = TRUE), ret_vw = weighted_mean(ret_adj_excess_f1, mktcap, na.rm = TRUE), ret_mkt = mean(mkt_ff_excess_f1, na.rm = TRUE), {{ var }} := mean({{ var }}, na.rm = TRUE), log_bm = mean(log_bm, na.rm = TRUE), mktcap = mean(mktcap, na.rm = TRUE), beta = mean(beta, na.rm = TRUE), n_stocks = n(), n_stocks_nyse = sum(exchange == \u0026quot;NYSE\u0026quot;, na.rm = TRUE)) %\u0026gt;% na.omit() %\u0026gt;% ungroup() ## long-short portfolio portfolios_ts_ls \u0026lt;- portfolios_ts %\u0026gt;% select(portfolio, date, ret_ew, ret_vw, ret_mkt) %\u0026gt;% filter(portfolio %in% c(max(portfolio), min(portfolio))) %\u0026gt;% pivot_wider(names_from = portfolio, values_from = c(ret_ew, ret_vw)) %\u0026gt;% mutate(ret_ew = .[[4]] - .[[3]], ret_vw = .[[6]] - .[[5]], portfolio = paste0(n_portfolios, \u0026quot;-1\u0026quot;)) %\u0026gt;% select(portfolio, date, ret_ew, ret_vw, ret_mkt) ## combine everything out \u0026lt;- portfolios_ts %\u0026gt;% mutate(portfolio = as.character(portfolio)) %\u0026gt;% bind_rows(portfolios_ts_ls) %\u0026gt;% mutate(portfolio = factor(portfolio, levels = c(as.character(seq(1, 10, 1)), paste0(n_portfolios, \u0026quot;-1\u0026quot;)))) return(out) } The table below provides the summary statistics for each portfolio using the above functions.\nportfolios_bm_nyse \u0026lt;- construct_univariate_portfolios(crsp, bm) portfolios_bm_all \u0026lt;- construct_univariate_portfolios(crsp, bm, nyse_breakpoints = FALSE) portfolios_table_nyse \u0026lt;- portfolios_bm_nyse %\u0026gt;% filter(portfolio != \u0026quot;10-1\u0026quot;) %\u0026gt;% group_by(portfolio) %\u0026gt;% summarize(bm_mean = mean(bm), log_bm_mean = mean(log_bm), mktcap_mean = mean(mktcap), n = mean(n_stocks), n_nyse = mean(n_stocks_nyse / n_stocks) * 100, beta = mean(beta)) %\u0026gt;% t() portfolios_table_nyse \u0026lt;- portfolios_table_nyse[-1, ] %\u0026gt;% as.numeric() %\u0026gt;% matrix(., ncol = 10) colnames(portfolios_table_nyse) \u0026lt;- seq(1, 10, 1) rownames(portfolios_table_nyse) \u0026lt;- c(\u0026quot;BM\u0026quot;, \u0026quot;Log(BM)\u0026quot;, \u0026quot;MktCap\u0026quot;, \u0026quot;Number of Stocks\u0026quot;, \u0026quot;% NYSE Stocks\u0026quot; , \u0026quot;Beta\u0026quot;) portfolios_table_all \u0026lt;- portfolios_bm_all %\u0026gt;% filter(portfolio != \u0026quot;10-1\u0026quot;) %\u0026gt;% group_by(portfolio) %\u0026gt;% summarize(bm_mean = mean(bm), log_bm_mean = mean(log_bm), mktcap_mean = mean(mktcap), n = mean(n_stocks), n_nyse = mean(n_stocks_nyse / n_stocks) * 100, beta = mean(beta)) %\u0026gt;% t() portfolios_table_all \u0026lt;- portfolios_table_all[-1, ] %\u0026gt;% as.numeric() %\u0026gt;% matrix(., ncol = 10) colnames(portfolios_table_all) \u0026lt;- seq(1, 10, 1) rownames(portfolios_table_all) \u0026lt;- c(\u0026quot;BM\u0026quot;, \u0026quot;Log(BM)\u0026quot;, \u0026quot;MktCap\u0026quot;, \u0026quot;Number of Stocks\u0026quot;, \u0026quot;% NYSE Stocks\u0026quot; , \u0026quot;Beta\u0026quot;) # combine to table and print html rbind(portfolios_table_nyse, portfolios_table_all) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;NYSE Breakpoints\u0026quot;, 1, 6) %\u0026gt;% pack_rows(\u0026quot;All Stocks Breakpoints\u0026quot;, 7, 12)    1  2  3  4  5  6  7  8  9  10     NYSE Breakpoints    BM  0.17  0.34  0.46  0.57  0.68  0.79  0.92  1.09  1.34  2.68    Log(BM)  -1.98  -1.15  -0.84  -0.63  -0.45  -0.29  -0.13  0.03  0.25  0.79    MktCap  3303.06  3008.75  2590.34  2031.89  1768.70  1595.45  1276.90  1132.78  915.48  618.58    Number of Stocks  555.87  381.03  344.00  326.27  323.71  325.71  330.87  350.88  378.26  473.68    % NYSE Stocks  30.89  38.77  42.16  43.75  44.36  44.58  43.75  41.80  39.25  32.43    Beta  1.07  0.97  0.90  0.86  0.82  0.78  0.74  0.70  0.68  0.63   All Stocks Breakpoints    BM  0.14  0.29  0.42  0.54  0.66  0.79  0.95  1.14  1.45  2.95    Log(BM)  -2.20  -1.31  -0.95  -0.69  -0.48  -0.29  -0.11  0.08  0.31  0.87    MktCap  3178.81  3136.80  2738.19  2105.06  1787.95  1479.09  1228.90  1063.14  880.77  603.66    Number of Stocks  379.38  378.87  379.04  378.82  378.81  379.19  378.90  378.85  378.96  379.47    % NYSE Stocks  28.50  35.84  40.43  42.75  43.66  44.23  41.99  40.58  37.64  31.47    Beta  1.08  1.00  0.92  0.88  0.83  0.79  0.74  0.70  0.66  0.63     By construction, the average BM ratio increases across portfolios. Market capitalization, betas and the number of stocks in each portfolio decrease across portfolios. These patterns are the same regardless of which breakpoints are used.\nGiven the portfolio returns, I want to evaluate whether a portfolio exhibits on average positive or negative excess returns. The following function estimates the mean excess return and CAPM alpha for each portfolio and computes corresponding Newey and West (1987) \\(t\\)-statistics (using six lags as common in the literature) testing the null hypothesis that the average portfolio excess return or CAPM alpha is zero. I just recycle the function I wrote in this post.\nestimate_portfolio_returns \u0026lt;- function(data, ret) { # estimate average returns per portfolio average_ret \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(rlang::as_name(enquo(ret)),\u0026quot; ~ 1\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6)))) %\u0026gt;% broom::tidy(model) %\u0026gt;% ungroup() %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # estimate capm alpha per portfolio average_capm_alpha \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(rlang::as_name(enquo(ret)),\u0026quot; ~ 1 + ret_mkt\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6))[1])) %\u0026gt;% broom::tidy(model)%\u0026gt;% ungroup() %\u0026gt;% filter(term == \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # construct output table out \u0026lt;- rbind(average_ret, average_capm_alpha) colnames(out) \u0026lt;-c(as.character(seq(1, 10, 1)), \u0026quot;10-1\u0026quot;) rownames(out) \u0026lt;- c(\u0026quot;Excess Return\u0026quot;, \u0026quot;t-Stat\u0026quot; , \u0026quot;CAPM Alpha\u0026quot;, \u0026quot;t-Stat\u0026quot;) return(out) } The following table provides the results of the univariate portfolio sorts using NYSE breakpoints.\nrbind(estimate_portfolio_returns(portfolios_bm_nyse, ret_ew), estimate_portfolio_returns(portfolios_bm_nyse, ret_vw)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;NYSE Breakpoints - Equal-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;NYSE Breakpoints - Value-Weighted Portfolio Returns\u0026quot;, 5, 8)     1  2  3  4  5  6  7  8  9  10  10-1     NYSE Breakpoints - Equal-Weighted Portfolio Returns    Excess Return  0.29  0.57  0.67  0.76  0.82  0.89  0.92  1.00  1.12  1.28  0.99    t-Stat  0.94  2.13  2.64  3.04  3.37  3.76  3.97  4.18  4.45  4.13  5.30    CAPM Alpha  -0.43  -0.08  0.05  0.17  0.25  0.35  0.41  0.49  0.59  0.73  1.16    t-Stat  -2.67  -0.64  0.38  1.40  2.03  2.85  3.27  3.78  4.14  3.78  5.95   NYSE Breakpoints - Value-Weighted Portfolio Returns    Excess Return  0.45  0.58  0.52  0.57  0.55  0.58  0.66  0.66  0.79  0.82  0.37    t-Stat  2.11  3.18  2.82  3.06  3.04  3.35  3.34  3.58  4.13  3.78  1.97    CAPM Alpha  -0.11  0.05  0.00  0.06  0.06  0.11  0.18  0.19  0.30  0.30  0.41    t-Stat  -1.24  0.92  0.02  0.78  0.67  1.52  1.80  1.73  2.76  2.33  2.08     Excess returns are monotonically increasing across BM portfolios, where stocks with high BM ratios seem to exhibit statistically significant excess returns. It is hence not surprising that the difference portfolio yields statistically significant positive excess returns. Adjusting the returns using the CAPM risk model has little effect on this result. Using value-weighted returns leads to a weaker relation between BM and expected returns, but the overall patterns are the same.\nThe next table provides the results of the univariate portfolio sorts using all stocks to calculate breakpoints.\nrbind(estimate_portfolio_returns(portfolios_bm_all, ret_ew), estimate_portfolio_returns(portfolios_bm_all, ret_vw)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;All Stocks Breakpoints - Equal-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;All Stocks Breakpoints - Value-Weighted Portfolio Returns\u0026quot;, 5, 8)     1  2  3  4  5  6  7  8  9  10  10-1     All Stocks Breakpoints - Equal-Weighted Portfolio Returns    Excess Return  0.17  0.46  0.59  0.71  0.81  0.90  0.95  1.04  1.15  1.33  1.16    t-Stat  0.52  1.57  2.31  2.85  3.29  3.76  3.96  4.24  4.52  4.17  5.69    CAPM Alpha  -0.57  -0.23  -0.04  0.11  0.23  0.36  0.42  0.52  0.63  0.78  1.35    t-Stat  -3.21  -1.64  -0.31  0.90  1.87  2.85  3.26  3.84  4.25  3.84  6.27   All Stocks Breakpoints - Value-Weighted Portfolio Returns    Excess Return  0.46  0.42  0.55  0.53  0.55  0.63  0.65  0.71  0.80  0.87  0.41    t-Stat  1.93  2.16  2.96  2.78  2.93  3.63  3.31  3.77  4.09  3.79  1.93    CAPM Alpha  -0.13  -0.13  0.03  0.01  0.04  0.16  0.16  0.24  0.29  0.34  0.47    t-Stat  -1.15  -1.95  0.49  0.14  0.50  2.10  1.56  2.28  2.45  2.46  2.10     Perhaps unsurprisingly, the patterns are essentially the same as in the case of using only NYSE stocks to calculate breakpoints.\nThis positive relation between the book-to-market ratio and expected stock returns is known as the value premium as stocks with high BM ratios (value stocks) tend to outperform stocks with low BM ratios (growth stocks).\n Regression Analysis As a last step, I analyze the relation between firm size and stock returns using Fama and MacBeth (1973) regression analysis. Each month, I perform a cross-sectional regression of one-month-ahead excess stock returns on the given measure of firm size. Time-series averages over these cross-sectional regressions then provide the desired results.\nAgain, I essentially recylce functions that I developed in an earlier post. Note that I depart from Bali et al. and condition on all three explanatory variables being defined in the data, regardless of which measure I am using. I do this to ensure comparability across results, i.e. I want to run the same set of regressions across different specifications. Otherwise I cannot evaluate whether adding a covariate, using a different measure or changes in sample composition lead to different results (or all of them).\nwinsorize \u0026lt;- function(x, cut = 0.005){ cut_point_top \u0026lt;- quantile(x, 1 - cut, na.rm = TRUE) cut_point_bottom \u0026lt;- quantile(x, cut, na.rm = TRUE) i \u0026lt;- which(x \u0026gt;= cut_point_top) x[i] \u0026lt;- cut_point_top j \u0026lt;- which(x \u0026lt;= cut_point_bottom) x[j] \u0026lt;- cut_point_bottom return(x) } fama_macbeth_regression \u0026lt;- function(data, model, cut = 0.005) { # prepare and winsorize data data_nested \u0026lt;- data %\u0026gt;% filter(!is.na(ret_adj_f1) \u0026amp; !is.na(size) \u0026amp; !is.na(beta) \u0026amp; !is.na(bm) \u0026amp; !is.na(log_bm)) %\u0026gt;% group_by(date) %\u0026gt;% mutate_at(vars(size, beta, bm, log_bm), ~winsorize(., cut = cut)) %\u0026gt;% nest() # perform cross-sectional regressions for each month cross_sectional_regs \u0026lt;- data_nested %\u0026gt;% mutate(model = map(data, ~lm(enexpr(model), data = .x))) %\u0026gt;% mutate(tidy = map(model, broom::tidy), glance = map(model, broom::glance), n = map(model, stats::nobs)) # extract average coefficient estimates fama_macbeth_coefs \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% summarize(coefficient = mean(estimate)) # compute newey-west standard errors of average coefficient estimates newey_west_std_errors \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% arrange(date) %\u0026gt;% group_modify(~enframe(sqrt(diag(sandwich::NeweyWest(lm(estimate ~ 1, data = .x), lag = 6))))) %\u0026gt;% select(term, nw_std_error = value) # put coefficient estimates and standard errors together and compute t-statistics fama_macbeth_coefs \u0026lt;- fama_macbeth_coefs %\u0026gt;% left_join(newey_west_std_errors, by = \u0026quot;term\u0026quot;) %\u0026gt;% mutate(nw_t_stat = coefficient / nw_std_error) %\u0026gt;% select(term, coefficient, nw_t_stat) %\u0026gt;% pivot_longer(cols = c(coefficient, nw_t_stat), names_to = \u0026quot;statistic\u0026quot;) %\u0026gt;% mutate(statistic = paste(term, statistic, sep = \u0026quot; \u0026quot;)) %\u0026gt;% select(-term) # extract average r-squared and average number of observations fama_macbeth_stats \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(c(glance, n)) %\u0026gt;% ungroup() %\u0026gt;% summarize(adj_r_squared = mean(adj.r.squared), n = mean(n)) %\u0026gt;% pivot_longer(c(adj_r_squared, n), names_to = \u0026quot;statistic\u0026quot;) # combine desired output and return results out \u0026lt;- rbind(fama_macbeth_coefs, fama_macbeth_stats) return(out) } The following table provides the regression results for various specifications.\nm1 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ bm) %\u0026gt;% rename(m1 = value) m2 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ bm + beta) %\u0026gt;% rename(m2 = value) m3 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ bm + size) %\u0026gt;% rename(m3 = value) m4 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ bm + beta + size) %\u0026gt;% rename(m4 = value) m5 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ log_bm) %\u0026gt;% rename(m5 = value) m6 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ log_bm + beta) %\u0026gt;% rename(m6 = value) m7 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ log_bm + size) %\u0026gt;% rename(m7 = value) m8 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ log_bm + beta + size) %\u0026gt;% rename(m8 = value) regression_table \u0026lt;- list(m1, m2, m3, m4, m5, m6, m7, m8) %\u0026gt;% reduce(full_join, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% right_join(tibble(statistic = c(\u0026quot;bm coefficient\u0026quot;, \u0026quot;bm nw_t_stat\u0026quot;, \u0026quot;log_bm coefficient\u0026quot;, \u0026quot;log_bm nw_t_stat\u0026quot;, \u0026quot;beta coefficient\u0026quot;, \u0026quot;beta nw_t_stat\u0026quot;, \u0026quot;size coefficient\u0026quot;, \u0026quot;size nw_t_stat\u0026quot;, \u0026quot;(Intercept) coefficient\u0026quot;, \u0026quot;(Intercept) nw_t_stat\u0026quot;, \u0026quot;adj_r_squared\u0026quot;, \u0026quot;n\u0026quot;)), by = \u0026quot;statistic\u0026quot;) regression_table %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;coefficient estimates\u0026quot;, 1, 10) %\u0026gt;% pack_rows(\u0026quot;summary statistics\u0026quot;, 11, 12)    statistic  m1  m2  m3  m4  m5  m6  m7  m8     coefficient estimates    bm coefficient  0.37  0.30  0.26  0.21        bm nw_t_stat  5.05  4.79  3.44  3.37        log_bm coefficient      0.38  0.34  0.28  0.25    log_bm nw_t_stat      5.73  5.77  3.75  4.08    beta coefficient   -0.21   -0.09   -0.16   -0.04    beta nw_t_stat   -1.69   -0.53   -1.39   -0.25    size coefficient    -0.11  -0.11    -0.10  -0.11    size nw_t_stat    -2.46  -2.02    -2.29  -2.03    (Intercept) coefficient  0.87  1.08  1.41  1.51  1.35  1.45  1.74  1.77    (Intercept) nw_t_stat  3.37  5.25  3.21  3.90  5.39  6.63  4.26  4.75   summary statistics    adj_r_squared  0.01  0.02  0.02  0.04  0.01  0.02  0.02  0.04    n  3730.47  3730.47  3730.47  3730.47  3730.47  3730.47  3730.47  3730.47     Across all specifications, I find a strong positive relation between BM and future excess returns, regardless of which set of controls is included or whether BM or its log-transformed version is used. Consistent with results from earlier posts, I detect no relation between beta and future stock returns and a strong negative relation between firm size and future stock returns. These results provide strong evidence of a value premium and no evidence that this premium is a manifestation of the relations between beta, market capitalization and future stock returns.\n ","date":1582675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582675200,"objectID":"6fde19af5a695c99b0a262cbe4cb0783","permalink":"https://christophscheuch.github.io/post/asset-pricing/value/","publishdate":"2020-02-26T00:00:00Z","relpermalink":"/post/asset-pricing/value/","section":"post","summary":"On the relation of book-to-market ratio and future stock returns","tags":["Asset Pricing"],"title":"Tidy Asset Pricing - Part IV: The Value Premium","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" In this note, I explore the relation between firm size and stock returns following the methodology outlined in Bali, Engle and Murray. In two earlier notes, I first prepared the CRSP sample in R and then estimated and analyzed market betas using the same data. I do not go into the theoretical foundations of the size effect, but rather focus on its estimation in a tidy manner. I just want to mention that the size effect refers to the observation that stocks with large market capitalization tend to have lower returns than stocks with small market capitalizations (e.g., Fama and French, 1992). The usual disclaimer applies, so the text below references an opinion and is for information purposes only and I do not intend to provide any investment advice. The code below replicates most of the results of Bali et al. up to a few basis points if I restrict my data to their sample period. If you spot any mistakes or want to share any suggestions for better implementation, just drop a comment below.\nI mainly use the following packages throughout this note:\nlibrary(tidyverse) library(lubridate) # for working with dates library(broom) # for tidying up estimation results library(kableExtra) # for nicer html tables Calculating Firm Size First, I load the data that I prepared earlier and add the market betas. I follow Bali et al. and use the betas estimated using daily return data and a twelve month estimation window as the measure for a stock’s covariance with the market.\ncrsp \u0026lt;- read_rds(\u0026quot;data/crsp.rds\u0026quot;) betas \u0026lt;- read_rds(\u0026quot;data/betas_daily.rds\u0026quot;) crsp \u0026lt;- crsp %\u0026gt;% left_join(betas %\u0026gt;% select(permno, date_start, beta = beta_12m), by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) # note: date_start refers to beginning of month Therer are two approaches to measure the market capitalization of a stock. The simplest approach is to take the share price and number of shares outstanding as of the end of each month for which the market capitalization is measured. This is the approach I used in the sample construction post to define the variable mktcap. Alternatively, Fama and French calculate market capitalization as of the last trading day of June in each year and hold the value constant for the months from June of the same year until May of the next year. The benefit of this approach is that the market capitalization measure does not vary with short-term movements in the stock price which may cause unwanted correlation with the stock returns. I implement the Fama-French approach below by defining a reference date for each month and then adding the June market cap as an additional column.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date), reference_date = if_else(month \u0026lt; 6, paste(year-1, 6, 30, sep = \u0026quot;-\u0026quot;), paste(year, 6, 30, sep = \u0026quot;-\u0026quot;))) crsp_mktcap_ff \u0026lt;- crsp %\u0026gt;% filter(month == 6) %\u0026gt;% select(permno, reference_date, mktcap_ff = mktcap) crsp \u0026lt;- crsp %\u0026gt;% left_join(crsp_mktcap_ff, by = c(\u0026quot;permno\u0026quot;, \u0026quot;reference_date\u0026quot;)) As it turns out, which approach I use has little impact on the results of the empirical analyses as the measures are highly correlated. While the timing of the calculation has a small effect, the distribution of measures might have a profound impact. As I show below, the distribution of market capitalization is highly skewed with a small number of stocks whose market capitalization is very large. For regression analyses, I hence use the log of market capitalization which I denote as size. For more meaningful interpretation of the market capitalization measures, I also calculate inflation-adjusted values (in terms of end of 2018 dollars).\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(mktcap_cpi = mktcap / cpi, mktcap_ff_cpi = mktcap_ff / cpi, size = log(mktcap), size_cpi = log(mktcap_cpi), size_ff = log(mktcap_ff), size_ff_cpi = log(mktcap_ff_cpi))  Summary Statistics I proceed to present summary statistics for the different measures of stock size using the full sample period from December 1925 until December 2018. I first compute corresponding summary statistics for each month and then average each summary statistics across all months. Note that I call the moments package to compute skewness and kurtosis.\n# compute summary statistics for each month crsp_ts \u0026lt;- crsp %\u0026gt;% select(date, mktcap, mktcap_ff:size_ff_cpi) %\u0026gt;% pivot_longer(cols = mktcap:size_ff_cpi, names_to = \u0026quot;measure\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(measure, date) %\u0026gt;% summarize(mean = mean(value), sd = sd(value), skew = moments::skewness(value), kurt = moments::kurtosis(value), min = min(value), q05 = quantile(value, 0.05), q25 = quantile(value, 0.25), q50 = quantile(value, 0.50), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95), max = max(value), n = n()) %\u0026gt;% ungroup() # average summary statistics across all months crsp_ts %\u0026gt;% select(-date) %\u0026gt;% group_by(measure) %\u0026gt;% summarize_all(list(mean)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   measure  mean  sd  skew  kurt  min  q05  q25  q50  q75  q95  max  n      mktcap  1099.38  5186.43  14.57  323.08  0.52  5.76  29.35  114.91  476.70  4169.19  140802.80  3164.76    mktcap_cpi  1974.27  8993.83  14.56  323.01  2.86  20.31  82.73  267.93  982.34  7382.44  238170.44  3164.34    mktcap_ff  1092.84  5091.25  14.34  311.15  0.67  6.17  30.18  116.82  479.06  4170.68  135991.85  3057.32    mktcap_ff_cpi  1964.99  8857.12  14.34  311.16  3.35  20.99  83.71  270.10  983.78  7377.41  231163.32  3056.89    size  3.94  1.81  0.30  2.99  -1.34  1.18  2.66  3.82  5.12  7.10  10.40  3164.76    size_cpi  5.47  1.81  0.30  2.99  0.19  2.71  4.19  5.35  6.65  8.63  11.93  3164.34    size_ff  3.97  1.79  0.33  2.98  -1.02  1.24  2.69  3.84  5.13  7.11  10.39  3057.32    size_ff_cpi  5.49  1.79  0.33  2.98  0.51  2.77  4.21  5.37  6.65  8.63  11.91  3056.89     To further investigate the distribution of market capitalization, I examine the percentage of total market cap that comprised very large stocks. The figure below plots the percentage of total stock market capitalization that is captured by the larges \\(x\\)% of stocks over the full sample period.\ncrsp_top \u0026lt;- crsp %\u0026gt;% select(permno, date, mktcap) %\u0026gt;% na.omit() %\u0026gt;% group_by(date) %\u0026gt;% mutate(top01 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.99), 1L, 0L), top05 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.95), 1L, 0L), top10 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.90), 1L, 0L), top25 = if_else(mktcap \u0026gt;= quantile(mktcap, 0.75), 1L, 0L)) %\u0026gt;% summarize(total = sum(mktcap), top01 = sum(mktcap[top01 == 1]) / total, top05 = sum(mktcap[top05 == 1]) / total, top10 = sum(mktcap[top10 == 1]) / total, top25 = sum(mktcap[top25 == 1]) / total) %\u0026gt;% select(-total) %\u0026gt;% pivot_longer(cols = top01:top25, names_to = \u0026quot;group\u0026quot;) crsp_top %\u0026gt;% mutate(group = case_when(group == \u0026quot;top01\u0026quot; ~ \u0026quot;Largest 1% of Stocks\u0026quot;, group == \u0026quot;top05\u0026quot; ~ \u0026quot;Largest 5% of Stocks\u0026quot;, group == \u0026quot;top10\u0026quot; ~ \u0026quot;Largest 10% of Stocks\u0026quot;, group == \u0026quot;top25\u0026quot; ~ \u0026quot;Largest 25% of Stocks\u0026quot;), group = factor(group, levels = c(\u0026quot;Largest 1% of Stocks\u0026quot;, \u0026quot;Largest 5% of Stocks\u0026quot;, \u0026quot;Largest 10% of Stocks\u0026quot;, \u0026quot;Largest 25% of Stocks\u0026quot;))) %\u0026gt;% ggplot(aes(x = date, y = value, group = group)) + geom_line(aes(color = group, linetype = group)) + scale_y_continuous(labels = scales::percent, breaks = scales::pretty_breaks(), limits = c(0, 1)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Percentage of Total Market Capitalization\u0026quot;, fill = \u0026quot;\u0026quot;, color = \u0026quot;\u0026quot;, linetype = \u0026quot;\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic()  Correlations The next table presents the time-series averages of the monthly cross-sectional (Pearson product-moment) correlations between different measures of market capitalization.\ncor_matrix \u0026lt;- crsp %\u0026gt;% select(mktcap, size, mktcap_ff, size_ff, beta) %\u0026gt;% cor(use = \u0026quot;complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) cor_matrix[upper.tri(cor_matrix, diag = TRUE)] \u0026lt;- NA cor_matrix[-1, -5] %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))    mktcap  size  mktcap_ff  size_ff      size  0.34       mktcap_ff  0.98  0.33      size_ff  0.34  0.98  0.34     beta  0.05  0.28  0.05  0.28     Indeed, the different approaches are highly correlated, mostly because the total market capitalization of a firm in most cases changes very little over the course of a year (see next section). The correlation between beta and size indicates that larger stocks tend to have higher market betas.\n Persistence Next, I turn to the persistence analysis, i.e. the correlation of each variable with its lagged values over time. To compute these correlations, I define the following function where I use the fairly new curly-curly operator which simplifies writing functions around tidyverse pipelines.\ncompute_persistence \u0026lt;- function(var, tau) { dates \u0026lt;- crsp %\u0026gt;% distinct(date) %\u0026gt;% arrange(date) %\u0026gt;% mutate(date_lag = lag(date, tau)) correlation \u0026lt;- crsp %\u0026gt;% select(permno, date, {{ var }}) %\u0026gt;% left_join(dates, by = \u0026quot;date\u0026quot;) %\u0026gt;% left_join(crsp %\u0026gt;% select(permno, date, {{ var }}) %\u0026gt;% rename(\u0026quot;{{ var }}_lag\u0026quot; := {{ var }}), by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_lag\u0026quot;=\u0026quot;date\u0026quot;)) %\u0026gt;% select(contains(rlang::quo_text(enquo(var)))) %\u0026gt;% cor(use = \u0026quot;pairwise.complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;) return(correlation[1, 2]) } persistence \u0026lt;- tribble(~tau, ~size, ~size_ff, 1, compute_persistence(size, 1), compute_persistence(size_ff, 1), 3, compute_persistence(size, 3), compute_persistence(size_ff, 3), 6, compute_persistence(size, 6), compute_persistence(size_ff, 6), 12, compute_persistence(size, 12), compute_persistence(size_ff, 12), 24, compute_persistence(size, 24), compute_persistence(size_ff, 24), 36, compute_persistence(size, 36), compute_persistence(size_ff, 36), 48, compute_persistence(size, 48), compute_persistence(size_ff, 48), 60, compute_persistence(size, 60), compute_persistence(size_ff, 60), 120, compute_persistence(size, 120), compute_persistence(size_ff, 120)) The following table provides the results of the persistence analysis. Indeed, the size measures exhibit a high persistence even up to 10 years. As the Fama-French measure is only updated every June, it mechanically exhibits a slightly higher persistence, but the impact is in fact minimal compared to the monthly updated variable.\npersistence %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   tau  size  size_ff      1  1.00  1.00    3  0.99  0.99    6  0.98  0.98    12  0.96  0.97    24  0.93  0.94    36  0.91  0.92    48  0.89  0.90    60  0.88  0.89    120  0.83  0.84      Portfolio Analysis To analyze the cross-sectional relation between market capitalization and future stock returns, I start with univariate portfolio analysis. The idea is to sort all stocks into portfolios based on quantiles calculated using NYSE stocks only and then calculate equal-weighted and value-weighted average returns for each portfolio.\nTurns out that I need to adjust the basic weighted.mean function to take out observations with missing weights because it would otherwise return missing values. This is important whenever a stock’s return is defined while the market capitalization is not (e.g. due to missing number of shares outstanding).\nweighted_mean \u0026lt;- function(x, w, ..., na.rm = FALSE){ if(na.rm){ x1 \u0026lt;- x[!is.na(x) \u0026amp; !is.na(w)] w \u0026lt;- w[!is.na(x) \u0026amp; !is.na(w)] x \u0026lt;- x1 } weighted.mean(x, w, ..., na.rm = FALSE) } The next function I use is a helper function that returns a list of quantile functions that I can throw into the summarize command to get a list of breakpoints for any given number of portfolios. The code snippet below is a modified version of the code found here. Most importantly, the function uses purrr:partial to fil in some arguments to the quantile functions. I apply the function to return breakpoints for 3 market cap portfolios as an example.\nget_breakpoints_funs \u0026lt;- function(var, n_portfolios = 10) { # get relevant percentiles percentiles \u0026lt;- seq(0, 1, length.out = (n_portfolios + 1)) percentiles \u0026lt;- percentiles[percentiles \u0026gt; 0 \u0026amp; percentiles \u0026lt; 1] # construct set of named quantile functions percentiles_names \u0026lt;- map_chr(percentiles, ~paste0(rlang::quo_text(enquo(var)), \u0026quot;_q\u0026quot;, .x*100)) percentiles_funs \u0026lt;- map(percentiles, ~partial(quantile, probs = .x, na.rm = TRUE)) %\u0026gt;% set_names(nm = percentiles_names) return(percentiles_funs) } get_breakpoints_funs(mktcap, n_portfolios = 3) ## $mktcap_q33.3333333333333 ## \u0026lt;partialised\u0026gt; ## function (...) ## quantile(probs = .x, na.rm = TRUE, ...) ## ## $mktcap_q66.6666666666667 ## \u0026lt;partialised\u0026gt; ## function (...) ## quantile(probs = .x, na.rm = TRUE, ...) Given a list of breakpoints, I want to sort stocks into the corresponding portfolio. This turns out to be a bit tricky if I want to write a function that does the sorting for a flexible number of portfolios. Fortunately, my brilliant colleague Stefan Voigt pointed out the findInterval function to me.\nget_portfolio \u0026lt;- function(x, breakpoints) { portfolio \u0026lt;- as.integer(1 + findInterval(x, unlist(breakpoints))) return(portfolio) } The next function finally constructs univariate portfolios for any sorting variable using NYSE breakpoints. Note that the rationale behind using only NYSE stocks to calculate quantiles is that for a large portio of the sample period, NYSE stocks tended to be much larger than stocks listed on AMEX or NASDAQ. If the breakpoints were calculated using all stocks in the CRSP sample, the breakpoints would effectively just separate NYSE stocks from all other stocks. However, it does not ensure an equal number of NYSE stocks in each portfolios, as I show below. Again, I leverage the new curly-curly operator and associated naming capabilities.\nconstruct_univariate_portfolios \u0026lt;- function(data, var, n_portfolios = 10) { # keep only observations where the sorting variable is defined data \u0026lt;- data %\u0026gt;% filter(!is.na({{ var }})) # determine breakpoints based on NYSE stocks only data_nyse \u0026lt;- data %\u0026gt;% filter(exchange == \u0026quot;NYSE\u0026quot;) %\u0026gt;% select(date, {{ var }}) var_funs \u0026lt;- get_breakpoints_funs({{ var }}, n_portfolios) quantiles \u0026lt;- data_nyse %\u0026gt;% group_by(date) %\u0026gt;% summarize_at(vars({{ var }}), lst(!!! var_funs)) %\u0026gt;% group_by(date) %\u0026gt;% nest() %\u0026gt;% rename(quantiles = data) # independently sort all stocks into portfolios based on NYSE breakpoints portfolios \u0026lt;- data %\u0026gt;% left_join(quantiles, by = \u0026quot;date\u0026quot;) %\u0026gt;% mutate(portfolio = map2_dbl({{ var }}, quantiles, get_portfolio)) %\u0026gt;% select(-quantiles) # compute average portfolio characteristics portfolios_ts \u0026lt;- portfolios %\u0026gt;% group_by(portfolio, date) %\u0026gt;% summarize(ret_ew = mean(ret_adj_excess_f1, na.rm = TRUE), ret_vw = weighted_mean(ret_adj_excess_f1, mktcap, na.rm = TRUE), ret_mkt = mean(mkt_ff_excess_f1, na.rm = TRUE), \u0026quot;{{ var }}_mean\u0026quot; := mean({{ var }}, na.rm = TRUE), \u0026quot;{{ var }}_sum\u0026quot; := sum({{ var }}, na.rm = TRUE), beta = mean(beta, na.rm = TRUE), n_stocks = n(), n_stocks_nyse = sum(exchange == \u0026quot;NYSE\u0026quot;, na.rm = TRUE)) %\u0026gt;% na.omit() %\u0026gt;% ungroup() ## long-short portfolio portfolios_ts_ls \u0026lt;- portfolios_ts %\u0026gt;% select(portfolio, date, ret_ew, ret_vw, ret_mkt) %\u0026gt;% filter(portfolio %in% c(max(portfolio), min(portfolio))) %\u0026gt;% pivot_wider(names_from = portfolio, values_from = c(ret_ew, ret_vw)) %\u0026gt;% mutate(ret_ew = .[[4]] - .[[3]], ret_vw = .[[6]] - .[[5]], portfolio = paste0(n_portfolios, \u0026quot;-1\u0026quot;)) %\u0026gt;% select(portfolio, date, ret_ew, ret_vw, ret_mkt) ## combine everything out \u0026lt;- portfolios_ts %\u0026gt;% mutate(portfolio = as.character(portfolio)) %\u0026gt;% bind_rows(portfolios_ts_ls) %\u0026gt;% mutate(portfolio = factor(portfolio, levels = c(as.character(seq(1, 10, 1)), paste0(n_portfolios, \u0026quot;-1\u0026quot;)))) return(out) } portfolios_mktcap \u0026lt;- construct_univariate_portfolios(crsp, mktcap) portfolios_mktcap_ff \u0026lt;- construct_univariate_portfolios(crsp, mktcap_ff) The table below provides the summary statistics for each portfolio.\n# compute market cap totals mktcap_totals \u0026lt;- crsp %\u0026gt;% group_by(date) %\u0026gt;% summarize(mktcap_total = sum(mktcap, na.rm = TRUE), mktcap_ff_total = sum(mktcap_ff, na.rm = TRUE)) # portfolio characteristics portfolios_mktcap_table \u0026lt;- portfolios_mktcap %\u0026gt;% filter(portfolio != \u0026quot;10-1\u0026quot;) %\u0026gt;% left_join(mktcap_totals, by = \u0026quot;date\u0026quot;) %\u0026gt;% group_by(portfolio) %\u0026gt;% summarize(mktcap_mean = mean(mktcap_mean), mktcap_share = mean(mktcap_sum / mktcap_total) * 100, n = mean(n_stocks), n_nyse = mean(n_stocks_nyse / n_stocks) * 100, beta = mean(beta)) %\u0026gt;% t() portfolios_mktcap_table \u0026lt;- portfolios_mktcap_table[-1, ] %\u0026gt;% as.numeric() %\u0026gt;% matrix(., ncol = 10) colnames(portfolios_mktcap_table) \u0026lt;- seq(1, 10, 1) rownames(portfolios_mktcap_table) \u0026lt;- c(\u0026quot;Market Cap (in $B)\u0026quot;, \u0026quot;% of Total Market Cap\u0026quot;, \u0026quot;Number of Stocks\u0026quot;, \u0026quot;% NYSE Stocks\u0026quot; , \u0026quot;Beta\u0026quot;) # repeat for fama-french measure portfolios_mktcap_ff_table \u0026lt;- portfolios_mktcap_ff %\u0026gt;% filter(portfolio != \u0026quot;10-1\u0026quot;) %\u0026gt;% left_join(mktcap_totals, by = \u0026quot;date\u0026quot;) %\u0026gt;% group_by(portfolio) %\u0026gt;% summarize(mktcap_ff_mean = mean(mktcap_ff_mean), mktcap_ff_share = mean(mktcap_ff_sum / mktcap_ff_total) * 100, n = mean(n_stocks), n_nyse = mean(n_stocks_nyse / n_stocks) * 100, beta = mean(beta)) %\u0026gt;% t() portfolios_mktcap_ff_table \u0026lt;- portfolios_mktcap_ff_table[-1, ] %\u0026gt;% as.numeric() %\u0026gt;% matrix(., ncol = 10) colnames(portfolios_mktcap_ff_table) \u0026lt;- seq(1, 10, 1) rownames(portfolios_mktcap_ff_table) \u0026lt;- c(\u0026quot;Market Cap (in $B)\u0026quot;, \u0026quot;% of Total Market Cap\u0026quot;, \u0026quot;Number of Stocks\u0026quot;, \u0026quot;% NYSE Stocks\u0026quot; , \u0026quot;Beta\u0026quot;) # combine to table and print html rbind(portfolios_mktcap_table, portfolios_mktcap_ff_table) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Turan-Engle-Murray)\u0026quot;, 1, 5) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Fama-French)\u0026quot;, 6, 10)    1  2  3  4  5  6  7  8  9  10     Market Cap (Turan-Engle-Murray)    Market Cap (in $B)  25.43  100.24  181.84  288.66  439.11  656.88  1000.36  1678.64  3353.75  16646.88    % of Total Market Cap  1.13  1.16  1.44  1.89  2.51  3.33  4.77  7.52  13.76  62.48    Number of Stocks  1417.84  387.86  266.60  217.65  186.92  162.65  150.42  141.72  134.15  129.71    % NYSE Stocks  43.28  56.31  64.16  69.63  74.92  81.14  85.10  88.52  92.13  94.54    Beta  0.76  0.99  1.02  1.02  1.02  1.01  1.02  1.01  1.00  1.01   Market Cap (Fama-French)    Market Cap (in $B)  26.30  102.10  182.39  288.65  435.70  648.76  985.41  1654.13  3306.93  16286.20    % of Total Market Cap  1.14  1.15  1.43  1.88  2.48  3.31  4.75  7.54  13.81  62.52    Number of Stocks  1360.21  363.88  255.62  209.18  180.04  158.78  146.58  139.24  131.72  127.46    % NYSE Stocks  44.19  57.60  65.18  70.70  76.04  81.80  85.78  88.71  92.37  94.76    Beta  0.76  1.00  1.02  1.03  1.03  1.01  1.02  1.01  1.00  1.01     The numbers are again very similar for both measures. The table illustrates the usage of NYSE breakpoints as the share of NYSE stocks increases across portfolios. The table also shows that the portfolio of largest firms takes up more than 60% of total market capitalization although it contains on average only about 130 firms.\nGiven the portfolio returns, I want to evaluate whether a portfolio exhibits on average positive or negative excess returns. The following function estimates the mean excess return and CAPM alpha for each portfolio and computes corresponding Newey and West (1987) \\(t\\)-statistics (using six lags as common in the literature) testing the null hypothesis that the average portfolio excess return or CAPM alpha is zero. I just recycle the function I wrote in this post.\nestimate_portfolio_returns \u0026lt;- function(data, ret) { # estimate average returns per portfolio average_ret \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(rlang::as_name(enquo(ret)),\u0026quot; ~ 1\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6)))) %\u0026gt;% broom::tidy(model) %\u0026gt;% ungroup() %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # estimate capm alpha per portfolio average_capm_alpha \u0026lt;- data %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% do(model = lm(paste0(rlang::as_name(enquo(ret)),\u0026quot; ~ 1 + ret_mkt\u0026quot;), data = .)) %\u0026gt;% mutate(nw_stderror = sqrt(diag(sandwich::NeweyWest(model, lag = 6))[1])) %\u0026gt;% broom::tidy(model)%\u0026gt;% ungroup() %\u0026gt;% filter(term == \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # construct output table out \u0026lt;- rbind(average_ret, average_capm_alpha) colnames(out) \u0026lt;-c(as.character(seq(1, 10, 1)), \u0026quot;10-1\u0026quot;) rownames(out) \u0026lt;- c(\u0026quot;Excess Return\u0026quot;, \u0026quot;t-Stat\u0026quot; , \u0026quot;CAPM Alpha\u0026quot;, \u0026quot;t-Stat\u0026quot;) return(out) } rbind(estimate_portfolio_returns(portfolios_mktcap, ret_ew), estimate_portfolio_returns(portfolios_mktcap_ff, ret_ew), estimate_portfolio_returns(portfolios_mktcap, ret_vw), estimate_portfolio_returns(portfolios_mktcap_ff, ret_vw)) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Turan-Engle-Murray) - Equal-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Fama-French) - Equal-Weighted Portfolio Returns\u0026quot;, 5, 8)%\u0026gt;% pack_rows(\u0026quot;Market Cap (Turan-Engle-Murray) - Value-Weighted Portfolio Returns\u0026quot;, 9, 12) %\u0026gt;% pack_rows(\u0026quot;Market Cap (Fama-French) - Value-Weighted Portfolio Returns\u0026quot;, 13, 16)    1  2  3  4  5  6  7  8  9  10  10-1     Market Cap (Turan-Engle-Murray) - Equal-Weighted Portfolio Returns    Excess Return  1.62  1.01  0.95  0.92  0.88  0.84  0.79  0.81  0.72  0.60  -1.01    t-Stat  3.90  3.41  3.35  3.72  3.73  3.81  3.69  4.03  3.84  3.62  -3.25    CAPM Alpha  0.65  0.08  0.04  0.07  0.06  0.05  0.02  0.07  0.02  -0.04  -0.68    t-Stat  2.89  0.65  0.40  0.84  0.75  0.85  0.42  1.59  0.65  -1.67  -2.90   Market Cap (Fama-French) - Equal-Weighted Portfolio Returns    Excess Return  1.46  1.04  1.00  0.92  0.89  0.88  0.82  0.79  0.73  0.61  -0.85    t-Stat  3.90  3.32  3.62  3.62  3.75  3.85  3.79  3.90  3.88  3.59  -3.16    CAPM Alpha  0.52  0.10  0.11  0.07  0.06  0.07  0.05  0.05  0.03  -0.04  -0.56    t-Stat  2.65  0.75  1.05  0.75  0.82  1.04  0.82  0.95  0.66  -1.45  -2.65   Market Cap (Turan-Engle-Murray) - Value-Weighted Portfolio Returns    Excess Return  1.34  1.00  0.96  0.92  0.88  0.84  0.80  0.81  0.71  0.60  -0.74    t-Stat  3.40  3.41  3.38  3.73  3.74  3.80  3.72  4.08  3.83  3.73  -2.50    CAPM Alpha  0.36  0.08  0.05  0.07  0.06  0.05  0.03  0.07  0.02  -0.01  -0.37    t-Stat  1.83  0.62  0.51  0.86  0.75  0.83  0.51  1.69  0.68  -0.41  -1.73   Market Cap (Fama-French) - Value-Weighted Portfolio Returns    Excess Return  1.08  0.94  0.97  0.90  0.88  0.87  0.81  0.79  0.72  0.60  -0.48    t-Stat  3.25  3.21  3.62  3.70  3.79  3.91  3.85  4.01  3.90  3.75  -2.08    CAPM Alpha  0.15  0.03  0.10  0.08  0.07  0.08  0.06  0.07  0.03  -0.01  -0.16    t-Stat  0.93  0.26  0.99  0.86  0.97  1.23  1.05  1.44  0.91  -0.34  -0.88     The results show that all porfolios deliver positive excess returns, but the returns decrease as firm size increases. As a consequence, the portfolio that is long the largest firms and short the smallest firms yields statistically significant negative excess returns. Even though the CAPM alpha of each portfolio is statistically indistinguishable from zero, the long-short portfolio still yields negative alphas (except for the Fama-French value-weighted portfolios). Taken together, the results of the univariate portfolio analysis indicate a negative relation between firm market capitalization and future stock returns.\n Regression Analysis As a last step, I analyze the relation between firm size and stock returns using Fama and MacBeth (1973) regression analysis. Each month, I perform a cross-sectional regression of one-month-ahead excess stock returns on the given measure of firm size. Time-series averages over these cross-sectional regressions then provide the desired results.\nAgain, I essentially recylce functions that I developed in an earlier post. Note that I depart from Bali et al. and condition on all three explanatory variables being defined in the data, regardless of which measure I am using. I do this to ensure comparability across results, i.e. I want to run the same set of regressions across different specifications. Otherwise I cannot evaluate whether adding a covariate, using a different measure or changes in sample composition lead to different results (or all of them).\nwinsorize \u0026lt;- function(x, cut = 0.005){ cut_point_top \u0026lt;- quantile(x, 1 - cut, na.rm = TRUE) cut_point_bottom \u0026lt;- quantile(x, cut, na.rm = TRUE) i \u0026lt;- which(x \u0026gt;= cut_point_top) x[i] \u0026lt;- cut_point_top j \u0026lt;- which(x \u0026lt;= cut_point_bottom) x[j] \u0026lt;- cut_point_bottom return(x) } fama_macbeth_regression \u0026lt;- function(data, model, cut = 0.005) { # prepare and winsorize data data_nested \u0026lt;- data %\u0026gt;% filter(!is.na(ret_adj_f1) \u0026amp; !is.na(size) \u0026amp; !is.na(size_ff) \u0026amp; !is.na(beta)) %\u0026gt;% group_by(date) %\u0026gt;% mutate_at(vars(size, size_ff, beta), ~winsorize(., cut = cut)) %\u0026gt;% nest() # perform cross-sectional regressions for each month cross_sectional_regs \u0026lt;- data_nested %\u0026gt;% mutate(model = map(data, ~lm(enexpr(model), data = .x))) %\u0026gt;% mutate(tidy = map(model, broom::tidy), glance = map(model, broom::glance), n = map(model, stats::nobs)) # extract average coefficient estimates fama_macbeth_coefs \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% summarize(coefficient = mean(estimate)) # compute newey-west standard errors of average coefficient estimates newey_west_std_errors \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% arrange(date) %\u0026gt;% group_modify(~enframe(sqrt(diag(sandwich::NeweyWest(lm(estimate ~ 1, data = .x), lag = 6))))) %\u0026gt;% select(term, nw_std_error = value) # put coefficient estimates and standard errors together and compute t-statistics fama_macbeth_coefs \u0026lt;- fama_macbeth_coefs %\u0026gt;% left_join(newey_west_std_errors, by = \u0026quot;term\u0026quot;) %\u0026gt;% mutate(nw_t_stat = coefficient / nw_std_error) %\u0026gt;% select(term, coefficient, nw_t_stat) %\u0026gt;% pivot_longer(cols = c(coefficient, nw_t_stat), names_to = \u0026quot;statistic\u0026quot;) %\u0026gt;% mutate(statistic = paste(term, statistic, sep = \u0026quot; \u0026quot;)) %\u0026gt;% select(-term) # extract average r-squared and average number of observations fama_macbeth_stats \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(c(glance, n)) %\u0026gt;% ungroup() %\u0026gt;% summarize(adj_r_squared = mean(adj.r.squared), n = mean(n)) %\u0026gt;% pivot_longer(c(adj_r_squared, n), names_to = \u0026quot;statistic\u0026quot;) # combine desired output and return results out \u0026lt;- rbind(fama_macbeth_coefs, fama_macbeth_stats) return(out) } m1 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size) %\u0026gt;% rename(m1 = value) m2 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size + beta) %\u0026gt;% rename(m2 = value) m3 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size_ff) %\u0026gt;% rename(m3 = value) m4 \u0026lt;- fama_macbeth_regression(crsp, ret_adj_f1 ~ size_ff + beta) %\u0026gt;% rename(m4 = value) The following table provides the regression results for various specifications.\nregression_table \u0026lt;- m1 %\u0026gt;% full_join(m2, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% full_join(m3, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% full_join(m4, by = \u0026quot;statistic\u0026quot;) %\u0026gt;% right_join(tibble(statistic = c(\u0026quot;(Intercept) coefficient\u0026quot;, \u0026quot;(Intercept) nw_t_stat\u0026quot;, \u0026quot;size coefficient\u0026quot;, \u0026quot;size nw_t_stat\u0026quot;, \u0026quot;size_ff coefficient\u0026quot;, \u0026quot;size_ff nw_t_stat\u0026quot;, \u0026quot;beta coefficient\u0026quot;, \u0026quot;beta nw_t_stat\u0026quot;, \u0026quot;adj_r_squared\u0026quot;, \u0026quot;n\u0026quot;)), by = \u0026quot;statistic\u0026quot;) colnames(regression_table) \u0026lt;- c(\u0026quot;statistic\u0026quot;, \u0026quot;m1\u0026quot;, \u0026quot;m2\u0026quot;, \u0026quot;m3\u0026quot;, \u0026quot;m4\u0026quot;) regression_table[, 1] \u0026lt;- c(\u0026quot;intercept\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;size\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;size_ff\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;adj_r_squared\u0026quot;, \u0026quot;n\u0026quot;) regression_table %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   statistic  m1  m2  m3  m4      intercept  1.73  1.76  1.64  1.70    t-stat  4.74  5.64  4.68  5.59    size  -0.17  -0.17      t-stat  -3.71  -3.17      size_ff    -0.15  -0.14    t-stat    -3.41  -2.86    beta   -0.09   -0.14    t-stat   -0.67   -1.05    adj_r_squared  0.02  0.05  0.02  0.04    n  2944.50  2944.50  2944.50  2944.50     The regression results show a statisticall significant negative relation between firm size and abnormal stock returns. The results also show that controling for market beta neither yields a statistically significant relation to stock returns, nor does it impact the coefficient estimate on firm size. This result contradicts the fundamental prediction of the CAPM that market beta is the only determinant of cross-sectional variation in expected returns. Also note that the explanatory power of size and beta is still very low (see, e.g., Elton, 1999).\n ","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582243200,"objectID":"7a00c6e36a3c165aa4ab709edb6cd6df","permalink":"https://christophscheuch.github.io/post/asset-pricing/size/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/post/asset-pricing/size/","section":"post","summary":"On the relation of firm size and future stock returns","tags":["Asset Pricing"],"title":"Tidy Asset Pricing - Part III: Size and Stock Returns","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" This note proposes an estimation and evaluation procedure for market betas following Bali, Engle and Murray (BEM). I do not go into details about the foundations of market beta, but simply refer to any treatment of the Capital Asset Pricing Model (CAPM). However, I provide details about all the functions that I use to compute my results. As the post introduces a number of approaches of BEM, there are also many functions to discuss. Subsequent blog posts then recycle most of the functions.\nThis note has two objectives: the first one is to implement several approaches to estimate a stock’s beta in a tidy manner and to empirically examine these measures. The second objective is to analyze the cross-sectional relation between stock returns and market beta. The following disclaimer applies: the text below references an opinion and is for information purposes only and I do not intend to provide any investment advice.\nI mainly use the following packages throughout this note:\nlibrary(tidyverse) # for grammar library(scales) # for nicer axes of figures library(viridis) # for nicer colors in figures library(slider) # for rolling window operations library(moments) # for skewness and kurtosis library(corrr) # for correlations in table format library(kableExtra) # for nicer html tables library(broom) # for converting statistical objects to tidy tibbles library(sandwich) # for Newey-West standard errors First, I load the monthly and daily return data that I prepared in an earlier post and compute the stock and market excess returns that are required for the beta estimation procedure.\ntbl.crsp_monthly \u0026lt;- read_rds(\u0026quot;data/crsp_monthly.rds\u0026quot;) %\u0026gt;% mutate(ret = ret_adj - rf_ff, mkt = mkt_ff - rf_ff) %\u0026gt;% select(permno, month, mkt, ret) tbl.crsp_daily \u0026lt;- read_rds(\u0026quot;data/crsp_daily.rds\u0026quot;) %\u0026gt;% mutate(ret = ret - rf_ff, mkt = mkt_ff - rf_ff) %\u0026gt;% select(permno, month, mkt, ret) Estimation According to the CAPM, cross-sectional variation in expected asset returns should be a function of the covariance between the return of the asset and the return on the market portfolio – the beta. To estimate stock-specific betas, I have to regress excess stock returns on excess returns of the market portfolio. Throughout the note, I use the market excess return and risk-free rate provided in Ken French’s data library. The estimation procedure is based on a rolling window estimation where I may use either monthly or daily returns and different window lengths. I follow BEM and examine nine different combinations of estimation periods and data frequencies. For monthly return data, I use excess return observations over the past 1, 2, 3 and 5 years, requiring at least 10, 20, 24 and 24 valid return observations, respectively. For daily data measures, I use period lengths of 1, 3, 6, 12 and 24 months and require 15, 50, 100, 200 and 450 days of valid return data. The function below implements these requirements for a given estimation window and data frequency and returns the estimated beta.\nfun.capm_regression \u0026lt;- function(x, window, freq) { # Drop missing values x \u0026lt;- na.omit(x) # Determine minimum number of observations depending on window size and data frequency if (freq == \u0026quot;monthly\u0026quot;) { if (window == 12) {check \u0026lt;- 10} if (window == 24) {check \u0026lt;- 20} if (window == 36) {check \u0026lt;- 24} if (window == 60) {check \u0026lt;- 24} } if (freq == \u0026quot;daily\u0026quot;) { if (window == 1) {check \u0026lt;- 15} if (window == 3) {check \u0026lt;- 50} if (window == 6) {check \u0026lt;- 100} if (window == 12) {check \u0026lt;- 200} if (window == 24) {check \u0026lt;- 450} } # Check if minimum number of observations is satisfied if (nrow(x) \u0026lt; check) { return(as.numeric(NA)) } else { reg \u0026lt;- lm(ret ~ mkt, data = x) return(as.numeric(reg$coefficients[2])) } } To perform the rolling window estimation, I use the slider package of Davis Vaughan, which provides a family of sliding window functions similar to purrr::map(). Most importantly, the slide_period function is able to handle months in its window input in a straight-forward manner. I thus avoid using any time-series package (e.g., zoo) and converting the data to fit the package functions, but rather stay in the tibble world.\nfun.rolling_capm_regression \u0026lt;- function(x, window, freq) { # Nested tibbles of daily data need to be binded if (freq == \u0026quot;daily\u0026quot;) { x \u0026lt;- bind_rows(x) } out \u0026lt;- slide_period_vec(.x = x, .i = x$month, .period = \u0026quot;month\u0026quot;, .f = function(x) {fun.capm_regression(x, window, freq)}, .before = (window-1), .complete = FALSE) return(out) } I use the above function to compute several different variants of the beta estimator put forward by BEM. First, I use monthly data to estimate betas with different years as input. However, the estimation below takes up a considerable amount of time when I use the whole CRSP sample (around 2.5 hours). In principle, I could considerably speed up the estimation procedure by parallelizing it across stocks (e.g. using multidplyr or putting the data into some SQL table). However, for the sake of exposition, I just provide the code for the joint estimation below.\ntbl.betas_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% group_by(permno) %\u0026gt;% arrange(month) %\u0026gt;% mutate(beta_1y = fun.rolling_capm_regression(cur_data(), window = 12, freq = \u0026quot;monthly\u0026quot;), beta_2y = fun.rolling_capm_regression(cur_data(), window = 24, freq = \u0026quot;monthly\u0026quot;), beta_3y = fun.rolling_capm_regression(cur_data(), window = 36, freq = \u0026quot;monthly\u0026quot;), beta_5y = fun.rolling_capm_regression(cur_data(), window = 60, freq = \u0026quot;monthly\u0026quot;)) %\u0026gt;% ungroup() %\u0026gt;% select(-c(mkt, ret)) %\u0026gt;% arrange(permno, month) Next, I use the same function to perform rolling window estimation on the daily return data. To use the above function, I first nest the data by month. the function then loads the relevant months of daily data and binds them together. Note that that the computation now takes about 3.5 hours for the whole CRSP sample, but is again in principle parallelizable.\ntbl.crsp_daily_nested \u0026lt;- tbl.crsp_daily %\u0026gt;% group_by(permno, month_group = month) %\u0026gt;% nest(daily_data = c(month, mkt, ret)) %\u0026gt;% ungroup() tbl.betas_daily \u0026lt;- tbl.crsp_daily_nested %\u0026gt;% group_by(permno) %\u0026gt;% arrange(month_group) %\u0026gt;% mutate(beta_1m = fun.rolling_capm_regression(daily_data, window = 1, freq = \u0026quot;daily\u0026quot;), beta_3m = fun.rolling_capm_regression(daily_data, window = 3, freq = \u0026quot;daily\u0026quot;), beta_6m = fun.rolling_capm_regression(daily_data, window = 6, freq = \u0026quot;daily\u0026quot;), beta_12m = fun.rolling_capm_regression(daily_data, window = 12, freq = \u0026quot;daily\u0026quot;), beta_24m = fun.rolling_capm_regression(daily_data, window = 24, freq = \u0026quot;daily\u0026quot;) ) %\u0026gt;% ungroup() %\u0026gt;% select(permno, month = month_group, beta_1m, beta_3m, beta_6m, beta_12m, beta_24m) %\u0026gt;% arrange(permno, month)  Descriptives Let us add the beta estimates to the monthly CRSP sample and look at some descriptives.\ntbl.betas \u0026lt;- read_rds(\u0026quot;data/crsp_monthly.rds\u0026quot;) %\u0026gt;% select(permno, month, ret_adj_excess_f1, mkt_ff_excess_f1, mktcap, mktcap_cpi, exchange) %\u0026gt;% left_join(tbl.betas_monthly, by = c(\u0026quot;permno\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% left_join(tbl.betas_daily, by = c(\u0026quot;permno\u0026quot;, \u0026quot;month\u0026quot;)) # Define vector of betas for convenience (e.g. factor labels and maps) vec.betas \u0026lt;- c(\u0026quot;beta_1y\u0026quot;, \u0026quot;beta_2y\u0026quot;, \u0026quot;beta_3y\u0026quot;, \u0026quot;beta_5y\u0026quot;, \u0026quot;beta_1m\u0026quot;, \u0026quot;beta_3m\u0026quot;, \u0026quot;beta_6m\u0026quot;, \u0026quot;beta_12m\u0026quot;, \u0026quot;beta_24m\u0026quot;) I first check whether our estimation procedure actually produce beta estimates. The figure below shows the share of stocks with a corresponding beta estimate for each month since 1927. I can see drops in 1962 and 1972 because that is when AMEX and NASDAQ enter the sample, respectively. As a new batch of stocks enters the sample, I cannot compute beta estimates until there is sufficient data available for these stocks. Finally, note that the estimates based on daily data drop in 2019 since I currently don’t have access to more recent daily CRSP data.\ntbl.coverage \u0026lt;- tbl.betas %\u0026gt;% select(month, permno, contains(\u0026quot;beta\u0026quot;)) %\u0026gt;% pivot_longer(cols = contains(\u0026quot;beta\u0026quot;), names_to = \u0026quot;measure\u0026quot;, values_to = \u0026quot;estimate\u0026quot;) %\u0026gt;% group_by(month, measure) %\u0026gt;% summarize(share = n_distinct(permno[!is.na(estimate)]) / n_distinct(permno)) %\u0026gt;% ungroup() fig.coverage \u0026lt;- tbl.coverage %\u0026gt;% mutate(measure = factor(measure, levels = vec.betas)) %\u0026gt;% ggplot(aes(x = month, y = share, color = measure, linetype = measure)) + geom_line() + labs(x = NULL, y = NULL, color = NULL, linetype = NULL, title = \u0026quot;Monthly share of stocks with beta estimate\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = percent, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()   As a next evaluation, I compute average summary statistics across all months, just like BEM.\n# Compute summary statistics for each month tbl.betas_ts \u0026lt;- tbl.betas %\u0026gt;% select(month, contains(\u0026quot;beta\u0026quot;)) %\u0026gt;% pivot_longer(cols = contains(\u0026quot;beta\u0026quot;), names_to = \u0026quot;measure\u0026quot;, values_drop_na = TRUE) %\u0026gt;% group_by(measure, month) %\u0026gt;% summarize(mean = mean(value), sd = sd(value), skew = skewness(value), kurt = kurtosis(value), min = min(value), q05 = quantile(value, 0.05), q25 = quantile(value, 0.25), q50 = quantile(value, 0.50), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95), max = max(value), n = n()) %\u0026gt;% ungroup() # Compute average summary statistics across months tab.betas_summary \u0026lt;- tbl.betas_ts %\u0026gt;% select(-month) %\u0026gt;% group_by(measure) %\u0026gt;% summarize_all(list(~mean(., na.rm = TRUE))) %\u0026gt;% mutate(measure = factor(measure, levels = vec.betas)) %\u0026gt;% arrange(measure) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) tab.betas_summary   measure  mean  sd  skew  kurt  min  q05  q25  q50  q75  q95  max  n      beta_1y  1.15  1.17  0.71  21.40  -7.60  -0.41  0.48  1.05  1.73  3.06  12.36  2972.14    beta_2y  1.16  0.82  0.63  9.21  -3.62  0.05  0.63  1.08  1.60  2.56  7.29  2611.63    beta_3y  1.16  0.70  0.64  6.31  -2.09  0.20  0.69  1.09  1.54  2.37  5.71  2255.79    beta_5y  1.15  0.61  0.56  6.91  -1.81  0.30  0.73  1.09  1.50  2.21  5.01  1975.01    beta_1m  0.86  1.26  0.27  27.77  -11.01  -0.81  0.20  0.77  1.47  2.83  12.76  3144.43    beta_3m  0.88  0.82  0.34  12.71  -4.92  -0.23  0.36  0.81  1.34  2.26  6.83  3112.60    beta_6m  0.89  0.68  0.39  7.08  -2.84  -0.05  0.43  0.83  1.30  2.08  4.89  3069.49    beta_12m  0.90  0.60  0.44  4.22  -1.61  0.06  0.47  0.84  1.27  1.96  3.68  2972.62    beta_24m  0.90  0.54  0.45  3.40  -0.91  0.13  0.50  0.85  1.25  1.87  3.03  2734.99     There appears to be more stability in the distribution of estimated betas as I extend the estimation period. This might indicate that longer estimation periods yield more precise beta estimates. However, the average number of observations mechanically decreases as the estimation windows increase. Moreover, estimates based on monthly observations are on average higher than estimates based on daily returns.\n Correlations The next table presents the time-series averages of the monthly cross-sectional (Pearson product-moment) correlations between different measures of market beta.\nfun.compute_correlations \u0026lt;- function(data, use = \u0026quot;complete.obs\u0026quot;, method = \u0026quot;pearson\u0026quot;, diagonal = 1, quiet = TRUE, shave_upper = TRUE) { data %\u0026gt;% correlate(use = use, method = method, diagonal = diagonal, quiet = quiet) %\u0026gt;% shave(., upper = shave_upper) } tbl.correlations \u0026lt;- tbl.betas %\u0026gt;% na.omit() %\u0026gt;% select(month, contains(\u0026quot;beta\u0026quot;)) %\u0026gt;% group_by(month) %\u0026gt;% nest(betas = contains(\u0026quot;beta\u0026quot;)) %\u0026gt;% mutate(correlations = map(betas, fun.compute_correlations)) %\u0026gt;% select(-betas) %\u0026gt;% unnest(correlations) %\u0026gt;% rename(measure = term) %\u0026gt;% mutate(measure = factor(measure, levels = vec.betas)) %\u0026gt;% group_by(measure) %\u0026gt;% summarize_at(vars(contains(\u0026quot;beta\u0026quot;)), list(~mean(., na.rm = TRUE))) tab.correlations \u0026lt;- tbl.correlations %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) tab.correlations   measure  beta_1y  beta_2y  beta_3y  beta_5y  beta_1m  beta_3m  beta_6m  beta_12m  beta_24m      beta_1y  1.00            beta_2y  0.76  1.00           beta_3y  0.66  0.87  1.00          beta_5y  0.58  0.78  0.88  1.00         beta_1m  0.23  0.27  0.29  0.29  1.00        beta_3m  0.36  0.41  0.42  0.43  0.69  1.00       beta_6m  0.42  0.48  0.50  0.50  0.59  0.85  1.00      beta_12m  0.46  0.54  0.56  0.56  0.53  0.75  0.89  1.00     beta_24m  0.42  0.57  0.61  0.62  0.48  0.68  0.80  0.91  1     Correlations range from as low as 0.22 between the one-year measure based on monthly data and the one-month measure based on daily data to as high as 0.91 between the one-year and two-year measure based on daily data. Again, correlations seem to increase as the length of the measurement period increases.\n Persistence If I use a measure of a stock’s beta in further analyses, I want it to be fairly stable over time. To examine whether this is the case, I calculate the persistence of each measure as the time-series averages of monthly cross-sectional correlations between month \\(t\\) and month \\(t+\\tau\\). Following BEM, I winsorize each measure at the 0.5% level on a monthly basis to mitigate the impact of outliers. The table below presents the persistence measures for lags one, three, six, 12, 24, 36, 48, 60 and 120 months.\nfun.winsorize \u0026lt;- function(x, cut = 0.005){ cut_point_top \u0026lt;- quantile(x, 1 - cut, na.rm = TRUE) cut_point_bottom \u0026lt;- quantile(x, cut, na.rm = TRUE) i \u0026lt;- which(x \u0026gt;= cut_point_top) x[i] \u0026lt;- cut_point_top j \u0026lt;- which(x \u0026lt;= cut_point_bottom) x[j] \u0026lt;- cut_point_bottom return(x) } fun.compute_persistence \u0026lt;- function(data, var, tau, cut = 0.005){ # Initialize output out \u0026lt;- list() for (t in 1:length(tau)) { # Create a table with lagged dates dates \u0026lt;- data %\u0026gt;% distinct(month) %\u0026gt;% mutate(month_lag = lag(month, tau[t])) # Winsorize the data data_winsorized \u0026lt;- data %\u0026gt;% filter(!is.na({{var}})) %\u0026gt;% group_by(month) %\u0026gt;% mutate(beta = fun.winsorize({{var}}, cut = cut)) %\u0026gt;% ungroup() %\u0026gt;% select(permno, month, beta) # Compute correlation to lagged value correlation \u0026lt;- data_winsorized %\u0026gt;% left_join(dates, by = \u0026quot;month\u0026quot;) %\u0026gt;% left_join(data_winsorized %\u0026gt;% select(permno, month, beta_lag = beta), by = c(\u0026quot;permno\u0026quot;, \u0026quot;month_lag\u0026quot; = \u0026quot;month\u0026quot;)) %\u0026gt;% select(month, beta, beta_lag) %\u0026gt;% na.omit() %\u0026gt;% nest(betas = c(beta, beta_lag)) %\u0026gt;% mutate(correlations = map(betas, fun.compute_correlations)) %\u0026gt;% unnest(correlations) %\u0026gt;% filter(term == \u0026quot;beta_lag\u0026quot;) %\u0026gt;% summarise(mean = mean(beta, na.rm = TRUE)) out[[t]] \u0026lt;- tibble(tau = tau[t], {{var}} := as.numeric(correlation)) } return(bind_rows(out)) } vec.tau \u0026lt;- c(1, 3, 6, 12, 24, 36, 48, 60, 120) fun.compute_persistence(tbl.betas, beta_1y, vec.tau) lst.persistence \u0026lt;- vec.betas %\u0026gt;% map(~fun.compute_persistence(tbl.betas, !!sym(.), vec.tau)) %\u0026gt;% set_names(vec.betas) tab.persistence \u0026lt;- lst.persistence %\u0026gt;% reduce(left_join) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) tab.persistence   tau  beta_1y  beta_2y  beta_3y  beta_5y  beta_1m  beta_3m  beta_6m  beta_12m  beta_24m      1  0.92  0.97  0.98  0.99  0.28  0.82  0.93  0.98  0.99    3  0.78  0.91  0.95  0.97  0.26  0.48  0.79  0.92  0.97    6  0.58  0.82  0.90  0.95  0.24  0.45  0.60  0.84  0.93    12  0.24  0.66  0.80  0.90  0.23  0.42  0.55  0.67  0.86    24  0.22  0.37  0.61  0.80  0.20  0.37  0.49  0.60  0.71    36  0.20  0.35  0.44  0.70  0.19  0.35  0.45  0.55  0.65    48  0.19  0.32  0.41  0.60  0.18  0.33  0.42  0.52  0.60    60  0.18  0.30  0.39  0.49  0.17  0.31  0.40  0.49  0.57    120  0.14  0.24  0.31  0.38  0.14  0.25  0.32  0.39  0.44     Of course, entries that correspond to lags for which the measurement periods overlap mechanically exhibit high persistence. The results indicate that the calculation of beta using short measurement periods is very noisy. Moreover, as the length of the measurement window increases, so does the persistence of beta calculated from daily return data. The measures based on monthly return data exhibit similar persistence patterns. In summary, the results indicate that the use of longer measurement periods results in more accurate measures of beta and that beta seems to be more accurately measured using daily return data.\n Portfolio Analysis Next, I turn to the relation of beta and stock returns. The fundamental prediction of the CAPM is that there is a positive relation between market beta and expected stock returns where the slope of this relation is the market risk premium. I should hence expect to find a positive cross-sectional association between beta and future excess returns. However, as I show below, contrary to this prediction, analyses fail to detect any strong relation or even exhibit a negative relation. This result is typically viewed as one of the most persistent empirical anomalies in all of empirical asset pricing.\nThe construction of univariate portfolios relies on a couple of helper functions. First, I need to introduce a wrapper for the basic weighted mean function to also exclude observations with missing weights.\nfun.weighted_mean \u0026lt;- function(x, w, ..., na.rm = FALSE){ if(na.rm){ x1 \u0026lt;- x[!is.na(x) \u0026amp; !is.na(w)] w \u0026lt;- w[!is.na(x) \u0026amp; !is.na(w)] x \u0026lt;- x1 } weighted.mean(x, w, ..., na.rm = FALSE) } The next function I introduce is a helper function that returns a list of quantile functions that I can throw into the summarize command to get a list of breakpoints for any given number of target portfolios. The code snippet below is a modified version of the code found here. Most importantly, the function uses purrr:partial to fill in some arguments to the quantile functions.\nfun.get_breakpoints_functions \u0026lt;- function(var, n_portfolios = 10) { # Get relevant percentiles percentiles \u0026lt;- seq(0, 1, length.out = (n_portfolios + 1)) percentiles \u0026lt;- percentiles[percentiles \u0026gt; 0 \u0026amp; percentiles \u0026lt; 1] # Construct set of named quantile functions percentiles_names \u0026lt;- map_chr(percentiles, ~str_c(rlang::quo_text(enquo(var)), \u0026quot;_q\u0026quot;, .x*100)) percentiles_funs \u0026lt;- map(percentiles, ~partial(quantile, probs = .x, na.rm = TRUE)) %\u0026gt;% set_names(nm = percentiles_names) return(percentiles_funs) } Given a list of breakpoints, I want to sort stocks into the corresponding portfolio. This turns out to be a bit tricky if I want to write a function that does the sorting for a flexible number of portfolios. Fortunately, my brilliant colleague Stefan Voigt pointed out the findInterval function to me.\nfun.get_portfolio \u0026lt;- function(x, breakpoints) { portfolio \u0026lt;- as.integer(1 + findInterval(x, unlist(breakpoints))) return(portfolio) } The next function finally constructs univariate portfolios for any sorting variable, an abitrary number of portfolios and using all stocks or only NYSE stocks to compute breakpoints. The rationale behind using only NYSE stocks to calculate quantiles is that for a large portion of the sample period, these stocks tend to be much larger than stocks listed on AMEX or NASDAQ. If the breakpoints are calculated using all stocks in the CRSP sample, the breakpoints would effectively just separate NYSE stocks from all other stocks. However, it does not ensure an equal number of NYSE stocks in each portfolios, as I demonstrate in later posts.\nFollowin BEM, I sort stocks into 10 portfolios ranked by the value of the beta estimate. The breakpoints for each portfolio are based on deciles constructed using all stocks. In addition to the 10 decile portfolios, the function below also adds the 10-1 portfolio. The function also computes a set of average stock characteristics for each portfolio, which becomes relevant in the other blog posts.\nfun.construct_univariate_portfolios \u0026lt;- function(data, var, n_portfolios = 10, nyse_breakpoints = FALSE) { # Keep only observations where sorting variable is defined data \u0026lt;- data %\u0026gt;% filter(!is.na({{var}})) # Determine breakpoints based on NYSE stocks only or on all stocks if (nyse_breakpoints == TRUE) { data_quantiles \u0026lt;- data %\u0026gt;% filter(exchange == \u0026quot;NYSE\u0026quot;) %\u0026gt;% select(month, {{var}}) } else { data_quantiles \u0026lt;- data %\u0026gt;% select(month, {{var}}) } # Compute quantiles var_funs \u0026lt;- fun.get_breakpoints_functions({{var}}, n_portfolios) quantiles \u0026lt;- data_quantiles %\u0026gt;% group_by(month) %\u0026gt;% summarize_at(vars({{var}}), lst(!!!var_funs)) %\u0026gt;% group_by(month) %\u0026gt;% nest(quantiles = -month) # Independently sort all stocks into portfolios based on breakpoints portfolios \u0026lt;- data %\u0026gt;% left_join(quantiles, by = \u0026quot;month\u0026quot;) %\u0026gt;% mutate(portfolio = map2_dbl({{var}}, quantiles, fun.get_portfolio)) %\u0026gt;% select(-quantiles) # Compute average portfolio characteristics portfolios_ts \u0026lt;- portfolios %\u0026gt;% group_by(portfolio, month) %\u0026gt;% summarize(ret_ew = mean(ret_adj_excess_f1, na.rm = TRUE), ret_vw = fun.weighted_mean(ret_adj_excess_f1, mktcap, na.rm = TRUE), ret_mkt = mean(mkt_ff_excess_f1, na.rm = TRUE), {{var}} := mean({{var}}, na.rm = TRUE), mktcap = mean(mktcap, na.rm = TRUE), mktcap_cpi_all = mean(mktcap_cpi, na.rm = TRUE), mktcap_cpi_nyse = mean(mktcap_cpi[exchange == \u0026quot;NYSE\u0026quot;], na.rm = TRUE), n_stocks = n(), n_stocks_nyse = sum(exchange == \u0026quot;NYSE\u0026quot;, na.rm = TRUE)) %\u0026gt;% ungroup() # Construct long-short portfolio portfolios_ts_ls \u0026lt;- portfolios_ts %\u0026gt;% select(portfolio, month, ret_ew, ret_vw, ret_mkt) %\u0026gt;% filter(portfolio %in% c(max(portfolio), min(portfolio))) %\u0026gt;% pivot_wider(names_from = portfolio, values_from = c(ret_ew, ret_vw)) %\u0026gt;% mutate(ret_ew = .[[4]] - .[[3]], ret_vw = .[[6]] - .[[5]], portfolio = paste0(n_portfolios, \u0026quot;-1\u0026quot;)) %\u0026gt;% select(portfolio, month, ret_ew, ret_vw, ret_mkt) # Combine everything out \u0026lt;- portfolios_ts %\u0026gt;% mutate(portfolio = as.character(portfolio)) %\u0026gt;% bind_rows(portfolios_ts_ls) %\u0026gt;% mutate(portfolio = factor(portfolio, levels = c(as.character(seq(1, n_portfolios, 1)), str_c(n_portfolios, \u0026quot;-1\u0026quot;)))) return(out) } lst.portfolios \u0026lt;- vec.betas %\u0026gt;% map(~fun.construct_univariate_portfolios(tbl.betas, !!sym(.))) %\u0026gt;% set_names(vec.betas) Given the portfolio returns, I want to evaluate whether the portfolio exhibit on average positive or negative excess returns. The following function estimates the mean excess return and CAPM alpha for each portfolio and computes corresponding Newey and West (1987) \\(t\\)-statistics (using six lags as common in the literature) testing the null hypothesis that the average portfolio excess return or CAPM alpha is zero.\nfun.estimate_portfolio_returns \u0026lt;- function(data, ret, n_portfolios = 10, lag = 6) { # Compute average returns per portfolio average_ret \u0026lt;- data %\u0026gt;% select(portfolio, month, ret = {{ret}}) %\u0026gt;% group_by(portfolio) %\u0026gt;% nest(data = c(month, ret)) %\u0026gt;% mutate(model = map(data, ~lm(\u0026quot;ret ~ 1\u0026quot;, data = .)), nw_stderror = map_dbl(model, ~sqrt(diag(sandwich::NeweyWest(., lag = lag)))), model = map(model, tidy)) %\u0026gt;% unnest(model) %\u0026gt;% ungroup() %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # Estimate capm alpha per portfolio average_capm_alpha \u0026lt;- data %\u0026gt;% select(portfolio, month, ret = {{ret}}, ret_mkt) %\u0026gt;% group_by(portfolio) %\u0026gt;% nest(data = c(month, ret, ret_mkt)) %\u0026gt;% mutate(model = map(data, ~lm(\u0026quot;ret ~ 1 + ret_mkt\u0026quot;, data = .)), nw_stderror = map_dbl(model, ~sqrt(diag(sandwich::NeweyWest(., lag = lag))[1])), model = map(model, tidy)) %\u0026gt;% unnest(model) %\u0026gt;% ungroup() %\u0026gt;% filter(term == \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% mutate(nw_tstat = estimate / nw_stderror) %\u0026gt;% select(estimate, nw_tstat) %\u0026gt;% t() # Construct output table out \u0026lt;- rbind(average_ret, average_capm_alpha) colnames(out) \u0026lt;-c(as.character(seq(1, n_portfolios, 1)), str_c(n_portfolios, \u0026quot;-1\u0026quot;)) rownames(out) \u0026lt;- c(\u0026quot;Excess Return\u0026quot;, \u0026quot;t-Stat\u0026quot;, \u0026quot;CAPM Alpha\u0026quot;, \u0026quot;t-Stat\u0026quot;) return(out) } Let us first apply the function to equal-weighted porfolios.\ntab.portfolio_returns_ew \u0026lt;- lst.portfolios %\u0026gt;% map(~fun.estimate_portfolio_returns(., ret_ew)) %\u0026gt;% reduce(rbind) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Year (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 2 Years (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 5, 8) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Years (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 9, 12) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 5 Years (Monthly Data) - Equal-Weighted Portfolio Returns\u0026quot;, 13, 16) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Month (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 17, 20) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Months (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 21, 24) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 6 Months (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 25, 28) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 12 Months (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 29, 32) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 24 Months (Daily Data) - Equal-Weighted Portfolio Returns\u0026quot;, 33, 36) tab.portfolio_returns_ew    1  2  3  4  5  6  7  8  9  10  10-1     Estimation Period: 1 Year (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  0.84  0.73  0.80  0.88  0.93  0.91  0.99  1.01  0.93  0.81  -0.03    t-Stat  2.98  3.08  3.32  3.63  3.65  3.28  3.45  3.32  2.93  2.24  -0.18    CAPM Alpha  0.24  0.15  0.15  0.19  0.17  0.08  0.09  0.04  -0.10  -0.32  -0.56    t-Stat  1.37  1.07  1.13  1.52  1.34  0.61  0.69  0.25  -0.62  -1.57  -3.63   Estimation Period: 2 Years (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  0.77  0.78  0.81  0.90  0.93  0.97  1.05  0.98  0.93  0.84  0.08    t-Stat  3.19  3.27  3.33  3.68  3.47  3.39  3.64  3.15  2.78  2.25  0.35    CAPM Alpha  0.30  0.24  0.21  0.23  0.19  0.18  0.19  0.05  -0.07  -0.25  -0.55    t-Stat  1.88  1.83  1.60  1.82  1.42  1.27  1.29  0.32  -0.43  -1.19  -3.15   Estimation Period: 3 Years (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  0.81  0.88  0.91  1.00  1.01  1.05  1.10  1.09  1.02  0.97  0.16    t-Stat  3.57  3.89  4.01  3.99  3.83  3.72  3.88  3.47  3.07  2.60  0.73    CAPM Alpha  0.33  0.30  0.27  0.27  0.22  0.18  0.18  0.07  -0.06  -0.23  -0.56    t-Stat  2.14  2.45  2.21  2.05  1.58  1.29  1.20  0.48  -0.32  -1.09  -3.13   Estimation Period: 5 Years (Monthly Data) - Equal-Weighted Portfolio Returns    Excess Return  0.78  0.91  0.94  0.97  1.05  1.13  1.03  1.05  1.01  0.93  0.15    t-Stat  3.64  4.41  4.20  4.30  4.20  4.14  3.79  3.54  3.20  2.57  0.67    CAPM Alpha  0.29  0.34  0.26  0.21  0.22  0.20  0.05  -0.01  -0.13  -0.36  -0.65    t-Stat  2.02  2.51  2.08  1.63  1.61  1.44  0.34  -0.05  -0.77  -1.86  -3.94   Estimation Period: 1 Month (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  1.04  0.94  0.92  1.02  0.98  1.01  0.99  0.96  0.93  0.74  -0.30    t-Stat  3.52  4.31  4.38  4.73  4.34  4.19  3.97  3.63  3.11  2.14  -2.17    CAPM Alpha  0.30  0.31  0.29  0.34  0.23  0.22  0.15  0.05  -0.10  -0.38  -0.69    t-Stat  1.90  3.25  3.39  3.98  2.80  2.46  1.58  0.48  -0.86  -2.34  -5.40   Estimation Period: 3 Months (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  1.06  0.95  1.00  0.95  1.03  0.98  1.00  0.93  0.90  0.71  -0.35    t-Stat  3.74  4.52  4.64  4.37  4.52  4.04  3.92  3.48  3.01  1.99  -2.07    CAPM Alpha  0.39  0.39  0.38  0.26  0.29  0.18  0.14  0.01  -0.14  -0.48  -0.87    t-Stat  2.71  3.88  4.10  2.90  3.35  2.03  1.45  0.10  -1.20  -2.91  -5.85   Estimation Period: 6 Months (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  1.13  0.98  0.98  0.97  1.05  1.02  0.97  0.92  0.89  0.73  -0.40    t-Stat  4.20  4.61  4.45  4.54  4.49  4.16  3.73  3.38  2.92  2.07  -2.22    CAPM Alpha  0.52  0.41  0.35  0.30  0.29  0.20  0.09  -0.02  -0.17  -0.48  -1.00    t-Stat  3.74  4.20  3.58  3.31  3.15  2.14  0.93  -0.15  -1.47  -2.96  -6.46   Estimation Period: 12 Months (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  1.13  1.03  1.02  1.02  1.04  1.05  1.01  0.90  0.87  0.72  -0.42    t-Stat  4.67  4.79  4.71  4.34  4.35  4.13  3.85  3.27  2.87  2.09  -2.15    CAPM Alpha  0.60  0.47  0.40  0.33  0.28  0.22  0.13  -0.02  -0.18  -0.48  -1.08    t-Stat  4.77  4.60  4.03  3.39  2.86  2.32  1.27  -0.21  -1.46  -3.15  -6.64   Estimation Period: 24 Months (Daily Data) - Equal-Weighted Portfolio Returns    Excess Return  1.14  1.01  1.08  1.05  1.08  1.02  1.03  0.97  0.92  0.69  -0.45    t-Stat  4.96  4.69  4.72  4.49  4.29  4.05  3.97  3.56  3.02  2.04  -2.17    CAPM Alpha  0.64  0.47  0.45  0.36  0.32  0.22  0.18  0.07  -0.10  -0.48  -1.12    t-Stat  5.62  4.68  4.20  3.73  3.14  2.19  1.76  0.61  -0.83  -3.15  -6.60     There is neither a clear pattern across the portfolios in average excess returns, nor does the 10-1 portfolio yield statistically significant excess returns. However, the CAPM alphas exhibit a decreasing pattern across portfolios and a statistically significant negative coefficient on the 10-1 portfolio for all beta estimates. This result is actually not surprising since the risk-adjustment using the CAPM model adjusts returns for the sensitivity to the market factor which is the same factor used to calculate beta. Stocks in the highest decile hence mechanically exhibit a higher sensitivity than stocks in the low decile. As a result, the 10-1 portfolio has a high sensitivity to the market portfolio. Since the market factor generates positive returns in the long run and the 10-1 portfolio has a positive sensitivity, the effect of the risk-adjustment is negative.\nNext, let us repeat the analysis using value-weighted returns for each portfolio.\ntab.portfolio_returns_vw \u0026lt;- lst.portfolios %\u0026gt;% map(~fun.estimate_portfolio_returns(., ret_vw)) %\u0026gt;% reduce(rbind) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Year (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 1, 4) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 2 Years (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 5, 8) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Years (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 9, 12) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 5 Years (Monthly Data) - Value-Weighted Portfolio Returns\u0026quot;, 13, 16) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 1 Month (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 17, 20) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 3 Months (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 21, 24) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 6 Months (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 25, 28) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 12 Months (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 29, 32) %\u0026gt;% pack_rows(\u0026quot;Estimation Period: 24 Months (Daily Data) - Value-Weighted Portfolio Returns\u0026quot;, 33, 36) tab.portfolio_returns_vw    1  2  3  4  5  6  7  8  9  10  10-1     Estimation Period: 1 Year (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.36  0.45  0.54  0.65  0.72  0.66  0.65  0.68  0.75  0.66  0.30    t-Stat  1.90  2.40  3.04  3.59  3.52  3.14  2.84  2.73  2.74  2.06  1.36    CAPM Alpha  -0.10  -0.05  -0.01  0.05  0.04  -0.07  -0.15  -0.22  -0.22  -0.45  -0.35    t-Stat  -0.80  -0.35  -0.06  0.55  0.39  -0.67  -1.42  -1.92  -1.64  -2.59  -1.86   Estimation Period: 2 Years (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.43  0.43  0.56  0.72  0.66  0.68  0.67  0.63  0.68  0.66  0.23    t-Stat  2.40  2.42  3.11  3.77  3.03  3.16  2.89  2.50  2.40  1.98  0.92    CAPM Alpha  0.04  -0.01  0.05  0.14  0.00  -0.02  -0.12  -0.22  -0.27  -0.44  -0.49    t-Stat  0.33  -0.09  0.41  1.32  -0.04  -0.18  -1.04  -1.86  -1.97  -2.69  -2.59   Estimation Period: 3 Years (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.42  0.50  0.58  0.69  0.68  0.65  0.70  0.69  0.70  0.75  0.33    t-Stat  2.45  2.78  3.35  3.54  3.18  3.02  2.90  2.69  2.44  2.23  1.26    CAPM Alpha  0.04  0.03  0.03  0.06  -0.04  -0.12  -0.15  -0.24  -0.33  -0.47  -0.50    t-Stat  0.26  0.19  0.26  0.53  -0.38  -1.04  -1.23  -2.03  -2.44  -2.91  -2.69   Estimation Period: 5 Years (Monthly Data) - Value-Weighted Portfolio Returns    Excess Return  0.52  0.59  0.72  0.67  0.76  0.69  0.70  0.76  0.65  0.78  0.26    t-Stat  3.21  3.57  4.27  3.52  3.67  3.30  3.14  3.13  2.34  2.49  1.03    CAPM Alpha  0.14  0.09  0.12  -0.03  -0.02  -0.14  -0.22  -0.23  -0.45  -0.47  -0.60    t-Stat  1.00  0.64  1.04  -0.25  -0.17  -1.22  -1.80  -1.85  -3.31  -3.01  -3.30   Estimation Period: 1 Month (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.58  0.56  0.62  0.70  0.67  0.70  0.77  0.79  0.75  0.54  -0.05    t-Stat  2.95  3.62  4.24  4.55  4.29  4.12  4.22  3.95  3.19  1.78  -0.25    CAPM Alpha  0.01  0.12  0.13  0.18  0.10  0.07  0.08  0.02  -0.15  -0.52  -0.53    t-Stat  0.12  1.64  2.30  3.22  2.23  1.35  1.45  0.38  -2.19  -3.92  -3.22   Estimation Period: 3 Months (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.47  0.63  0.60  0.66  0.68  0.75  0.78  0.78  0.71  0.55  0.08    t-Stat  2.31  4.35  4.17  4.46  4.42  4.25  4.17  3.87  2.89  1.84  0.37    CAPM Alpha  0.01  0.22  0.16  0.14  0.12  0.11  0.09  0.02  -0.19  -0.53  -0.54    t-Stat  0.07  2.94  2.38  2.36  2.36  1.95  1.56  0.28  -2.40  -4.18  -2.82   Estimation Period: 6 Months (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.59  0.68  0.58  0.62  0.72  0.70  0.77  0.74  0.68  0.64  0.05    t-Stat  3.35  4.71  4.07  4.30  4.66  4.10  4.31  3.57  2.80  2.15  0.25    CAPM Alpha  0.17  0.27  0.15  0.14  0.15  0.07  0.08  -0.02  -0.22  -0.46  -0.63    t-Stat  1.58  3.37  2.13  2.22  2.76  1.25  1.38  -0.28  -2.75  -3.80  -3.53   Estimation Period: 12 Months (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.64  0.64  0.68  0.66  0.67  0.74  0.80  0.72  0.69  0.62  -0.02    t-Stat  3.87  4.58  4.78  4.24  4.50  4.28  4.29  3.48  2.81  2.07  -0.10    CAPM Alpha  0.26  0.23  0.24  0.14  0.13  0.14  0.12  -0.04  -0.20  -0.47  -0.73    t-Stat  2.69  3.09  3.50  2.22  2.35  2.30  1.86  -0.72  -2.53  -3.90  -4.19   Estimation Period: 24 Months (Daily Data) - Value-Weighted Portfolio Returns    Excess Return  0.54  0.69  0.64  0.69  0.68  0.71  0.77  0.76  0.62  0.58  0.05    t-Stat  3.64  4.69  4.49  4.42  4.34  4.29  4.24  3.60  2.57  1.97  0.20    CAPM Alpha  0.21  0.30  0.20  0.19  0.14  0.12  0.12  0.03  -0.23  -0.48  -0.69    t-Stat  2.25  3.62  2.74  2.92  2.44  2.00  1.95  0.45  -3.07  -4.28  -4.04     The results show that the negative relation between beta and future stock returns detected using equal-weighted portfolios is weaker using value-weighted portfolios. Taken together, the portfolio analyses provide evidence that contradicts the predictions of the CAPM. According to the CAPM, returns should increase across the decile portfolios and risk-adjusted returns should be statistically indistinguishable from zero.\n Regression Analysis As a last step, I analyze the relation between beta and stock returns using Fama and MacBeth (1973) regression analysis. Each month, I perform a cross-sectional regression of one-month-ahead excess stock returns on the given measure of beta. Time-series averages over these cross-sectional regressions then provide the desired results.\nThe next function performs Fama-MacBeth regressions for a given variable and returns the time-series average of monthly regression coefficients and corresponding \\(t\\)-statistics.\nfun.fama_macbeth_regression \u0026lt;- function(data, model, model_name = \u0026quot;value\u0026quot;) { # Prepare and winsorize data data_nested \u0026lt;- data %\u0026gt;% group_by(month) %\u0026gt;% nest(data = -month) # Perform cross-sectional regressions for each month cross_sectional_regs \u0026lt;- data_nested %\u0026gt;% mutate(model = map(data, ~lm(model, data = .x)), tidy = map(model, tidy), glance = map(model, glance), n = map_int(model, nobs)) # Extract average coefficient estimates fama_macbeth_coefs \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% group_by(term) %\u0026gt;% summarize(coefficient = mean(estimate)) # Compute newey-west standard errors of average coefficient estimates newey_west_std_errors \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(tidy) %\u0026gt;% select(month, term, estimate) %\u0026gt;% group_by(term) %\u0026gt;% arrange(month) %\u0026gt;% nest(data_nw = c(month, estimate)) %\u0026gt;% mutate(nw_std_error = map_dbl(data_nw, function(x) sqrt(diag(NeweyWest(lm(estimate ~ 1, data = x), lag = 6))))) %\u0026gt;% select(term, nw_std_error) # Put coefficient estimates and standard errors together and compute t-statistics fama_macbeth_coefs \u0026lt;- fama_macbeth_coefs %\u0026gt;% left_join(newey_west_std_errors, by = \u0026quot;term\u0026quot;) %\u0026gt;% mutate(nw_t_stat = coefficient / nw_std_error) %\u0026gt;% select(term, coefficient, nw_t_stat) %\u0026gt;% pivot_longer(cols = c(coefficient, nw_t_stat), names_to = \u0026quot;statistic\u0026quot;, values_to = model_name) %\u0026gt;% mutate(statistic = paste(term, statistic, sep = \u0026quot; \u0026quot;)) %\u0026gt;% select(-term) # Extract average r-squared and average number of observations fama_macbeth_stats \u0026lt;- cross_sectional_regs %\u0026gt;% unnest(c(glance, n)) %\u0026gt;% ungroup() %\u0026gt;% summarize(adj_r_squared = mean(adj.r.squared), n = mean(n)) %\u0026gt;% pivot_longer(c(adj_r_squared, n), names_to = \u0026quot;statistic\u0026quot;, values_to = model_name) # Combine desired output and return results out \u0026lt;- rbind(fama_macbeth_coefs, fama_macbeth_stats) return(out) } Before I apply the function, I winsorize all beta measures at 0.05% on a monthly basis to mitigate the impact of outliers or estimation errors.\ntbl.betas_regression \u0026lt;- tbl.betas %\u0026gt;% filter(!is.na(ret_adj_excess_f1)) %\u0026gt;% group_by(month) %\u0026gt;% mutate_at(vars(contains(\u0026quot;beta\u0026quot;)), ~fun.winsorize(., cut = 0.005)) %\u0026gt;% ungroup() lst.regressions \u0026lt;- vec.betas %\u0026gt;% map(function(x) {tbl.betas_regression %\u0026gt;% filter(!is.na(!!sym(x))) %\u0026gt;% fun.fama_macbeth_regression(str_c(\u0026quot;ret_adj_excess_f1 ~ \u0026quot;, x), x)}) %\u0026gt;% set_names(vec.betas) The following table provides the regression results for each beta measure.\ntbl.regressions \u0026lt;- lst.regressions %\u0026gt;% map(~mutate(., statistic = str_replace_all(statistic, str_c(vec.betas, collapse = \u0026quot;|\u0026quot;), \u0026quot;beta\u0026quot;))) %\u0026gt;% reduce(full_join) tbl.regressions$statistic \u0026lt;- c(\u0026quot;intercept\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;t-stat\u0026quot;, \u0026quot;adj. R-squared\u0026quot;, \u0026quot;n\u0026quot;) tab.regressions \u0026lt;- tbl.regressions %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;)) tab.regressions   statistic  beta_1y  beta_2y  beta_3y  beta_5y  beta_1m  beta_3m  beta_6m  beta_12m  beta_24m      intercept  0.82  0.78  0.83  0.84  0.99  1.04  1.08  1.14  1.18    t-stat  3.39  3.59  4.16  4.48  4.26  4.60  4.91  5.30  5.59    beta  0.04  0.07  0.11  0.10  -0.06  -0.12  -0.15  -0.21  -0.23    t-stat  0.69  0.73  0.96  0.79  -1.28  -1.61  -1.66  -2.02  -2.05    adj. R-squared  0.02  0.02  0.03  0.03  0.01  0.02  0.03  0.03  0.03    n  2944.37  2587.02  2232.09  1949.31  3131.99  3094.10  3045.12  2948.86  2713.91     For all measures based on monthly data, I do not find any statistically significant relation between beta and stock returns, while the measures based on daily returns do exhibit a statistically significant relation for longer estimation periods. Moreover, the intercepts are positive and statistically significant across all beta measures, although, controlling for the effect of beta, the average excess stock returns should be zero according to the CAPM. The results thus provide no evidence in support of the CAPM.\nIt is worth noting that the average adjusted R-squared ranges only from 2 to 3 percent. Low values of R-squared are common in empirical asset pricing studies. The main reason put forward in the literature is that predicting stock returns is that realized stock returns are just a very noisy measure for expected stock returns (e.g., Elton, 1999).\nIn all future blog posts, I use the 12-month beta estimate based on daily data just asBEM do. The figure below shows the cumulative log returns of the corresponding value-weighted beta portfolios. It clearly demonstrates that high-beta stocks underperform low-beta stocks over the long-run, at least in the CRSP data.\nfig.cum_returns \u0026lt;- lst.portfolios$beta_12m %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(month) %\u0026gt;% mutate(cum_return = cumsum(log(1+ret_vw/100))) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(x = month, y = cum_return, color = portfolio, linetype = portfolio)) + geom_line() + labs(x = NULL, y = NULL, color = NULL, linetype = NULL, title = \u0026quot;Cumulative log returns of value-weighted beta portfolios\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = percent, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()    ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"059dc45735d97d9d59b75c7129c49aa0","permalink":"https://christophscheuch.github.io/post/asset-pricing/beta/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/asset-pricing/beta/","section":"post","summary":"On the estimation on market betas and their relation to stock returns","tags":["Asset Pricing"],"title":"Tidy Asset Pricing - Part II: Beta and Stock Returns","type":"post"},{"authors":["[Francesco D'Acunto](http://www.francescodacunto.com/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Michael Weber](http://faculty.chicagobooth.edu/michael.weber/)"],"categories":null,"content":"","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"a7c6029d7166af6cc3091628ba3cdea0","permalink":"https://christophscheuch.github.io/publication/perceived-precautionary-savings/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/publication/perceived-precautionary-savings/","section":"publication","summary":"In a representative sample of new borrowers, access to lines of credit increases the spending of more liquid households permanently. Liquid consumers reduce their existing savings but do not tap into negative deposits, and hence do not raise debt. Through our FinTech bank setting, we elicit consumers' risk preferences, beliefs, perceptions, and other characteristics directly. No proxies for precautionary savings motives differ systematically across liquid and illiquid consumers. Liquid consumers appear to have higher subjective beliefs about precautionary savings, despite no different objective characteristics. Our results have implication for the transmission of policy to households through bank credit.Presentations: Columbia New Technologies in Finance Conference 2019, Red Rock Finance Conference 2019, Summer Finance Conference at ISB 2019, $7^{th}$ ABFER Annual Conference, LBS Summer Finance Symposium 2019, CFPB Conference 2019, AEA and AFA Annual Meeting 2020","tags":null,"title":"Perceived Precautionary Savings Motives: Evidence from FinTech","type":"publication"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" Seasonalcharts.de claims that stock indices exhibit persistent seasonality that may be exploited through an appropriate trading strategy. As part of a job application, I had to replicate the seasonal pattern for the DAX and then test whether this pattern entails a profitable trading strategy. To sum up, I indeed find that a trading strategy that holds the index only over a specific season outperforms the market significantly, but these results might be driven by a few outliers. The text below references an opinion and is for information purposes only. I do not intend to provide any investment advice.\nThe code is structured in a way that allows for a straight-forward replication of the methodoloty for other indices. The document was knitted in RStudio using the following packages:\nlibrary(tidyverse) # for grammar library(scales) # for pretty breaks in figures library(tidyquant) # for data download library(knitr) # for html knitting library(kableExtra) # for nicer tables Data Preparation First, download data from yahoo finance using the tidyquant package. Note that the DAX was officially launched in July 1988, so this is where our sample starts.\ndax_raw \u0026lt;- tq_get(\u0026quot;^GDAXI\u0026quot;, get = \u0026quot;stock.prices\u0026quot;, from = \u0026quot;1988-07-01\u0026quot;, to = \u0026quot;2019-11-30\u0026quot;)  Then, select only date and the adjusted price (i.e., closing price after adjustments for all applicable splits and dividend distributions) as the relevant variables and compute summary statistics to check for missing or weird values. The results are virtually the same if I use unadjusted closing prices.\ndax \u0026lt;- dax_raw %\u0026gt;% select(date, price = adjusted) summary(dax) ## date price ## Min. :1988-07-01 Min. : 1150 ## 1st Qu.:1996-04-04 1st Qu.: 2512 ## Median :2004-01-08 Median : 5286 ## Mean :2004-02-06 Mean : 5657 ## 3rd Qu.:2011-12-03 3rd Qu.: 7445 ## Max. :2019-11-29 Max. :13560 ## NA\u0026#39;s :160 I replace the missing values by the last available index value.\ndax \u0026lt;- dax %\u0026gt;% arrange(date) %\u0026gt;% fill(price, .direction = \u0026quot;down\u0026quot;) As a last immediate plausibility check, I plot the dax over the whole sample period.\ndax %\u0026gt;% ggplot(aes(x = date, y = price)) + geom_line() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Adjusted Price\u0026quot;) + scale_x_date(expand = c(0, 0), breaks = \u0026quot;5 years\u0026quot;) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() The data seems to be good to go, so let us turn to some graphical evidence for seasonality.\n Graphical Evidence for Seasonality To replicate the average seasonal pattern that we can find here, I construct an index of cumulative returns that starts at 100 in each year of my sample. To do so, I nest the data by year (i.e., create a list-column of returns for each full year in the sample).\ndax_nested \u0026lt;- dax %\u0026gt;% filter(date \u0026gt;= \u0026quot;1988-01-01\u0026quot; \u0026amp; date \u0026lt;= \u0026quot;2018-12-31\u0026quot;) %\u0026gt;% mutate(year = year(date)) %\u0026gt;% group_by(year) %\u0026gt;% nest() Next, I define the function that creates the seasonal pattern, given the sample of returns of a specific year, apply the function to each year and compute summary statistics for each trading day across all years in my sample.\nget_seasonality \u0026lt;- function(data) { data %\u0026gt;% arrange(date) %\u0026gt;% mutate(trading_day = 1:n(), # use number of trading days as index ret = (log(price) - lag(log(price)))*100, ret = if_else(is.na(ret), 0, ret), cum_ret = 100 + cumsum(ret)) %\u0026gt;% select(trading_day, ret, cum_ret) } dax_seasonality \u0026lt;- dax_nested %\u0026gt;% mutate(seasonality = map(data, get_seasonality)) %\u0026gt;% select(year, seasonality) %\u0026gt;% unnest(cols = c(seasonality)) %\u0026gt;% ungroup() dax_seasonality_summary \u0026lt;- dax_seasonality %\u0026gt;% group_by(trading_day) %\u0026gt;% summarize(mean = mean(cum_ret), q05 = quantile(cum_ret, 0.05), q95 = quantile(cum_ret, 0.95)) The latter data frame contains the average cumulative return and corresponding quantiles for each trading day of a year in my sample. Let us now take a look at the average pattern across trading days.\ndax_seasonality_summary %\u0026gt;% ggplot(aes(x = trading_day)) + geom_line(aes(y = mean)) + labs(x = \u0026quot;Trading Days\u0026quot;, y = \u0026quot;Cumulative Returns (in %)\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() While it is unclear what exactly Seasonalcharts plots on their website, the above pattern seems to be fairly consistent with their claimed seasonality. However, even if the pattern seems to be on average there, let us add confidence intervals before we think about trading strategies.\ndax_seasonality_summary %\u0026gt;% ggplot(aes(x = trading_day)) + geom_ribbon(aes(ymin = q05, ymax = q95), alpha = 0.25) + geom_line(aes(y = mean)) + labs(x = \u0026quot;Trading Days\u0026quot;, y = \u0026quot;Cumulative Returns (in %)\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() Naturally, these confidence intervals mechanically increase over the course of a year as each index starts at 100. Even with confidence intervals, the pattern is still visible. Given the graphical evidence, let us now turn to the analysis of a trading strategy that aims to exploit this seasonal pattern.\n Trading Strategy The main idea of Seasonalcharts is to implement the strategy proposed by Jacobsen and Bouman (2002) and Jacobsen and Zhan (2018) which they label ‘The Halloween Indicator’ (or ‘Sell in May Effect’). The main finding of these papers is that stock indices returns seem significantly lower duing the May-October period than during the remainder of the year. The corresponding trading strategy holds an index during the months November-April, but holds the risk-free asset in the May-October period.\nTo replicate their approach (and avoid noise in the daily data), I focus on monthly returns from now on.\ndax_monthly \u0026lt;- dax %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% group_by(year, month) %\u0026gt;% slice(which.max(date)) %\u0026gt;% ungroup() %\u0026gt;% arrange(date) %\u0026gt;% mutate(ret = (log(price) - lag(log(price))) * 100) %\u0026gt;% na.omit() nrow(dax_monthly) ## [1] 376 As usual in empirical asset pricing, we do not care about raw returns, but returns in excess of the risk-free asset. I simply add the European risk free rate from the Fama-French data library as the corresponding reference point. Of course, one could use other measures for the risk-free rate, but the impact on the results won’t be substantial.\ntemp \u0026lt;- tempfile() download.file(\u0026quot;https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Europe_3_Factors_CSV.zip\u0026quot;, temp) unzip(temp, \u0026quot;Europe_3_Factors.csv\u0026quot;) rf_raw \u0026lt;- read_csv(\u0026quot;Europe_3_Factors.csv\u0026quot;, skip = 3) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## `Mkt-RF` = col_character(), ## SMB = col_character(), ## HML = col_character(), ## RF = col_character() ## ) rf \u0026lt;- rf_raw %\u0026gt;% mutate(date = ymd(paste0(X1, \u0026quot;01\u0026quot;)), year = year(date), month = month(date), rf = as.numeric(RF)) %\u0026gt;% filter(date \u0026lt;= \u0026quot;2019-09-01\u0026quot;) %\u0026gt;% select(year, month, rf) dax_monthly \u0026lt;- dax_monthly %\u0026gt;% left_join(rf, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% mutate(excess_ret = ret - rf) %\u0026gt;% na.omit() nrow(dax_monthly) # lose a few obs b/c ff data starts in july 1990 ## [1] 351 Next, let us examine the average excess returns per month in our sample. Before we do that, I define a new (factor) column that ensures that R sorts months correctly in the following analyses.\ndax_monthly$month_factor \u0026lt;- factor(x = months(dax_monthly$date), levels = c(\u0026quot;January\u0026quot;, \u0026quot;February\u0026quot;, \u0026quot;March\u0026quot;, \u0026quot;April\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;June\u0026quot;, \u0026quot;July\u0026quot;, \u0026quot;August\u0026quot;, \u0026quot;September\u0026quot;, \u0026quot;October\u0026quot;, \u0026quot;November\u0026quot;, \u0026quot;December\u0026quot;)) Let us now take a look at the average excess returns per month. I also add the standard deviation, 5% and 95% quantiles, and t-statistic of a t-test of the null hypothesis that average returns are zero in a given month.\ndax_monthly %\u0026gt;% group_by(`Month` = month_factor) %\u0026gt;% summarize(Mean = mean(excess_ret), SD = sd(excess_ret), Q05 = quantile(excess_ret, 0.05), Q95 = quantile(excess_ret, 0.95), `t-Statistic` = sqrt(n()) * mean(excess_ret) / sd(excess_ret) ) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   Month  Mean  SD  Q05  Q95  t-Statistic      January  0.50  5.93  -9.88  8.46  0.45    February  0.63  5.49  -8.72  8.16  0.62    March  0.38  4.46  -6.05  7.41  0.46    April  2.78  5.68  -4.24  13.02  2.63    May  0.11  4.20  -5.90  5.73  0.14    June  -0.69  4.55  -8.13  5.48  -0.82    July  0.95  6.38  -7.59  9.23  0.82    August  -3.07  7.05  -19.05  2.87  -2.38    September  -2.94  8.46  -19.82  5.66  -1.90    October  2.18  6.49  -9.12  11.57  1.80    November  1.79  4.27  -4.86  6.70  2.25    December  1.43  5.43  -6.27  8.53  1.41     August and September seem to usually exhibit negative excess returns with an average of about -3% (statistically significant) over all years, while April and November are the only months that tend to exhibit statistically significant positive excess returns. For a graphical illustration of the above table, I complement it with boxplots for each month. The takeaway is essentially the same, but we can see that August and September exhibit a couple of outliers that might considerably drive the results.\ndax_monthly %\u0026gt;% ggplot(aes(x = month_factor, y = excess_ret)) + geom_boxplot() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Monthly Excess Return (in %)\u0026quot;) + theme_classic() Let us proceed to test for the presence of statistically significant excess returns due to seasonal patterns. In the above table, I only test for significance for each month seperately. To test for positive returns in a joint model, I regress the monthly excess returns on month indicators. Note that I always adjust the standard errors to be heteroskedasticity robust.\nsummary(lm(excess_ret ~ month_factor, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ month_factor, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.5324 -3.0783 0.7171 3.7981 16.4934 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.4970 1.0852 0.458 0.6473 ## month_factorFebruary 0.1380 1.5347 0.090 0.9284 ## month_factorMarch -0.1179 1.5347 -0.077 0.9388 ## month_factorApril 2.2835 1.5347 1.488 0.1377 ## month_factorMay -0.3909 1.5347 -0.255 0.7991 ## month_factorJune -1.1862 1.5347 -0.773 0.4401 ## month_factorJuly 0.4564 1.5218 0.300 0.7645 ## month_factorAugust -3.5626 1.5218 -2.341 0.0198 * ## month_factorSeptember -3.4373 1.5218 -2.259 0.0245 * ## month_factorOctober 1.6789 1.5347 1.094 0.2747 ## month_factorNovember 1.2913 1.5347 0.841 0.4007 ## month_factorDecember 0.9298 1.5347 0.606 0.5450 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.844 on 339 degrees of freedom ## Multiple R-squared: 0.08555, Adjusted R-squared: 0.05588 ## F-statistic: 2.883 on 11 and 339 DF, p-value: 0.001222 Seems like August and September have on average indeed lower returns than January (which is the omitted reference point in this regression). Note that the size of the coefficients from the regression are the same as in the table above (i.e., constant plus coefficient). Next, I follow Jacobsen and Bouman (2002) and simply regression excess returns on dummies that indicate specific seasons, i.e., I estimate the model \\[ y_t=\\alpha + \\beta D_t + \\epsilon_t, \\] where \\(D_t\\) is a dummy variable equal to one for the months in a specific season and zero otherwise. I consider both the ‘Halloween’ season (where the dummy is one for November-April) and a `Seasonality’ season which only excludes July-September (and the dummy is one for October-June). If \\(D_t\\) is statistically significant and positive for the corresponding season, then I take this as evidence for the presence of seasonality effects.\nhalloween_months \u0026lt;- c(11, 12, 1, 2, 3, 4) seasonality_months \u0026lt;- c(10, 11, 12, 1, 2, 3, 4, 5, 6) dax_monthly \u0026lt;- dax_monthly %\u0026gt;% mutate(halloween = if_else(month %in% halloween_months, 1L, 0L), seasonality = if_else(month %in% seasonality_months, 1L, 0L)) The first model considers the `Halloween’ effect:\nsummary(lm(excess_ret ~ halloween, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ halloween, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.8772 -2.9011 0.5344 3.8357 18.0227 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.5954 0.4473 -1.331 0.18402 ## halloween 1.8465 0.6353 2.906 0.00389 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.951 on 349 degrees of freedom ## Multiple R-squared: 0.02363, Adjusted R-squared: 0.02083 ## F-statistic: 8.447 on 1 and 349 DF, p-value: 0.00389 I indeed find evidence that excess returns are higher during the months November-April relative to the remaining months. Let us take this spiel even further by adding even more months:\nsummary(lm(excess_ret ~ seasonality, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ seasonality, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.7885 -3.0178 0.5807 4.0105 18.2628 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.6842 0.6226 -2.705 0.007159 ** ## seasonality 2.6952 0.7220 3.733 0.000221 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.906 on 349 degrees of freedom ## Multiple R-squared: 0.0384, Adjusted R-squared: 0.03564 ## F-statistic: 13.94 on 1 and 349 DF, p-value: 0.0002207 The effect seems to be even stronger if I also include October, May and June.\nAs a last step, let us compare five different strategies: (i) buy and hold the index over the full year, (ii) go long in the index outside of the Halloween season and otherwise hold the risk-free asset, (iii) go long in the index outside of the Halloween season and otherwise short the index, (iv) buy the index outside of the extended seasonality period and otherwise invest in the risk-free asset, and (v) go long in the index outside of the extended seasonality period and short the index otherwise. Below I compare the returns of the three different strategies on an annual basis:\ndax_monthly \u0026lt;- dax_monthly %\u0026gt;% mutate(excess_ret_halloween = if_else(halloween == 1, ret, rf), excess_ret_halloween_short = if_else(halloween == 1, ret, -ret), excess_ret_seasonality = if_else(seasonality == 1, ret, rf), excess_ret_seasonality_short = if_else(seasonality == 1, ret, -ret)) dax_monthly %\u0026gt;% group_by(year) %\u0026gt;% summarize(`Buy and Hold` = sum(excess_ret), `Seasonality` = sum(excess_ret_seasonality), `Seasonality-Short` = sum(excess_ret_seasonality_short), `Halloween` = sum(excess_ret_halloween), `Halloween-Short` = sum(excess_ret_halloween_short)) %\u0026gt;% pivot_longer(-year, names_to = \u0026quot;strategy\u0026quot;, values_to = \u0026quot;excess_ret\u0026quot;) %\u0026gt;% ggplot(aes(x = year, group = strategy)) + geom_col(aes(y = excess_ret, fill = strategy), position = \u0026quot;dodge\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Annual Excess Return (in %)\u0026quot;, fill = \u0026quot;Strategy\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + theme_classic() The ‘Halloween’ and ‘Seasonality’ strategies seem to outperform the ‘Buy and Hold’ strategy in most of the years with the ‘Seasonality’ typically outperforming ‘Halloween’. The strategies that short the index rather than holding the risk free assets also outperform their counterparts. Plotting the overall cumulative excess return of the five strategies confirms these conjectures.\ndax_monthly %\u0026gt;% arrange(date) %\u0026gt;% mutate(`Buy and Hold` = 100 + cumsum(excess_ret), `Seasonality` = 100 + cumsum(excess_ret_seasonality), `Seasonality-Short` = 100 + cumsum(excess_ret_seasonality_short), `Halloween` = 100 + cumsum(excess_ret_halloween), `Halloween-Short` = 100 + cumsum(excess_ret_halloween_short)) %\u0026gt;% select(date, `Buy and Hold`, `Seasonality`, `Seasonality-Short`, `Halloween`, `Halloween-Short`) %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;strategy\u0026quot;, values_to = \u0026quot;cum_excess_ret\u0026quot;) %\u0026gt;% ggplot(aes(x = date)) + geom_line(aes(y = cum_excess_ret, color = strategy)) + scale_x_date(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Excess Return (in %)\u0026quot;, color = \u0026quot;Strategy\u0026quot;) + theme_classic() Which of these strategies might constitute a better investment opportunity? For a very simple assessment, let us compute the corresponding Sharpe ratios. Note that I annualize Sharpe ratios by multiplying them with \\(\\sqrt{12}\\) which strictly speaking only works under IID distributed returns (which is typically unlikely to be the case), but which suffices for the purpose of this note.\nsharpe_ratio \u0026lt;- function(x) { sqrt(12) * mean(x) / sd(x) } dax_monthly %\u0026gt;% arrange(date) %\u0026gt;% summarize(`Buy and Hold` = sharpe_ratio(excess_ret), `Seasonality` = sharpe_ratio(excess_ret_seasonality), `Seasonality-Short` = sharpe_ratio(excess_ret_seasonality_short), `Halloween` = sharpe_ratio(excess_ret_halloween), `Halloween-Short` = sharpe_ratio(excess_ret_halloween_short)) ## # A tibble: 1 x 5 ## `Buy and Hold` Seasonality `Seasonality-Short` Halloween `Halloween-Shor~ ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.184 0.737 0.754 0.774 0.530 The ‘Seasonality-Short’ strategy delivers the highest cumulative excess return in my sample period, but the ‘Halloween’ strategy exhibits a slightly higher Sharpe ratio than the others and thus constitutes a better investment opportunity.\n ","date":1580428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580428800,"objectID":"aa6c77ee45e495b5b8c47c28f78082e9","permalink":"https://christophscheuch.github.io/post/asset-pricing/dax-seasonality/","publishdate":"2020-01-31T00:00:00Z","relpermalink":"/post/asset-pricing/dax-seasonality/","section":"post","summary":"A quick evaluation of seasonal patterns in an equity index","tags":["Asset Pricing","Random"],"title":"Seasonality in the DAX","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" As a newbie to the empirical asset pricing literature, I found it quite hard to fully grasp how results come about solely relying on the data descriptions in the corresponding papers. Fortunately, Bali, Engle and Murray (BEM) provide an excellent textbook that not only provides a comprehensive overview of the empirical research on the cross-section of expected returns, but also offers detailed description of common practices in sample construction and methodology. In this series of notes, I replicate the main results of their textbook in R. Moreover, I try to implement all analyses in a tidy manner using mainly packages beloning to the tidyverse family since it’s fun and fairly transparent.\nIn this first note I start out with the raw monthly CRSP data, compute some descriptive statistics and construct the main sample I use in later analyses. In the following notes, I use the constructed sample to investigate the size effect. As a start, here is the list of packages that I use throughout the note.\nlibrary(tidyverse) # for grammar library(scales) # for nicer axes of figures library(viridis) # for nicer colors in figures library(vroom) # for fast csv reading library(lubridate) # for working with dates I just downloaded the full monthly CRSP sample from WRDS. The full monthly CRSP sample from December 1925 until December 2019 has about 1.2GB. After reading in the data using the incredibly fast vroom package, I convert all column names to lower case out of a personal preference (alternatively, I could also use janitor::clean_names()). Note that I only read in the required columns and that I enforce the character format because I prefer to parse the column types correctly as a next step.\ntbl.crsp_msf \u0026lt;- vroom(\u0026quot;raw/crsp_msf.csv\u0026quot;, col_types = cols(.default = col_character()), col_select = c(PERMNO, date, RET, SHROUT, ALTPRC, EXCHCD, SHRCD, SICCD, DLRET, DLSTCD)) colnames(tbl.crsp_msf) \u0026lt;- str_to_lower(colnames(tbl.crsp_msf)) Next, I make sure that all relevant variables are correctly parsed.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_msf %\u0026gt;% transmute(permno = as.integer(permno), # security identifier date = ymd(date), # date of the observation month = floor_date(date, \u0026quot;month\u0026quot;), # month identifier ret = as.numeric(ret) * 100, # return (in percent) shrout = as.numeric(shrout), # shares outstanding (in thousands) altprc = as.numeric(altprc), # last traded price in a month exchcd = as.integer(exchcd), # exchange code shrcd = as.integer(shrcd), # share code siccd = as.integer(siccd), # industry code dlret = as.numeric(dlret) * 100, # delisting return (in percent) dlstcd = as.integer(dlstcd) # delisting code ) rm(tbl.crsp_msf) As common in the literature, I focus on US-based common stocks in the CRSP database. US-based common stocks are identified with share codes 10 and 11, where the first digit pins down ordinary common shares and the second digit indicates that the secudity has not been further defined (0) or does not need to be further defined (1). I refer to the original CRSP documentation on a full description of the meaning of this distinction.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% select(-shrcd) I apply the following two checks to most data sets I work with. I make sure to keep only distinct observations and that there is only one observation by date for each security (namely, the first row for any permno-date combination). The main reason is that I want to work with a nice panel of unique security-date combinations where no pair appears more than once in the data.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% distinct(permno, date, .keep_all = TRUE) # remove duplicates  Following BEM, I compute the market capitalization as the absolute value of the product of shares outstanding and the price of the stock as of the end of the last trading day of a given month. I have to take the absolute value since altprc is the negative of average of bid and ask from last traded price for which the data is available if there is no last traded price. Since the shares outstanding are reported in thousands of shares, I divide by 1000 such that the resulting measure is in millions of dollars.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% mutate(mktcap = abs(shrout * altprc) / 1000, # in millions of dollars mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap))  As a first glance at the data, I look at the stock exchange composition of the CRSP sample. Stocks listed on NYSDE, AMEX, and NASDAQ are indicated with values of 1 or 31, 2 or 32, and 3 or 33, respectively, in the exchange code filed. I collect stocks traded on all other exchange (e.g., Arca Stock Market, Boston Stock Exchange, Chicago Stock Exchange, etc.) in a separate category.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% mutate(exchange = case_when(exchcd %in% c(1, 31) ~ \u0026quot;NYSE\u0026quot;, exchcd %in% c(2, 32) ~ \u0026quot;AMEX\u0026quot;, exchcd %in% c(3, 33) ~ \u0026quot;NASDAQ\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot;)) The figure below plots the number of stocks per exchange. NYSE has the longest history in the data, but NASDAQ exhibits a considerable large number of stocks. Interestingly, the number of stocks on AMEX is decreasing steadily over the last couple of decades. By the end of 2019, there are 2,227 stocks on NASDAQ, 1,274 on NYSE, 167 on AMEX and 3 belonging to the other category.\nfig.securities \u0026lt;- tbl.crsp_monthly %\u0026gt;% count(exchange, date) %\u0026gt;% ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) + geom_line() + labs(x = NULL, y = NULL, color = NULL, linetype = NULL, title = \u0026quot;Monthly number of securities by exchange\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()   Next, I look at the market capitalization per exchange. To do so, I adjust the market capitalization values for inflation using the using the consumer price index from the US Bureau of Labor Statistics as provided by the FRED database. All values are in end of 2019 dollars for intertemporal comparability. NYSE has by far the largest market capitalization, followed by NASDAQ. At the end of 2019, total market value on NYSE was 19,123 billion and 12,536 billion on NASDAQ. AMEX only plays a minor role by now with a total market capitalization of 42 billion at the end of 2019, while other exchanges only make up 13 billion.\ntbl.cpi_raw \u0026lt;- read_csv(\u0026quot;raw/CPIAUCNS.csv\u0026quot;) tbl.cpi \u0026lt;- tbl.cpi_raw %\u0026gt;% filter(DATE \u0026lt;= max(tbl.crsp_monthly$date)) %\u0026gt;% transmute(month = floor_date(DATE, \u0026quot;month\u0026quot;), cpi = CPIAUCNS) %\u0026gt;% arrange(month) %\u0026gt;% mutate(cpi = cpi / last(cpi)) tbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% left_join(tbl.cpi, by = c(\u0026quot;month\u0026quot;)) %\u0026gt;% mutate(mktcap_cpi = mktcap / cpi) fig.mktcap \u0026lt;- tbl.crsp_monthly %\u0026gt;% group_by(exchange, date) %\u0026gt;% summarize(mktcap = sum(mktcap_cpi, na.rm = TRUE) / 1000) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(x = date, y = mktcap, color = exchange, linetype = exchange)) + geom_line() + labs(x = NULL, y = NULL, color = NULL, linetype = NULL, title = \u0026quot;Monthly total market value (billions of Dec 2019 Dollars) by exchange\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()   Now, I turn to the industry composition of the CRSP sample. I follow the below convention on defining industries using the standard industry classification (SIC) codes.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% mutate(industry = case_when(siccd \u0026gt;= 1 \u0026amp; siccd \u0026lt;= 999 ~ \u0026quot;Agriculture\u0026quot;, siccd \u0026gt;= 1000 \u0026amp; siccd \u0026lt;= 1499 ~ \u0026quot;Mining\u0026quot;, siccd \u0026gt;= 1500 \u0026amp; siccd \u0026lt;= 1799 ~ \u0026quot;Construction\u0026quot;, siccd \u0026gt;= 2000 \u0026amp; siccd \u0026lt;= 3999 ~ \u0026quot;Manufacturing\u0026quot;, siccd \u0026gt;= 4000 \u0026amp; siccd \u0026lt;= 4999 ~ \u0026quot;Transportation\u0026quot;, siccd \u0026gt;= 5000 \u0026amp; siccd \u0026lt;= 5199 ~ \u0026quot;Wholesale\u0026quot;, siccd \u0026gt;= 5200 \u0026amp; siccd \u0026lt;= 5999 ~ \u0026quot;Retail\u0026quot;, siccd \u0026gt;= 6000 \u0026amp; siccd \u0026lt;= 6799 ~ \u0026quot;Finance\u0026quot;, siccd \u0026gt;= 7000 \u0026amp; siccd \u0026lt;= 8999 ~ \u0026quot;Services\u0026quot;, siccd \u0026gt;= 9000 \u0026amp; siccd \u0026lt;= 9999 ~ \u0026quot;Public\u0026quot;, TRUE ~ \u0026quot;Missing\u0026quot;)) The figure below plots the number of stocks in the sample in each of the SIC industries. Most of the stocks are apparently in Manufacturing albeit the number peaked somewhere in the 90ies. The number of public administration stocks seems to the the only category on the rise in recent years.\nfig.industries \u0026lt;- tbl.crsp_monthly %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% count(industry, date) %\u0026gt;% ggplot(aes(x = date, y = n, color = industry, linetype = industry)) + geom_line() + labs(x = NULL, y = NULL, color = NULL, linetype = NULL, title = \u0026quot;Monthly number of securities by industry\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()   I repeat the exercise by computing the market value of all stocks belonging to the respective industries. All values are again in terms of billions of end of 2019 dollars. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Towards the end of the sample financial firms and services begin to make up a substantial portion of the market value.\nfig.industries_mktcap \u0026lt;- tbl.crsp_monthly %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% group_by(industry, date) %\u0026gt;% summarize(mktcap = sum(mktcap_cpi, na.rm = TRUE) / 1000) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(x = date, y = mktcap, color = industry, linetype = industry)) + geom_line() + labs(x = NULL, y = NULL, color = NULL, linetype = NULL, title = \u0026quot;Monthly total market value (billions of Dec 2019 Dollars) by industry\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()   Next, I turn to the calculation of returns. Monthly stock returns are in the ret field (i.e., the return realized by holding the stock from its last trade in the previous month to its last trade in the current month). However, I have to adjust for delistings which are fortunately recorded by CRSP. The adjustment procedure below follows Shumway (1997). Apparently, the adjustment kicks in on both tails of the return distribution as I can see from the summary statistics.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% mutate(ret_adj = case_when(is.na(dlstcd) ~ ret, !is.na(dlstcd) \u0026amp; !is.na(dlret) ~ dlret, dlstcd %in% c(500, 520, 580, 584) | (dlstcd \u0026gt;= 551 \u0026amp; dlstcd \u0026lt;= 574) ~ -30, TRUE ~ -100)) %\u0026gt;% select(-c(dlret, dlstcd)) summary(select(tbl.crsp_monthly, ret, ret_adj))  ## ret ret_adj ## Min. : -99.36 Min. :-100.00 ## 1st Qu.: -6.40 1st Qu.: -6.45 ## Median : 0.00 Median : 0.00 ## Mean : 1.18 Mean : 1.02 ## 3rd Qu.: 6.99 3rd Qu.: 6.95 ## Max. :2400.00 Max. :6366.67 ## NA\u0026#39;s :123119 NA\u0026#39;s :104382 As a main reference point for each stock return, I want to consider the return on the market. According to the Capital Asset Pricing Model (CAPM), cross-sectional variation in expected asset returns should be a function of the covariance between the return of the asset and the return on the market portfolio. The value-weighted portfolio of all US-based common stocks in the CRSP database is one of the main proxies used in the empirical asset pricing literature which I also get from WRDS.\ntbl.crsp_si \u0026lt;- read_csv(\u0026quot;raw/crsp_si.csv\u0026quot;) colnames(tbl.crsp_si) \u0026lt;- str_to_lower(colnames(tbl.crsp_si)) tbl.crsp_monthly_market \u0026lt;- tbl.crsp_si %\u0026gt;% transmute(month = floor_date(ymd(date), \u0026quot;month\u0026quot;), vwretd = vwretd * 100, ewretd = ewretd * 100) tbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% left_join(tbl.crsp_monthly_market, by = \u0026quot;month\u0026quot;) In most cases, it makes sense to use stock returns in excess of the risk-free security over the same period. I take the monthly risk-free rate from Ken French’s data library and also add the other Fama-French factors while I am already at it. In particular, I also add another measure for the return on the market portfolio provided by Fama and French. The main difference to the CRSP measure lies in the fact that Fama and French exclude firms that are not based in the US, closed-end funds and other securities that are not common stocks. I discuss the other factors in later notes where I try to replicate them.\ntbl.factors_ff_monthly \u0026lt;- read_csv(\u0026quot;raw/F-F_Research_Data_Factors.csv\u0026quot;, skip = 3) %\u0026gt;% transmute(month = floor_date(ymd(paste0(X1, \u0026quot;01\u0026quot;)), \u0026quot;month\u0026quot;), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF), smb_ff = as.numeric(SMB), hml_ff = as.numeric(HML)) tbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% left_join(tbl.factors_ff_monthly, by = \u0026quot;month\u0026quot;)  As a final descriptive statistic, I plot the cumulative log returns of our market measures (i.e., equal-weighted, value-weighted and the Fama-French market factor). Clearly, the value-weighted CRSP index and the Fama-French market factor are highly correlated, while the equal-weighted CRSP index stands out due its high returns apparently driven by smaller stocks.\ntbl.market \u0026lt;- tbl.crsp_monthly %\u0026gt;% distinct(date, vwretd, ewretd, mkt_ff) %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;portfolio\u0026quot;, values_to = \u0026quot;return\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% mutate(cum_return = cumsum(log(1+return/100))) %\u0026gt;% ungroup() fig.market \u0026lt;- tbl.market %\u0026gt;% mutate(portfolio = case_when(portfolio == \u0026quot;vwretd\u0026quot; ~ \u0026quot;CRSP (Value-Weighted)\u0026quot;, portfolio == \u0026quot;ewretd\u0026quot; ~ \u0026quot;CRSP (Equal-Weighted)\u0026quot;, portfolio == \u0026quot;mkt_ff\u0026quot; ~ \u0026quot;MKT (Fama-French)\u0026quot;)) %\u0026gt;% ggplot(aes(x = date, y = cum_return, color = portfolio, linetype = portfolio)) + geom_line() + labs(x = NULL, y = NULL, color = NULL, linetype = NULL, title = \u0026quot;Cumulative log return for different market factors\u0026quot;) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + scale_y_continuous(labels = percent, breaks = pretty_breaks()) + theme_classic() + scale_color_viridis_d()   The focus of empirical asset pricing is to examine the ability of different stock characteristics to predict the cross section of future stock returns. This is why I add the (excess) returns for the subsequent month of each stock to the sample. Note that I use beginning of month dates to ensure correct matching of dates (you may in principle also use consistent end-of-month dates, but subtracting months from the first day of month is easier).\ntbl.crsp_monthly_f1 \u0026lt;- tbl.crsp_monthly %\u0026gt;% transmute(permno = permno, month = floor_date(date, \u0026quot;month\u0026quot;) %m-% months(1), ret_excess_f1 = ret - rf_ff, ret_adj_excess_f1 = ret_adj - rf_ff, mkt_ff_excess_f1 = mkt_ff - rf_ff) tbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% left_join(tbl.crsp_monthly_f1, by = c(\u0026quot;permno\u0026quot;, \u0026quot;month\u0026quot;)) To wrap up this data set, I arrange everything by security and date before saving the sample and provide some summary statistics of all the variables that were used above. I end up with a monthly CRSP file that has about 700 MB.\ntbl.crsp_monthly \u0026lt;- tbl.crsp_monthly %\u0026gt;% arrange(permno, date) write_rds(tbl.crsp_monthly, \u0026quot;data/crsp_monthly.rds\u0026quot;) summary(tbl.crsp_monthly) ## permno date month ret ## Min. :10000 Min. :1925-12-31 Min. :1925-12-01 Min. : -99.36 ## 1st Qu.:21960 1st Qu.:1977-04-29 1st Qu.:1977-04-01 1st Qu.: -6.40 ## Median :49373 Median :1990-12-31 Median :1990-12-01 Median : 0.00 ## Mean :50206 Mean :1988-03-18 Mean :1988-02-18 Mean : 1.18 ## 3rd Qu.:78749 3rd Qu.:2002-04-30 3rd Qu.:2002-04-01 3rd Qu.: 6.99 ## Max. :93436 Max. :2019-12-31 Max. :2019-12-01 Max. :2400.00 ## NA\u0026#39;s :123119 ## shrout altprc exchcd siccd ## Min. : 0 Min. : -1832.5 Min. :-2.000 Min. : 0 ## 1st Qu.: 2175 1st Qu.: 1.6 1st Qu.: 1.000 1st Qu.:3079 ## Median : 6774 Median : 10.8 Median : 2.000 Median :3842 ## Mean : 40744 Mean : 27.0 Mean : 2.107 Mean :4511 ## 3rd Qu.: 23212 3rd Qu.: 25.2 3rd Qu.: 3.000 3rd Qu.:6030 ## Max. :29206400 Max. :339590.0 Max. :33.000 Max. :9999 ## NA\u0026#39;s :3743 NA\u0026#39;s :72367 NA\u0026#39;s :2411 ## mktcap exchange cpi mktcap_cpi ## Min. : 0.0 Length:3645522 Min. :0.04903 Min. : 0.0 ## 1st Qu.: 15.9 Class :character 1st Qu.:0.23349 1st Qu.: 47.6 ## Median : 66.8 Mode :character Median :0.52068 Median : 180.9 ## Mean : 1490.3 Mean :0.49551 Mean : 2233.2 ## 3rd Qu.: 357.8 3rd Qu.:0.69968 3rd Qu.: 812.0 ## Max. :1304764.7 Max. :1.00145 Max. :1304764.7 ## NA\u0026#39;s :72504 NA\u0026#39;s :72504 ## industry ret_adj vwretd ewretd ## Length:3645522 Min. :-100.00 Min. :-29.1731 Min. :-31.275 ## Class :character 1st Qu.: -6.45 1st Qu.: -1.7553 1st Qu.: -1.987 ## Mode :character Median : 0.00 Median : 1.2963 Median : 1.402 ## Mean : 1.02 Mean : 0.9308 Mean : 1.123 ## 3rd Qu.: 6.95 3rd Qu.: 3.8775 3rd Qu.: 4.231 ## Max. :6366.67 Max. : 39.4143 Max. : 66.594 ## NA\u0026#39;s :104382 NA\u0026#39;s :497 NA\u0026#39;s :497 ## rf_ff mkt_ff smb_ff hml_ff ## Min. :-0.06 Min. :-29.100 Min. :-16.720 Min. :-13.280 ## 1st Qu.: 0.15 1st Qu.: -1.750 1st Qu.: -1.690 1st Qu.: -1.320 ## Median : 0.39 Median : 1.290 Median : 0.080 Median : 0.270 ## Mean : 0.37 Mean : 0.949 Mean : 0.128 Mean : 0.366 ## 3rd Qu.: 0.51 3rd Qu.: 3.920 3rd Qu.: 1.860 3rd Qu.: 1.780 ## Max. : 1.35 Max. : 38.950 Max. : 36.700 Max. : 35.460 ## NA\u0026#39;s :3627 NA\u0026#39;s :3627 NA\u0026#39;s :3627 NA\u0026#39;s :3627 ## ret_excess_f1 ret_adj_excess_f1 mkt_ff_excess_f1 ## Min. : -99.52 Min. :-101.31 Min. :-29.130 ## 1st Qu.: -6.77 1st Qu.: -6.82 1st Qu.: -2.020 ## Median : -0.36 Median : -0.37 Median : 1.020 ## Mean : 0.81 Mean : 0.65 Mean : 0.578 ## 3rd Qu.: 6.65 3rd Qu.: 6.62 3rd Qu.: 3.520 ## Max. :2399.66 Max. :6365.91 Max. : 38.850 ## NA\u0026#39;s :126291 NA\u0026#39;s :107577 NA\u0026#39;s :29162 Finally, here is the code chunk for setting up the daily return data from CRSP. Note that the raw daily CRSP sample has about 24GB, whereas the final sample is about 3 GB. I just use a scientific cluster with plenty of memory to run this part.\n# Read in data tbl.crsp_dsf \u0026lt;- vroom(\u0026quot;raw/crsp_dsf.csv\u0026quot;, col_types = cols(.default = col_character()), col_select = c(PERMNO, date, RET, SHRCD)) colnames(tbl.crsp_dsf) \u0026lt;- str_to_lower(colnames(tbl.crsp_dsf)) # Keep only common stocks and relevant columns tbl.crsp_daily \u0026lt;- tbl.crsp_dsf %\u0026gt;% mutate(shrcd = as.integer(shrcd)) %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% transmute(permno = as.integer(permno), date = ymd(date), ret = as.numeric(ret) * 100 ) rm(tbl.crsp_dsf) # Keep only distinct observations tbl.crsp_daily \u0026lt;- tbl.crsp_daily %\u0026gt;% distinct(permno, date, .keep_all = TRUE) # Add fama french factors (https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) tbl.factors_ff_daily \u0026lt;- read_csv(\u0026quot;raw/F-F_Research_Data_Factors_daily.csv\u0026quot;, skip = 3) %\u0026gt;% transmute(date = ymd(X1), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF)) %\u0026gt;% filter(date \u0026lt;= max(tbl.crsp_daily$date)) tbl.crsp_daily \u0026lt;- tbl.crsp_daily %\u0026gt;% left_join(tbl.factors_ff_daily, by = \u0026quot;date\u0026quot;) # Drop rows with missing values tbl.crsp_daily \u0026lt;- tbl.crsp_daily %\u0026gt;% na.omit() # Set reference month tbl.crsp_daily \u0026lt;- tbl.crsp_daily %\u0026gt;% mutate(month = floor_date(date, \u0026quot;month\u0026quot;)) %\u0026gt;% select(permno, date, month, ret, rf_ff, mkt_ff) # Save sample write_rds(tbl.crsp_daily, \u0026quot;data/crsp_daily.rds\u0026quot;)  ","date":1579824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"26840e564e4a98d43b53d838e4ba623d","permalink":"https://christophscheuch.github.io/post/asset-pricing/crsp-sample/","publishdate":"2020-01-24T00:00:00Z","relpermalink":"/post/asset-pricing/crsp-sample/","section":"post","summary":"On the preparation of monthly return data for empirical research on the cross-section of expected returns","tags":["Asset Pricing"],"title":"Tidy Asset Pricing - Part I: The CRSP Sample in R","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":"In Hautsch, et al. (2019), we focus on the market for Bitcoin (BTC) against US Dollar (USD) and estimate the corresponding spotvolatilities for 16 different exchanges. In this note, I replicate the estimation procedure for the spotvolatilities of other cryptocurrencies, namely Ether (ETH), Litecoin (LTC), and Ripple (XRP) against USD.\nBefore I proceed to the results, I briefly sketch the estimation procedure. To estimate the spot volatility, I follow the approach of Kristensen (2010). For each market $i$, asset $j$, and minute $t$, I estimate $\\sigma_{i,j,t}^2$ by $$\\widehat{{\\sigma}_{i,j,t}}^2(h_T) = \\sum\\limits_{l=1}^\\infty K\\left(l - t, h_T\\right)\\left(b_{i,j,l} - b_{i,j,l-1}\\right)^2,$$ where $K\\left(l - t, h_T\\right)$ is a one-sided Gaussian kernel smoother with bandwidth $h_T$ and $b_{i,j,l}$ corresponds to the quoted bid price on market $i$ for asset $j$ at time $l$. The choice of the bandwidth $h_T$ involves a trade-off between the variance and the bias of the estimator. Using too many observations introduces a bias if the volatility is time-varying, whereas shrinking the estimation window through a lower bandwidth increases the variance of the estimator. Kristensen (2010) hence proposes to choose $h_T$ such that information on day $T-1$ is used for the estimation on day $T$, i.e., the bandwith on any day is the result of minimizing the integrated squared error of estimates on the previous day.\nI employ the data that I collected together with my colleague Stefan Voigt. Since January 2018, we gather the first 25 levels of all open buy and sell orders of a couple of exchanges on a minute level using our R package. The spotvolatility estimation procedure from above, however, only rests on the first level of bids. To ensure comparability across asset pairs, I focus on exchanges in our sample that feature trading of all four asset pairs (Binance, Bbitfinex, Bitstamp, Cex, Gate, Kraken, Lykke, Poloniex, xBTCe).\nNow, let us take a look at the resulting volatility estimates for all asset pairs. The app below shows the average daily spotvolatility estimate across all exchanges (solid lines) and corresponding range of average daily spotvolatility estimates (shaded areas). Overall, all four asset pairs exhibit a strong correlation over the last two years. Feel free to play around with the app by comparing asset pairs seperately or focussing on subperiods.\n Shiny App Iframe    \n","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"fa8ef2201f05612cb5e54b1b492fbc4f","permalink":"https://christophscheuch.github.io/post/settlement-latency/spotvolas/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/post/settlement-latency/spotvolas/","section":"post","summary":"An application of the spotvolatility estimator from [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) to other cryptocurrencies","tags":["Blockchain"],"title":"Volatility in Cryptocurrency Markets","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":" In its essence, the distributed ledger technology (DLT) is a digital record-keeping system that allows for the verification, updating and storage of transfers of ownership without the need for a designated third party. It relies on a single ledger that is distributed among many different parties who are incentivized to ensure a truthful representation of the transaction history. Nakamoto first popularized the idea of DLTs in a financial context with the Bitcoin protocol and the underlying concept of blockchain. Nowadays, however, Bitcoin is just one of several hundred applications that use the blockchain technology, while other forms of DLTs, in particular directed acyclic graphs (DAG), are actively explored as well. In the following, we first describe the building blocks of DLT before we turn to a more detailed discussion of blockchains and DAGs.\nFundamentals of Distributed Ledgers DLT solves the fundamental problems that arise in the context of digital transfer of ownership. Transactions are pieces of information that agents authorize to be sent to other agents. A record-keeping system has to ensure that transactions are signed and recorded in the correct order. In principle, a single authority could verify signatures and consistency of transactions, but it would be prone to failures. As a result, it might be desirable to distribute this type of information to a system with multiple machines that can sustain the failure of single units. A fault-tolerant design would enable a system to continue its operation, even when one or several units stop working.\nTo achieve this goal, DLT essentially combines two fundamental concepts. First, a distributed ledger is based on asymmetrical cryptography that enables digital signatures of transactions. On the one hand, the sender of a transaction wants to be the sole owner of the signature that allows the transfer of assets from her private wealth. On the other hand, a record-keeping system requires information about the identities of the parties involved. Cryptographic algorithms ensure that any private keys are only known to their owners, while public keys may be disseminated widely. That is, everybody can check whether a private key is valid, but nobody can back out the private key from public information.1\nSecond, a distributed ledger is conceptually a distributed system which checks whether transactions should be in the system and in which order. In a distributed system, many machines are connected through a network and ensure that the system keeps operating even when some machines fail or try to mess up the system. For instance, if the sender of a transaction is also a potential validator, then she has an incentive for dishonest behavior, such as double-spending or revoking transactions. Individual machines have to reach some form of consensus about actual transaction histories. This consensus can be achieved through different network structures such as blockchain or DAG.\nBlockchain In the context of blockchain, the typical solution to the consensus problem involves competition among potential validators for the right to append information to the ledger.2 The most common consensus protocol, Proof-of-Work (PoW), involves solving a computationally expensive problem where the winner gets the right to update the ledger and typically receives a reward. This particular form of DLT is called blockchain since transactions are not verified individually, but rather appended to the ledger in blocks. Validators bundle transactions that wait for verification and try to solve the problem. However, the system\u0026rsquo;s protocol limits the number of transactions that can be included in a single block. This limit leads to a queue of unconfirmed transactions and validators are free to choose the transactions they try to append to the blockchain. Average verification times thus not only depend on the number of unconfirmed transactions, but also on the fee associated with a transaction, as validators find it more attractive to include transactions with high fees in their blocks.\nThe computationally difficult problem typically relies on cryptographic hash functions, which map an input of arbitrary size to output of fixed size and cannot be inverted. In the Bitcoin network, validators bundle the information of several transactions and a reference to the current state of the blockchain and plug the data into a hash function. The hash function converts this input into a sequence of characters and numbers of certain length. The system\u0026rsquo;s protocol then requires that the output starts with a certain number of zeros. The probability of calculating a hash that starts with many zeros is very low and to generate a new hash, validators include a random number called nonce that can lead to a very different output. The difficulty of the problem is then determined by the number of leading zeros validators have to find. Depending on total available computational power, the system regularly adjusts the target to achieve an average of 10 minutes between two consecutive blocks.\nWhile validators in a PoW system utilize substantial computational resources to win the competition for block generation, validators might also be randomly chosen based on their wealth. In the so called Proof-of-Stake (PoS) protocol, validators stake their tokens to be able to create blocks. The higher a validator\u0026rsquo;s stake, the higher are the chances of creating the next block. After successfully appending a new block, the validator receives transaction fees just as in the case of PoW. If the validator submits an incorrect block or is offline during a staking period, then she is penalized and (at least partly) loses her stake. The penalty might either arise explicitly through a deduction of funds from the stake or implicitly as dishonest behavior creates a feedback on the value of the stake. In particular, if a validator appends to the blockchain in a way that perpetuates disagreement, then she imposes a cost upon all users of the particular blockchain. Such behavior lowers the value of the whole network and is also reflected in a lower valuation of the misbehaving validator\u0026rsquo;s stake. The endogenous feedback between validators\u0026rsquo; behavior and the value of their stakes incentives them to eventually reach consensus.\nOther consensus protocols combine features of PoW and PoS. For instance, delegated Proof-of-Stake (DPoS) relies both on stakeholders, who elect validators and have voting rights proportional to their stake, and validators, who exert effort to append information to the ledger. The reputation of validators determines their chance for reelection, while stakeholders have incentives to select truthful validators.\nDirected Acyclic Graphs While blockchains record transactions in blocks, DAGs store information in single transactions. More specifically, any transaction represents a node in a graph (i.e., a set of vertices connected by edges) and each new transaction confirms at least one previous transaction, depending on the configuration of the underlying protocol. The longer the branch on which a transaction is based, the more certain is its validity. Intuitively, only once a transaction is broadcasted sufficiently throughout the network, it is verified. DAGs thus hinge on a steady flow of new transactions that enter the network to verify and reference old transactions. The connections between transactions are directed (i.e., the edges in the form of confirmations are one way) and the whole graph is acyclic (i.e., it is impossible to traverse the entire graph starting from a single edge). Given a high number of transactions, DAG ledgers scale better and can achieve consensus faster than blockchains which rely on fixed block sizes and limited verification rates.\nSettlement Latency in Distributed Systems A distributed system features settlement latency in transaction verification, as it is ex-ante unclear how long it takes until validators achieve consensus or to broadcast that consensus through the network. For PoW, latency depends on the time it takes for validators to find a solution to the computationally expensive problem. In the Bitcoin protocol, for instance, validators append a new block on average every 10 minutes, while the process takes about 20 seconds in the Ethereum protocol. For PoS, latency depends on how long disagreement on the correct order of transactions persists. For both blockchain-based protocols, the information still needs to be distributed to all other nodes in the network, possibly facing technological limitations that prevent instant percolation. Technological limits are also particularly relevant for DAGs which rely on a large number of nodes that verify transactions and distribute information through the network. Overall, any distributed system that refrains from using designated third-parties bearing the counterparty risk associated with transactions thus features settlement latency.\n The most simple illustrative example for asymmetric cryptography is the multiplication of prime numbers. One can easily multiply two prime numbers (private key) to get a large number (public key), but it can be difficult to infer the initial set of numbers from the product.\r^ The problem is more severe in a permissionless blockchain where anybody can access and potentially update the blockchain. Other variants, where only few institutions or individuals are entitled to direct access to the blockchain, so-called permissioned blockchains, limit the problem to few players.\r^   ","date":1575331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575331200,"objectID":"385edcc2abfadfe0677ae2eb7a7d1acd","permalink":"https://christophscheuch.github.io/post/settlement-latency/fundamentals/","publishdate":"2019-12-03T00:00:00Z","relpermalink":"/post/settlement-latency/fundamentals/","section":"post","summary":"A companion piece to [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) on how latency enters blockchain-based settlement","tags":["Blockchain"],"title":"Distributed Ledger Technology and Settlement Latency","type":"post"},{"authors":["[Alexander Mürmann](https://www.wu.ac.at/finance/people/faculty/muermann/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1510790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510790400,"objectID":"56c84d7d745a1cc19c70899dab3e5554","permalink":"https://christophscheuch.github.io/publication/fishing-with-pearls/","publishdate":"2017-11-16T00:00:00Z","relpermalink":"/publication/fishing-with-pearls/","section":"publication","summary":"We provide novel evidence of banks establishing lending relationships with prestigious firms to signal their quality and attract future business. Using survey data on firm-level prestige, we show that lenders compete more intensely for prestigious borrowers and offer lower upfront fees to initiate lending relationships with prestigious firms. We also find that banks expand their lending after winning prestigious clients. Prestigious firms benefit from these relations as they face lower costs of borrowing even though prestige has no predictive power for credit risk. Our results are robust to matched sample analyses and a regression discontinuity design.Presentations: FIRS Conference 2018","tags":null,"title":"Fishing with Pearls: The Value of Lending Relationships with Prestigious Firms","type":"publication"}]