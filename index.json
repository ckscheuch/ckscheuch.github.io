[{"authors":["admin"],"categories":null,"content":"I am a PhD student at the Vienna Graduate School of Finance. My research interests focus on the economics of technological innovations in the financial industry. I recently released a paper that shows how blockchain-based settlement may introduce limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the availability of overdraft facilities through mobile banking apps. I currently investigate the potential of crowdfunding mechanisms to elicit demand information and improve the screening of viable projects.\nBefore starting my PhD studies, I was a research assistant in the field of labor economics at the Institute for Advanced Studies and an intern at Ithuba Capital and the Austrian Financial Market Authority.\n","date":1579824000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1579824000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://christophscheuch.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at the Vienna Graduate School of Finance. My research interests focus on the economics of technological innovations in the financial industry. I recently released a paper that shows how blockchain-based settlement may introduce limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the availability of overdraft facilities through mobile banking apps. I currently investigate the potential of crowdfunding mechanisms to elicit demand information and improve the screening of viable projects.","tags":null,"title":"Christoph Scheuch","type":"authors"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" As a newbie to the empirical asset pricing literature, I found it quite hard to fully grasp how results come about solely relying on the data descriptions in the corresponding papers. Fortunately, Bali, Engle and Murray provide an excellent textbook that not only provides a comprehensive overview of the empirical research on the cross-section of expected returns, but also offers detailed description of common practices in sample construction and methodology. In this series of notes, I replicate the main results of their textbook in R. Moreover, I try to implement all analyses in a tidy manner using mainly packages beloning to the [tidyverse](https://www.tidyverse.org/) family since it’s fun and fairly transparent.\nIn this first note I start out with the raw monthly CRSP data, compute some descriptive statistics and construct the main sample I use in later analyses. In the next note, I’ll use the constructed sample to investigate the size effect. So let us start with the list of packages that I use throughout the note.\nlibrary(tidyverse) # for grammar library(lubridate) # for working with dates library(scales) # for nicer axes of figures I just downloaded the full monthly CRSP sample from WRDS. After reading in the data, I convert all column names to lower case out of a personal preference.\ncrspa_msf \u0026lt;- read_csv(here::here(\u0026quot;raw/crspa_msf.csv\u0026quot;)) # \u0026quot;here\u0026quot; needed just to properly knit markdown document colnames(crspa_msf) \u0026lt;- tolower(colnames(crspa_msf)) nrow(crspa_msf) ## [1] 4568082 Next, I make sure that all relevant variables are correctly parsed.\ncrsp \u0026lt;- crspa_msf %\u0026gt;% transmute(permno = as.integer(permno), # security identifier date = ymd(date), # month identifier ret = as.numeric(ret) * 100, # return (convert to percent) shrout = as.numeric(shrout), # shares outstanding (in thousands) altprc = as.numeric(altprc), # last traded price in a month exchcd = as.integer(exchcd), # exchange code shrcd = as.integer(shrcd), # share code siccd = as.integer(siccd), # industry code dlret = as.numeric(dlret) * 100, # delisting return (converted to percent) dlstcd = as.integer(dlstcd) # delisting code ) nrow(crsp) ## [1] 4568082 As common in the literature, I focus on US-based common stocks in the CRSP database. US-based common stocks are identified with share codes 10 and 11, where the first digit pins down ordinary common shares and the second digit indicates that the secudity has not been further defined (0) or does not need to be further defined (1). I refer to the original CRSP documentation on a full description of the meaning of this distinction.\ncrsp \u0026lt;- crsp %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% select(-shrcd) nrow(crsp) ## [1] 3630251 I apply the following two checks to all data sets I work with. I make sure to keep only distinct observations and I check that there is only one observation by date for each security. The main reason is that I want to work with a nice panel of unique security-date combinations where no pair appears more than once in the data.\ncrsp \u0026lt;- crsp %\u0026gt;% distinct() nrow(crsp) ## [1] 3601076 crsp %\u0026gt;% group_by(permno, date) %\u0026gt;% filter(n() \u0026gt; 1) %\u0026gt;% nrow() == 0 ## [1] TRUE If the last part of the code returns FALSE, I usually investigate where the failure comes from (typically related to reporting errors). Fortunately, everything looks good in the CRSP data.\nFollowing Bali, Engle and Murray, I compute the market capitalization as the absolute value of the product of shares outstanding and the price of the stock as of the end of the last trading day of a given month. I have to take the absolute value since altprc is the negative of average of bid and ask from last traded pricefor which the data is available if there is no last traded price. Since the shares outstanding are reported in thousands of shares, I divide by 1000 such that the resulting measure is in millions of dollars.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(mktcap = abs(shrout * altprc) / 1000, # in millions of dollars mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap))  As a first glance at the data, let us look at the stock exchange composition of the CRSP sample. Stocks listed on NYSDE, AMEX, and NASDAQ are indicated with values of 1 or 31, 2 or 32, and 3 or 33, respectively, in the exchange code filed. I collect stocks traded on all other exchange (e.g., Arca Stock Market, Boston Stock Exchange, Chicago Stock Exchange, etc.) in a separate category.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(exchange = case_when(exchcd %in% c(1, 31) ~ \u0026quot;NYSE\u0026quot;, exchcd %in% c(2, 32) ~ \u0026quot;AMEX\u0026quot;, exchcd %in% c(3, 33) ~ \u0026quot;NASDAQ\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot;)) The figure below plots the number of stocks per exchange. NYSE has the longest history in the data, but NASDAQ exhibits a considerable large number of stocks. Interestingly, the number of stocks on AMEX is decreasing steadily over the last couple of decades. By the end of 2018, there are 2,221 stocks on NASDAQ, 1,274 on NYSE, 167 on AMEX and 3 belonging to the other category.\ncrsp %\u0026gt;% count(exchange, date) %\u0026gt;% ggplot(aes(x = date, y = n, group = exchange)) + geom_line(aes(color = exchange, linetype = exchange)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Securities\u0026quot;, color = \u0026quot;Exchange\u0026quot;, linetype = \u0026quot;Exchange\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Next, let us look at the market capitalization per exchange. To do so, I adjust the market capitalization values for inflation using the using the consumer price index from the US Bureau of Labor Statistics. All values are in end of 2018 dollars for intertemporal comparability. NYSE has by far the largest market capilization, followed by NASDAQ. At the end of 2018, total market value on NYSE was 15,649 billion and 9,386 billion on NASDAQ. AMEX only plays a minor role by now with a total market capitalization of 39 billion at the end of 2018, while other exchanges only make up 15 billion.\ncpi_raw \u0026lt;- read_csv(here::here(\u0026quot;raw/CPIAUCNS.csv\u0026quot;)) cpi \u0026lt;- cpi_raw %\u0026gt;% filter(DATE \u0026lt;= max(crsp$date)) %\u0026gt;% transmute(year = year(DATE), month = month(DATE), cpi = CPIAUCNS) cpi$cpi \u0026lt;- cpi$cpi / cpi$cpi[nrow(cpi)] crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(cpi, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) crsp %\u0026gt;% group_by(exchange, date) %\u0026gt;% summarize(mktcap = sum(mktcap / cpi, na.rm = TRUE) / 1000) %\u0026gt;% ggplot(aes(x = date, y = mktcap, group = exchange)) + geom_line(aes(color = exchange, linetype = exchange)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Total Market Value (Billions of Dec 2018 Dollars)\u0026quot;, color = \u0026quot;Exchange\u0026quot;, linetype = \u0026quot;Exchange\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Now let us turn to the industry composition of the CRSP sample. I follow the below convention on defining industries using the standard industry classification (SIC) codes.\n## define industry labels crsp \u0026lt;- crsp %\u0026gt;% mutate(industry = case_when(siccd \u0026gt;= 1 \u0026amp; siccd \u0026lt;= 999 ~ \u0026quot;Agriculture\u0026quot;, siccd \u0026gt;= 1000 \u0026amp; siccd \u0026lt;= 1499 ~ \u0026quot;Mining\u0026quot;, siccd \u0026gt;= 1500 \u0026amp; siccd \u0026lt;= 1799 ~ \u0026quot;Construction\u0026quot;, siccd \u0026gt;= 2000 \u0026amp; siccd \u0026lt;= 3999 ~ \u0026quot;Manufacturing\u0026quot;, siccd \u0026gt;= 4000 \u0026amp; siccd \u0026lt;= 4999 ~ \u0026quot;Transportation\u0026quot;, siccd \u0026gt;= 5000 \u0026amp; siccd \u0026lt;= 5199 ~ \u0026quot;Wholesale\u0026quot;, siccd \u0026gt;= 5200 \u0026amp; siccd \u0026lt;= 5999 ~ \u0026quot;Retail\u0026quot;, siccd \u0026gt;= 6000 \u0026amp; siccd \u0026lt;= 6799 ~ \u0026quot;Finance\u0026quot;, siccd \u0026gt;= 7000 \u0026amp; siccd \u0026lt;= 8999 ~ \u0026quot;Services\u0026quot;, siccd \u0026gt;= 9000 \u0026amp; siccd \u0026lt;= 9999 ~ \u0026quot;Public\u0026quot;, TRUE ~ \u0026quot;Missing\u0026quot;)) The figure below plots the number of stocks in the sample in each of the SIC industries. Most of the stocks are apparently in Manufacturing albeit the number peaked somewhere in the 90ies. The number of public administration stocks seems to the the only category on the rise in recent years.\n## plot number of stocks by industry crsp %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% count(industry, date) %\u0026gt;% ggplot(aes(x = date, y = n, group = industry)) + geom_line(aes(color = industry, linetype = industry)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Securities\u0026quot;, color = \u0026quot;Industry\u0026quot;, linetype = \u0026quot;Industry\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Let us repeat the excercise by computing the market value of all stocks belonging to the respective industries. All values are again in terms of billions of end of 2018 dollars. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Towards the end of the sample financial firms and services begin to make up a substantial portion of the market value.\ncrsp %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% group_by(industry, date) %\u0026gt;% summarize(mktcap = sum(mktcap / cpi, na.rm = TRUE) / 1000) %\u0026gt;% ggplot(aes(x = date, y = mktcap, group = industry)) + geom_line(aes(color = industry, linetype = industry)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Total Market Value (Billions of Dec 2018 Dollars)\u0026quot;, color = \u0026quot;Industry\u0026quot;, linetype = \u0026quot;Industry\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Next, I turn to the calculation of returns. Monthly stock returns are in the ret field (i.e., the return realized by holding the stock from its last trade in the previous month to its last trade in the current month). However, we have to adjust for delistings which are fortunately recorded by CRSP. The adjustment procedure below follows Shumway (1997). Apparently, the adjustment kicks in on both tails of the return distribution as we can see from the summary statistics.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(ret_adj = case_when(!is.na(dlret) ~ dlret, is.na(dlret) \u0026amp; !is.na(dlstcd) ~ -100, is.na(dlret) \u0026amp; (dlstcd %in% c(500, 520, 580, 584) | (dlstcd \u0026gt;= 551 \u0026amp; dlstcd \u0026lt;= 574)) ~ -30, TRUE ~ ret)) %\u0026gt;% select(-c(dlret, dlstcd)) summary(crsp %\u0026gt;% select(ret, ret_adj))  ## ret ret_adj ## Min. : -99.36 Min. :-100.00 ## 1st Qu.: -6.41 1st Qu.: -6.43 ## Median : 0.00 Median : 0.00 ## Mean : 1.17 Mean : 1.03 ## 3rd Qu.: 6.98 3rd Qu.: 6.94 ## Max. :2400.00 Max. :4700.00 ## NA\u0026#39;s :121850 NA\u0026#39;s :103306 As a main reference point for each stock return, we want to consider the return on the market. According to the Capital Asset Pricing Model (CAPM), cross-sectional variation in expected asset returns should be a function of the covariance between the return of the asset and the return on the market portfolio. The value-weighted portfolio of all US-based common stocks in the CRSP database is one of the main proxies used in the empirical asset pricing literature which I also get from WRDS.\ncrspa_si \u0026lt;- read_csv(here::here(\u0026quot;raw/crspa_si.csv\u0026quot;)) colnames(crspa_si) \u0026lt;- tolower(colnames(crspa_si)) crsp_market \u0026lt;- crspa_si %\u0026gt;% transmute(year = year(ymd(date)), month = month(ymd(date)), vwretd = vwretd * 100, ewretd = ewretd * 100) crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(crsp_market, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) In most cases, it makes sense to use stock returns in excess of the risk-free security over the same period. I take the monthly risk-free rate from Ken French’s data library and also add the other Fama-French factors while we are already at it. In particular, we also add another measure for the return on the market portfolio provided by Fama and French. The main difference to the CRSP measure lies in the fact that Fama and French exclude firms that are not based in the US, closed-end funds and other securities that are not common stocks. I’ll discuss the other factors in later notes where I’ll try to replicate them.\nfactors_ff \u0026lt;- read_csv(here::here(\u0026quot;raw/F-F_Research_Data_Factors.csv\u0026quot;), skip = 3) factors_ff \u0026lt;- factors_ff %\u0026gt;% transmute(date = ymd(paste0(X1, \u0026quot;01\u0026quot;)), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF), smb_ff = as.numeric(SMB), hml_ff = as.numeric(HML),) %\u0026gt;% filter(date \u0026lt;= max(crsp$date)) %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% select(-date) crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(factors_ff, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) As a final descriptive statistic, I plot the cumulative returns of our market measures (i.e., equal-weighted, value-weighted and the Fama-French market factor). Clearly, the value-weighted CRSP index and the Fama-French market factor are highly correlated, while the equal-weighted CRSP index stands out due its high returns apparently driven by smaller stocks.\nmarket \u0026lt;- crsp %\u0026gt;% distinct(date, vwretd, ewretd, mkt_ff) %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;portfolio\u0026quot;, values_to = \u0026quot;return\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% mutate(cum_return = cumsum(return)) %\u0026gt;% ungroup() market %\u0026gt;% mutate(portfolio = case_when(portfolio == \u0026quot;vwretd\u0026quot; ~ \u0026quot;CRSP (Value-Weighted)\u0026quot;, portfolio == \u0026quot;ewretd\u0026quot; ~ \u0026quot;CRSP (Equal-Weighted)\u0026quot;, portfolio == \u0026quot;mkt_ff\u0026quot; ~ \u0026quot;MKT (Fama-French)\u0026quot;)) %\u0026gt;% ggplot(aes(x = date, y = cum_return, group = portfolio)) + geom_line(aes(color = portfolio, linetype = portfolio)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Log Return (in %)\u0026quot;, color = \u0026quot;Portfolio\u0026quot;, linetype = \u0026quot;Portfolio\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() The focus of empirical asset pricing is to examine the ability of different stock characteristics to predict the cross section of future stock returns. This is why I add the (excess) returns for the subsequent month of each stock to the sample. Note that I use beginning of month dates to ensure correct matching of dates.\ncrsp_f1 \u0026lt;- crsp %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;) %m-% months(1), ret_excess = ret - rf_ff, ret_adj_excess = ret_adj - rf_ff) %\u0026gt;% select(permno, date_start, ret_f1 = ret, ret_adj_f1 = ret_adj, ret_excess_f1 = ret_excess, ret_adj_excess_f1 = ret_adj_excess) crsp \u0026lt;- crsp %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;)) %\u0026gt;% left_join(crsp_f1, by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) To wrap up this note, I arrange everything by security and date before saving the sample and provide some summary statistics of all the variables that were used above.\ncrsp \u0026lt;- crsp %\u0026gt;% arrange(permno, date) write_rds(crsp, here::here(\u0026quot;data/crsp.rds\u0026quot;)) summary(crsp) ## permno date ret shrout ## Min. :10000 Min. :1925-12-31 Min. : -99.36 Min. : 0 ## 1st Qu.:22103 1st Qu.:1977-01-31 1st Qu.: -6.41 1st Qu.: 2141 ## Median :49403 Median :1990-08-31 Median : 0.00 Median : 6615 ## Mean :50216 Mean :1987-10-29 Mean : 1.17 Mean : 39672 ## 3rd Qu.:78394 3rd Qu.:2001-10-31 3rd Qu.: 6.98 3rd Qu.: 22510 ## Max. :93436 Max. :2018-12-31 Max. :2400.00 Max. :29206400 ## NA\u0026#39;s :121850 NA\u0026#39;s :3707 ## altprc exchcd siccd mktcap ## Min. : -1832.5 Min. :-2.000 Min. : 0 Min. : 0.0 ## 1st Qu.: 1.6 1st Qu.: 1.000 1st Qu.:3079 1st Qu.: 15.6 ## Median : 10.7 Median : 2.000 Median :3841 Median : 65.2 ## Mean : 25.7 Mean : 2.106 Mean :4493 Mean : 1409.7 ## 3rd Qu.: 25.0 3rd Qu.: 3.000 3rd Qu.:6030 3rd Qu.: 345.2 ## Max. :326000.0 Max. :33.000 Max. :9999 Max. :1099436.1 ## NA\u0026#39;s :71504 NA\u0026#39;s :2380 NA\u0026#39;s :71641 ## exchange cpi industry ret_adj ## Length:3601076 Min. :0.05015 Length:3601076 Min. :-100.00 ## Class :character 1st Qu.:0.23285 Class :character 1st Qu.: -6.43 ## Mode :character Median :0.52382 Mode :character Median : 0.00 ## Mean :0.50056 Mean : 1.03 ## 3rd Qu.:0.70731 3rd Qu.: 6.94 ## Max. :1.00658 Max. :4700.00 ## NA\u0026#39;s :103306 ## vwretd ewretd rf_ff mkt_ff ## Min. :-29.173 Min. :-31.275 Min. :-0.060 Min. :-29.100 ## 1st Qu.: -1.761 1st Qu.: -1.987 1st Qu.: 0.150 1st Qu.: -1.750 ## Median : 1.264 Median : 1.402 Median : 0.390 Median : 1.270 ## Mean : 0.915 Mean : 1.116 Mean : 0.372 Mean : 0.933 ## 3rd Qu.: 3.893 3rd Qu.: 4.231 3rd Qu.: 0.510 3rd Qu.: 3.920 ## Max. : 39.414 Max. : 66.594 Max. : 1.350 Max. : 38.950 ## NA\u0026#39;s :497 NA\u0026#39;s :497 NA\u0026#39;s :3627 NA\u0026#39;s :3627 ## smb_ff hml_ff date_start ## Min. :-16.860 Min. :-13.280 Min. :1925-12-01 ## 1st Qu.: -1.660 1st Qu.: -1.230 1st Qu.:1977-01-01 ## Median : 0.040 Median : 0.260 Median :1990-08-01 ## Mean : 0.146 Mean : 0.369 Mean :1987-09-30 ## 3rd Qu.: 1.860 3rd Qu.: 1.770 3rd Qu.:2001-10-01 ## Max. : 36.700 Max. : 35.460 Max. :2018-12-01 ## NA\u0026#39;s :3627 NA\u0026#39;s :3627 ## ret_f1 ret_adj_f1 ret_excess_f1 ret_adj_excess_f1 ## Min. : -99.36 Min. :-100.00 Min. : -99.52 Min. :-101.31 ## 1st Qu.: -6.41 1st Qu.: -6.43 1st Qu.: -6.79 1st Qu.: -6.81 ## Median : 0.00 Median : 0.00 Median : -0.37 Median : -0.37 ## Mean : 1.17 Mean : 1.03 Mean : 0.80 Mean : 0.66 ## 3rd Qu.: 6.98 3rd Qu.: 6.94 3rd Qu.: 6.64 3rd Qu.: 6.61 ## Max. :2400.00 Max. :4700.00 Max. :2399.66 Max. :4699.24 ## NA\u0026#39;s :122004 NA\u0026#39;s :103476 NA\u0026#39;s :125006 NA\u0026#39;s :106484  ","date":1579824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579824000,"objectID":"b057ae3769c2f259d5f198f960ccfd0e","permalink":"https://christophscheuch.github.io/post/asst-pricing/crsp-sample/","publishdate":"2020-01-24T00:00:00Z","relpermalink":"/post/asst-pricing/crsp-sample/","section":"post","summary":"On the preparation of monthly return data for empirical research on the cross-section of expected returns.","tags":["Academic"],"title":"Tidy Asset Pricing - Part I: The CRSP Sample in R","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":"In Hautsch, et al. (2019), we focus on the market for Bitcoin (BTC) against US Dollar (USD) and estimate the corresponding spotvolatilities for 16 different exchanges. In this note, I replicate the estimation procedure for the spotvolatilities of other cryptocurrencies, namely Ether (ETH), Litecoin (LTC), and Ripple (XRP) against USD.\nBefore I proceed to the results, I briefly sketch the estimation procedure. To estimate the spot volatility, I follow the approach of Kristensen (2010). For each market $i$, asset $j$, and minute $t$, I estimate $\\sigma_{i,j,t}^2$ by $$\\widehat{{\\sigma}_{i,j,t}}^2(h_T) = \\sum\\limits_{l=1}^\\infty K\\left(l - t, h_T\\right)\\left(b_{i,j,l} - b_{i,j,l-1}\\right)^2,$$ where $K\\left(l - t, h_T\\right)$ is a one-sided Gaussian kernel smoother with bandwidth $h_T$ and $b_{i,j,l}$ corresponds to the quoted bid price on market $i$ for asset $j$ at time $l$. The choice of the bandwidth $h_T$ involves a trade-off between the variance and the bias of the estimator. Using too many observations introduces a bias if the volatility is time-varying, whereas shrinking the estimation window through a lower bandwidth increases the variance of the estimator. Kristensen (2010) hence proposes to choose $h_T$ such that information on day $T-1$ is used for the estimation on day $T$, i.e., the bandwith on any day is the result of minimizing the integrated squared error of estimates on the previous day.\nI employ the data that I collected together with my colleague Stefan Voigt. Since January 2018, we gather the first 25 levels of all open buy and sell orders of a couple of exchanges on a minute level using our R package. The spotvolatility estimation procedure from above, however, only rests on the first level of bids. To ensure comparability across asset pairs, I focus on exchanges in our sample that feature trading of all four asset pairs (Binance, Bbitfinex, Bitstamp, Cex, Gate, Kraken, Lykke, Poloniex, xBTCe).\nNow, let us take a look at the resulting volatility estimates for all asset pairs. The app below shows the average daily spotvolatility estimate across all exchanges (solid lines) and corresponding range of average daily spotvolatility estimates (shaded areas). Overall, all four asset pairs exhibit a strong correlation over the last two years. Feel free to play around with the app by comparing asset pairs seperately or focussing on subperiods.\n Shiny App Iframe    \n","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"fa8ef2201f05612cb5e54b1b492fbc4f","permalink":"https://christophscheuch.github.io/post/settlement-latency/spotvolas/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/post/settlement-latency/spotvolas/","section":"post","summary":"An application of the spotvolatility estimator from [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) to other cryptocurrencies","tags":["Academic"],"title":"Volatility in Cryptocurrency Markets","type":"post"},{"authors":["[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1575590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575590400,"objectID":"de2008daaf489ae7401ced461e486911","permalink":"https://christophscheuch.github.io/publication/demand-uncertainty/","publishdate":"2019-12-06T00:00:00Z","relpermalink":"/publication/demand-uncertainty/","section":"publication","summary":"Reward-based crowdfunding allows entrepreneurs to sell claims on future products to finance investments. Entrepreneurs thereby generate demand information and may condition their investment decisions on it. I characterize the profit-maximizing crowdfunding mechanism in a setting where consumers have private information about product valuations. Two types of demand uncertainty matter for the profit-maximizing mechanism: the number of consumers who value the product and the magnitude of their valuation. Entrepreneurs may finance all viable projects by committing to prices that decrease in the number of pledgers, thus granting consumers with high valuations an information rent. If the forgone rent is large, however, entrepreneurs prefer fixed high prices that preclude financing of demand states with low valuations.","tags":null,"title":"Crowdfunding with Private Consumer Valuations","type":"publication"},{"authors":["[Francesco D'Acunto](http://www.francescodacunto.com/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Michael Weber](http://faculty.chicagobooth.edu/michael.weber/)"],"categories":null,"content":"","date":1575417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575417600,"objectID":"a7c6029d7166af6cc3091628ba3cdea0","permalink":"https://christophscheuch.github.io/publication/perceived-precautionary-savings/","publishdate":"2019-12-04T00:00:00Z","relpermalink":"/publication/perceived-precautionary-savings/","section":"publication","summary":"We study the consumption response to the provision of credit lines to individuals that previously did not have access to credit combined with the possibility to elicit directly a large set of preferences, beliefs, and motives. As expected, users react to the availability of credit by increasing their spending permanently and reallocating consumption from non-discretionary to discretionary goods and services. Surprisingly, though, liquid users react more than others and this pattern is a robust feature of the data. Moreover, liquid users lower their savings rate, but do not tap into negative deposits. The credit line seems to act as a form of insurance against future negative shocks and its mere presence makes users spend their existing liquidity without accumulating any debt. By eliciting preferences, beliefs, and motives directly, we show these results are not fully consistent with models of financial constraints, buffer stock models with and without durables, present-bias preferences, uncertainty about future income, bequest motives, or the canonical life-cycle permanent income model. We label this channel the perceived precautionary savings channel, because liquid households behave as if they faced strong precautionary savings motives even though no observables suggest they should based on standard theoretical models. ","tags":null,"title":"Perceived Precautionary Savings Motives: Evidence from FinTech","type":"publication"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":" In its essence, the distributed ledger technology (DLT) is a digital record-keeping system that allows for the verification, updating and storage of transfers of ownership without the need for a designated third party. It relies on a single ledger that is distributed among many different parties who are incentivized to ensure a truthful representation of the transaction history. Nakamoto first popularized the idea of DLTs in a financial context with the Bitcoin protocol and the underlying concept of blockchain. Nowadays, however, Bitcoin is just one of several hundred applications that use the blockchain technology, while other forms of DLTs, in particular directed acyclic graphs (DAG), are actively explored as well. In the following, we first describe the building blocks of DLT before we turn to a more detailed discussion of blockchains and DAGs.\nFundamentals of distributed ledgers DLT solves the fundamental problems that arise in the context of digital transfer of ownership. Transactions are pieces of information that agents authorize to be sent to other agents. A record-keeping system has to ensure that transactions are signed and recorded in the correct order. In principle, a single authority could verify signatures and consistency of transactions, but it would be prone to failures. As a result, it might be desirable to distribute this type of information to a system with multiple machines that can sustain the failure of single units. A fault-tolerant design would enable a system to continue its operation, even when one or several units stop working.\nTo achieve this goal, DLT essentially combines two fundamental concepts. First, a distributed ledger is based on asymmetrical cryptography that enables digital signatures of transactions. On the one hand, the sender of a transaction wants to be the sole owner of the signature that allows the transfer of assets from her private wealth. On the other hand, a record-keeping system requires information about the identities of the parties involved. Cryptographic algorithms ensure that any private keys are only known to their owners, while public keys may be disseminated widely. That is, everybody can check whether a private key is valid, but nobody can back out the private key from public information.1\nSecond, a distributed ledger is conceptually a distributed system which checks whether transactions should be in the system and in which order. In a distributed system, many machines are connected through a network and ensure that the system keeps operating even when some machines fail or try to mess up the system. For instance, if the sender of a transaction is also a potential validator, then she has an incentive for dishonest behavior, such as double-spending or revoking transactions. Individual machines have to reach some form of consensus about actual transaction histories. This consensus can be achieved through different network structures such as blockchain or DAG.\nBlockchain In the context of blockchain, the typical solution to the consensus problem involves competition among potential validators for the right to append information to the ledger.2 The most common consensus protocol, Proof-of-Work (PoW), involves solving a computationally expensive problem where the winner gets the right to update the ledger and typically receives a reward. This particular form of DLT is called blockchain since transactions are not verified individually, but rather appended to the ledger in blocks. Validators bundle transactions that wait for verification and try to solve the problem. However, the system\u0026rsquo;s protocol limits the number of transactions that can be included in a single block. This limit leads to a queue of unconfirmed transactions and validators are free to choose the transactions they try to append to the blockchain. Average verification times thus not only depend on the number of unconfirmed transactions, but also on the fee associated with a transaction, as validators find it more attractive to include transactions with high fees in their blocks.\nThe computationally difficult problem typically relies on cryptographic hash functions, which map an input of arbitrary size to output of fixed size and cannot be inverted. In the Bitcoin network, validators bundle the information of several transactions and a reference to the current state of the blockchain and plug the data into a hash function. The hash function converts this input into a sequence of characters and numbers of certain length. The system\u0026rsquo;s protocol then requires that the output starts with a certain number of zeros. The probability of calculating a hash that starts with many zeros is very low and to generate a new hash, validators include a random number called nonce that can lead to a very different output. The difficulty of the problem is then determined by the number of leading zeros validators have to find. Depending on total available computational power, the system regularly adjusts the target to achieve an average of 10 minutes between two consecutive blocks.\nWhile validators in a PoW system utilize substantial computational resources to win the competition for block generation, validators might also be randomly chosen based on their wealth. In the so called Proof-of-Stake (PoS) protocol, validators stake their tokens to be able to create blocks. The higher a validator\u0026rsquo;s stake, the higher are the chances of creating the next block. After successfully appending a new block, the validator receives transaction fees just as in the case of PoW. If the validator submits an incorrect block or is offline during a staking period, then she is penalized and (at least partly) loses her stake. The penalty might either arise explicitly through a deduction of funds from the stake or implicitly as dishonest behavior creates a feedback on the value of the stake. In particular, if a validator appends to the blockchain in a way that perpetuates disagreement, then she imposes a cost upon all users of the particular blockchain. Such behavior lowers the value of the whole network and is also reflected in a lower valuation of the misbehaving validator\u0026rsquo;s stake. The endogenous feedback between validators\u0026rsquo; behavior and the value of their stakes incentives them to eventually reach consensus.\nOther consensus protocols combine features of PoW and PoS. For instance, delegated Proof-of-Stake (DPoS) relies both on stakeholders, who elect validators and have voting rights proportional to their stake, and validators, who exert effort to append information to the ledger. The reputation of validators determines their chance for reelection, while stakeholders have incentives to select truthful validators.\nDirected acyclic graphs While blockchains record transactions in blocks, DAGs store information in single transactions. More specifically, any transaction represents a node in a graph (i.e., a set of vertices connected by edges) and each new transaction confirms at least one previous transaction, depending on the configuration of the underlying protocol. The longer the branch on which a transaction is based, the more certain is its validity. Intuitively, only once a transaction is broadcasted sufficiently throughout the network, it is verified. DAGs thus hinge on a steady flow of new transactions that enter the network to verify and reference old transactions. The connections between transactions are directed (i.e., the edges in the form of confirmations are one way) and the whole graph is acyclic (i.e., it is impossible to traverse the entire graph starting from a single edge). Given a high number of transactions, DAG ledgers scale better and can achieve consensus faster than blockchains which rely on fixed block sizes and limited verification rates.\nSettlement latency in distributed systems A distributed system features settlement latency in transaction verification, as it is ex-ante unclear how long it takes until validators achieve consensus or to broadcast that consensus through the network. For PoW, latency depends on the time it takes for validators to find a solution to the computationally expensive problem. In the Bitcoin protocol, for instance, validators append a new block on average every 10 minutes, while the process takes about 20 seconds in the Ethereum protocol. For PoS, latency depends on how long disagreement on the correct order of transactions persists. For both blockchain-based protocols, the information still needs to be distributed to all other nodes in the network, possibly facing technological limitations that prevent instant percolation. Technological limits are also particularly relevant for DAGs which rely on a large number of nodes that verify transactions and distribute information through the network. Overall, any distributed system that refrains from using designated third-parties bearing the counterparty risk associated with transactions thus features settlement latency.\n The most simple illustrative example for asymmetric cryptography is the multiplication of prime numbers. One can easily multiply two prime numbers (private key) to get a large number (public key), but it can be difficult to infer the initial set of numbers from the product.\r^ The problem is more severe in a permissionless blockchain where anybody can access and potentially update the blockchain. Other variants, where only few institutions or individuals are entitled to direct access to the blockchain, so-called permissioned blockchains, limit the problem to few players.\r^   ","date":1575331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575331200,"objectID":"385edcc2abfadfe0677ae2eb7a7d1acd","permalink":"https://christophscheuch.github.io/post/settlement-latency/fundamentals/","publishdate":"2019-12-03T00:00:00Z","relpermalink":"/post/settlement-latency/fundamentals/","section":"post","summary":"A companion piece to [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) on how latency enters blockchain-based settlement","tags":["Academic"],"title":"Distributed Ledger Technology and Settlement Latency","type":"post"},{"authors":["[Nikolaus Hautsch](https://homepage.univie.ac.at/nikolaus.hautsch/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Stefan Voigt](https://voigtstefan.github.io/)"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"7a755c2d16372cc64eb49a7f21d349a3","permalink":"https://christophscheuch.github.io/publication/stochastic-latency/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/stochastic-latency/","section":"publication","summary":"Distributed ledger technologies replace trusted intermediaries with time-consuming consensus protocols to record the transfer of ownership. This settlement latency imposes limits to arbitrage and hinders price discovery. We theoretically derive arbitrage bounds that increase with expected latency, latency uncertainty, volatility and risk aversion. Using Bitcoin orderbook and network data, we estimate arbitrage bounds of on average 121 basis points, explaining 91% of the observed cross-market price differences. Consistent with our theory, periods of high latency risk exhibit large price differences, while asset flows chase arbitrage opportunities. Decentralized settlement without centralized clearing thus introduces a non-trivial friction that affects market efficiency.","tags":null,"title":"Trust Takes Time: Limits to Arbitrage in Decentralized Markets","type":"publication"},{"authors":["[Alexander Mürmann](https://www.wu.ac.at/finance/people/faculty/muermann/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1510790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510790400,"objectID":"56c84d7d745a1cc19c70899dab3e5554","permalink":"https://christophscheuch.github.io/publication/fishing-with-pearls/","publishdate":"2017-11-16T00:00:00Z","relpermalink":"/publication/fishing-with-pearls/","section":"publication","summary":"We provide novel evidence of banks establishing lending relationships with prestigious firms to signal their quality and attract future business. Using survey data on firm-level prestige, we show that lenders compete more intensely for prestigious borrowers and offer lower upfront fees to initiate lending relationships with prestigious firms. We also find that banks expand their lending after winning prestigious clients. Prestigious firms benefit from these relations as they face lower costs of borrowing even though prestige has no predictive power for credit risk. Our results are robust to matched sample analyses and a regression discontinuity design.","tags":null,"title":"Fishing with Pearls: The Value of Lending Relationships with Prestigious Firms","type":"publication"}]