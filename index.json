[{"authors":["admin"],"categories":null,"content":"I am a PhD student at the Vienna Graduate School of Finance. My research interests focus on the economics of technological innovations in the financial industry. I recently released a paper that shows how blockchain-based settlement may introduce limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the availability of overdraft facilities through mobile banking apps. I currently investigate the potential of crowdfunding mechanisms to elicit demand information and improve the screening of viable projects.\nBefore starting my PhD studies, I was a research assistant in the field of labor economics at the Institute for Advanced Studies and an intern at Ithuba Capital and the Austrian Financial Market Authority.\n","date":1580428800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1580428800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://christophscheuch.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at the Vienna Graduate School of Finance. My research interests focus on the economics of technological innovations in the financial industry. I recently released a paper that shows how blockchain-based settlement may introduce limits to arbitrage in cross-market trading. In another paper, I analyze how consumers react to the availability of overdraft facilities through mobile banking apps. I currently investigate the potential of crowdfunding mechanisms to elicit demand information and improve the screening of viable projects.","tags":null,"title":"Christoph Scheuch","type":"authors"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" Seasonalcharts.de claims that stock indices exhibit persistent seasonality that may be exploited through an appropriate trading strategy. As part of a job application, I had to replicate the seasonal pattern for the DAX and then test whether this pattern entails a profitable trading strategy. To sum up, I indeed find that a trading strategy that holds the index only over a specific season outperforms the market significantly, but these results might be driven by a few outliers. The text below references an opinion and is for information purposes only. I do not intend to provide any investment advice.\nThe code is structured in a way that allows for a straight-forward replication of the methodoloty for other indices. The document was knitted in RStudio using the following packages:\nlibrary(tidyverse) # for grammar library(scales) # for pretty breaks in figures library(tidyquant) # for data download library(knitr) # for html knitting library(kableExtra) # for nicer tables Data Preparation First, download data from yahoo finance using the tidyquant package. Note that the DAX was officially launched in July 1988, so this is where our sample starts.\ndax_raw \u0026lt;- tq_get(\u0026quot;^GDAXI\u0026quot;, get = \u0026quot;stock.prices\u0026quot;, from = \u0026quot;1988-07-01\u0026quot;, to = \u0026quot;2019-11-30\u0026quot;)  Then, select only date and the adjusted price (i.e., closing price after adjustments for all applicable splits and dividend distributions) as the relevant variables and compute summary statistics to check for missing or weird values. The results are virtually the same if I use unadjusted closing prices.\ndax \u0026lt;- dax_raw %\u0026gt;% select(date, price = adjusted) summary(dax) ## date price ## Min. :1988-07-01 Min. : 1150 ## 1st Qu.:1996-04-04 1st Qu.: 2512 ## Median :2004-01-08 Median : 5286 ## Mean :2004-02-06 Mean : 5657 ## 3rd Qu.:2011-12-03 3rd Qu.: 7445 ## Max. :2019-11-29 Max. :13560 ## NA\u0026#39;s :160 I replace the missing values by the last available index value.\ndax \u0026lt;- dax %\u0026gt;% arrange(date) %\u0026gt;% fill(price, .direction = \u0026quot;down\u0026quot;) As a last immediate plausibility check, I plot the dax over the whole sample period.\ndax %\u0026gt;% ggplot(aes(x = date, y = price)) + geom_line() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Adjusted Price\u0026quot;) + scale_x_date(expand = c(0, 0), breaks = \u0026quot;5 years\u0026quot;) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() The data seems to be good to go, so let us turn to some graphical evidence for seasonality.\n Graphical Evidence for Seasonality To replicate the average seasonal pattern that we can find here, I construct an index of cumulative returns that starts at 100 in each year of my sample. To do so, I nest the data by year (i.e., create a list-column of returns for each full year in the sample).\ndax_nested \u0026lt;- dax %\u0026gt;% filter(date \u0026gt;= \u0026quot;1988-01-01\u0026quot; \u0026amp; date \u0026lt;= \u0026quot;2018-12-31\u0026quot;) %\u0026gt;% mutate(year = year(date)) %\u0026gt;% group_by(year) %\u0026gt;% nest() Next, I define the function that creates the seasonal pattern, given the sample of returns of a specific year, apply the function to each year and compute summary statistics for each trading day across all years in my sample.\nget_seasonality \u0026lt;- function(data) { data %\u0026gt;% arrange(date) %\u0026gt;% mutate(trading_day = 1:n(), # use number of trading days as index ret = (log(price) - lag(log(price)))*100, ret = if_else(is.na(ret), 0, ret), cum_ret = 100 + cumsum(ret)) %\u0026gt;% select(trading_day, ret, cum_ret) } dax_seasonality \u0026lt;- dax_nested %\u0026gt;% mutate(seasonality = map(data, get_seasonality)) %\u0026gt;% select(year, seasonality) %\u0026gt;% unnest(cols = c(seasonality)) %\u0026gt;% ungroup() dax_seasonality_summary \u0026lt;- dax_seasonality %\u0026gt;% group_by(trading_day) %\u0026gt;% summarize(mean = mean(cum_ret), q05 = quantile(cum_ret, 0.05), q95 = quantile(cum_ret, 0.95)) The latter data frame contains the average cumulative return and corresponding quantiles for each trading day of a year in my sample. Let us now take a look at the average pattern across trading days.\ndax_seasonality_summary %\u0026gt;% ggplot(aes(x = trading_day)) + geom_line(aes(y = mean)) + labs(x = \u0026quot;Trading Days\u0026quot;, y = \u0026quot;Cumulative Returns (in %)\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() While it is unclear what exactly Seasonalcharts plots on their website, the above pattern seems to be fairly consistent with their claimed seasonality. However, even if the pattern seems to be on average there, let us add confidence intervals before we think about trading strategies.\ndax_seasonality_summary %\u0026gt;% ggplot(aes(x = trading_day)) + geom_ribbon(aes(ymin = q05, ymax = q95), alpha = 0.25) + geom_line(aes(y = mean)) + labs(x = \u0026quot;Trading Days\u0026quot;, y = \u0026quot;Cumulative Returns (in %)\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + theme_classic() Naturally, these confidence intervals mechanically increase over the course of a year as each index starts at 100. Even with confidence intervals, the pattern is still visible. Given the graphical evidence, let us now turn to the analysis of a trading strategy that aims to exploit this seasonal pattern.\n Trading Strategy The main idea of Seasonalcharts is to implement the strategy proposed by Jacobsen and Bouman (2002) and Jacobsen and Zhan (2018) which they label ‘The Halloween Indicator’ (or ‘Sell in May Effect’). The main finding of these papers is that stock indices returns seem significantly lower duing the May-October period than during the remainder of the year. The corresponding trading strategy holds an index during the months November-April, but holds the risk-free asset in the May-October period.\nTo replicate their approach (and avoid noise in the daily data), I focus on monthly returns from now on.\ndax_monthly \u0026lt;- dax %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% group_by(year, month) %\u0026gt;% slice(which.max(date)) %\u0026gt;% ungroup() %\u0026gt;% arrange(date) %\u0026gt;% mutate(ret = (log(price) - lag(log(price))) * 100) %\u0026gt;% na.omit() nrow(dax_monthly) ## [1] 376 As usual in empirical asset pricing, we do not care about raw returns, but returns in excess of the risk-free asset. I simply add the European risk free rate from the Fama-French data library as the corresponding reference point. Of course, one could use other measures for the risk-free rate, but the impact on the results won’t be substantial.\ntemp \u0026lt;- tempfile() download.file(\u0026quot;https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Europe_3_Factors_CSV.zip\u0026quot;, temp) unzip(temp, \u0026quot;Europe_3_Factors.csv\u0026quot;) rf_raw \u0026lt;- read_csv(\u0026quot;Europe_3_Factors.csv\u0026quot;, skip = 3) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## `Mkt-RF` = col_character(), ## SMB = col_character(), ## HML = col_character(), ## RF = col_character() ## ) rf \u0026lt;- rf_raw %\u0026gt;% mutate(date = ymd(paste0(X1, \u0026quot;01\u0026quot;)), year = year(date), month = month(date), rf = as.numeric(RF)) %\u0026gt;% filter(date \u0026lt;= \u0026quot;2019-09-01\u0026quot;) %\u0026gt;% select(year, month, rf) dax_monthly \u0026lt;- dax_monthly %\u0026gt;% left_join(rf, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% mutate(excess_ret = ret - rf) %\u0026gt;% na.omit() nrow(dax_monthly) # lose a few obs b/c ff data starts in july 1990 ## [1] 351 Next, let us examine the average excess returns per month in our sample. Before we do that, I define a new (factor) column that ensures that R sorts months correctly in the following analyses.\ndax_monthly$month_factor \u0026lt;- factor(x = months(dax_monthly$date), levels = c(\u0026quot;January\u0026quot;, \u0026quot;February\u0026quot;, \u0026quot;March\u0026quot;, \u0026quot;April\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;June\u0026quot;, \u0026quot;July\u0026quot;, \u0026quot;August\u0026quot;, \u0026quot;September\u0026quot;, \u0026quot;October\u0026quot;, \u0026quot;November\u0026quot;, \u0026quot;December\u0026quot;)) Let us now take a look at the average excess returns per month. I also add the standard deviation, 5% and 95% quantiles, and t-statistic of a t-test of the null hypothesis that average returns are zero in a given month.\ndax_monthly %\u0026gt;% group_by(`Month` = month_factor) %\u0026gt;% summarize(Mean = mean(excess_ret), SD = sd(excess_ret), Q05 = quantile(excess_ret, 0.05), Q95 = quantile(excess_ret, 0.95), `t-Statistic` = sqrt(n()) * mean(excess_ret) / sd(excess_ret) ) %\u0026gt;% kable(digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;))   Month  Mean  SD  Q05  Q95  t-Statistic      January  0.50  5.93  -9.88  8.46  0.45    February  0.63  5.49  -8.72  8.16  0.62    March  0.38  4.46  -6.05  7.41  0.46    April  2.78  5.68  -4.24  13.02  2.63    May  0.11  4.20  -5.90  5.73  0.14    June  -0.69  4.55  -8.13  5.48  -0.82    July  0.95  6.38  -7.59  9.23  0.82    August  -3.07  7.05  -19.05  2.87  -2.38    September  -2.94  8.46  -19.82  5.66  -1.90    October  2.18  6.49  -9.12  11.57  1.80    November  1.79  4.27  -4.86  6.70  2.25    December  1.43  5.43  -6.27  8.53  1.41     August and September seem to usually exhibit negative excess returns with an average of about -3% (statistically significant) over all years, while April and November are the only months that tend to exhibit statistically significant positive excess returns. For a graphical illustration of the above table, I complement it with boxplots for each month. The takeaway is essentially the same, but we can see that August and September exhibit a couple of outliers that might considerably drive the results.\ndax_monthly %\u0026gt;% ggplot(aes(x = month_factor, y = excess_ret)) + geom_boxplot() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Monthly Excess Return (in %)\u0026quot;) + theme_classic() Let us proceed to test for the presence of statistically significant excess returns due to seasonal patterns. In the above table, I only test for significance for each month seperately. To test for positive returns in a joint model, I regress the monthly excess returns on month indicators. Note that I always adjust the standard errors to be heteroskedasticity robust.\nsummary(lm(excess_ret ~ month_factor, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ month_factor, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.5324 -3.0783 0.7171 3.7981 16.4934 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.4970 1.0852 0.458 0.6473 ## month_factorFebruary 0.1380 1.5347 0.090 0.9284 ## month_factorMarch -0.1179 1.5347 -0.077 0.9388 ## month_factorApril 2.2835 1.5347 1.488 0.1377 ## month_factorMay -0.3909 1.5347 -0.255 0.7991 ## month_factorJune -1.1862 1.5347 -0.773 0.4401 ## month_factorJuly 0.4564 1.5218 0.300 0.7645 ## month_factorAugust -3.5626 1.5218 -2.341 0.0198 * ## month_factorSeptember -3.4373 1.5218 -2.259 0.0245 * ## month_factorOctober 1.6789 1.5347 1.094 0.2747 ## month_factorNovember 1.2913 1.5347 0.841 0.4007 ## month_factorDecember 0.9298 1.5347 0.606 0.5450 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.844 on 339 degrees of freedom ## Multiple R-squared: 0.08555, Adjusted R-squared: 0.05588 ## F-statistic: 2.883 on 11 and 339 DF, p-value: 0.001222 Seems like August and September have on average indeed lower returns than January (which is the omitted reference point in this regression). Note that the size of the coefficients from the regression are the same as in the table above (i.e., constant plus coefficient). Next, I follow Jacobsen and Bouman (2002) and simply regression excess returns on dummies that indicate specific seasons, i.e., I estimate the model \\[ y_t=\\alpha + \\beta D_t + \\epsilon_t, \\] where \\(D_t\\) is a dummy variable equal to one for the months in a specific season and zero otherwise. I consider both the ‘Halloween’ season (where the dummy is one for November-April) and a `Seasonality’ season which only excludes July-September (and the dummy is one for October-June). If \\(D_t\\) is statistically significant and positive for the corresponding season, then I take this as evidence for the presence of seasonality effects.\nhalloween_months \u0026lt;- c(11, 12, 1, 2, 3, 4) seasonality_months \u0026lt;- c(10, 11, 12, 1, 2, 3, 4, 5, 6) dax_monthly \u0026lt;- dax_monthly %\u0026gt;% mutate(halloween = if_else(month %in% halloween_months, 1L, 0L), seasonality = if_else(month %in% seasonality_months, 1L, 0L)) The first model considers the `Halloween’ effect:\nsummary(lm(excess_ret ~ halloween, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ halloween, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.8772 -2.9011 0.5344 3.8357 18.0227 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.5954 0.4473 -1.331 0.18402 ## halloween 1.8465 0.6353 2.906 0.00389 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.951 on 349 degrees of freedom ## Multiple R-squared: 0.02363, Adjusted R-squared: 0.02083 ## F-statistic: 8.447 on 1 and 349 DF, p-value: 0.00389 I indeed find evidence that excess returns are higher during the months November-April relative to the remaining months. Let us take this spiel even further by adding even more months:\nsummary(lm(excess_ret ~ seasonality, data = dax_monthly), robust = TRUE) ## ## Call: ## lm(formula = excess_ret ~ seasonality, data = dax_monthly) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.7885 -3.0178 0.5807 4.0105 18.2628 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.6842 0.6226 -2.705 0.007159 ** ## seasonality 2.6952 0.7220 3.733 0.000221 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.906 on 349 degrees of freedom ## Multiple R-squared: 0.0384, Adjusted R-squared: 0.03564 ## F-statistic: 13.94 on 1 and 349 DF, p-value: 0.0002207 The effect seems to be even stronger if I also include October, May and June.\nAs a last step, let us compare five different strategies: (i) buy and hold the index over the full year, (ii) go long in the index outside of the Halloween season and otherwise hold the risk-free asset, (iii) go long in the index outside of the Halloween season and otherwise short the index, (iv) buy the index outside of the extended seasonality period and otherwise invest in the risk-free asset, and (v) go long in the index outside of the extended seasonality period and short the index otherwise. Below I compare the returns of the three different strategies on an annual basis:\ndax_monthly \u0026lt;- dax_monthly %\u0026gt;% mutate(excess_ret_halloween = if_else(halloween == 1, ret, rf), excess_ret_halloween_short = if_else(halloween == 1, ret, -ret), excess_ret_seasonality = if_else(seasonality == 1, ret, rf), excess_ret_seasonality_short = if_else(seasonality == 1, ret, -ret)) dax_monthly %\u0026gt;% group_by(year) %\u0026gt;% summarize(`Buy and Hold` = sum(excess_ret), `Seasonality` = sum(excess_ret_seasonality), `Seasonality-Short` = sum(excess_ret_seasonality_short), `Halloween` = sum(excess_ret_halloween), `Halloween-Short` = sum(excess_ret_halloween_short)) %\u0026gt;% pivot_longer(-year, names_to = \u0026quot;strategy\u0026quot;, values_to = \u0026quot;excess_ret\u0026quot;) %\u0026gt;% ggplot(aes(x = year, group = strategy)) + geom_col(aes(y = excess_ret, fill = strategy), position = \u0026quot;dodge\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Annual Excess Return (in %)\u0026quot;, fill = \u0026quot;Strategy\u0026quot;) + scale_x_continuous(expand = c(0, 0), breaks = pretty_breaks()) + theme_classic() The ‘Halloween’ and ‘Seasonality’ strategies seem to outperform the ‘Buy and Hold’ strategy in most of the years with the ‘Seasonality’ typically outperforming ‘Halloween’. The strategies that short the index rather than holding the risk free assets also outperform their counterparts. Plotting the overall cumulative excess return of the five strategies confirms these conjectures.\ndax_monthly %\u0026gt;% arrange(date) %\u0026gt;% mutate(`Buy and Hold` = 100 + cumsum(excess_ret), `Seasonality` = 100 + cumsum(excess_ret_seasonality), `Seasonality-Short` = 100 + cumsum(excess_ret_seasonality_short), `Halloween` = 100 + cumsum(excess_ret_halloween), `Halloween-Short` = 100 + cumsum(excess_ret_halloween_short)) %\u0026gt;% select(date, `Buy and Hold`, `Seasonality`, `Seasonality-Short`, `Halloween`, `Halloween-Short`) %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;strategy\u0026quot;, values_to = \u0026quot;cum_excess_ret\u0026quot;) %\u0026gt;% ggplot(aes(x = date)) + geom_line(aes(y = cum_excess_ret, color = strategy)) + scale_x_date(expand = c(0, 0), breaks = pretty_breaks()) + scale_y_continuous(breaks = pretty_breaks()) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Excess Return (in %)\u0026quot;, color = \u0026quot;Strategy\u0026quot;) + theme_classic() Which of these strategies might constitute a better investment opportunity? For a very simple assessment, let us compute the corresponding Sharpe ratios. Note that I annualize Sharpe ratios by multiplying them with \\(\\sqrt{12}\\) which strictly speaking only works under IID distributed returns (which is typically unlikely to be the case), but which suffices for the purpose of this note.\nsharpe_ratio \u0026lt;- function(x) { sqrt(12) * mean(x) / sd(x) } dax_monthly %\u0026gt;% arrange(date) %\u0026gt;% summarize(`Buy and Hold` = sharpe_ratio(excess_ret), `Seasonality` = sharpe_ratio(excess_ret_seasonality), `Seasonality-Short` = sharpe_ratio(excess_ret_seasonality_short), `Halloween` = sharpe_ratio(excess_ret_halloween), `Halloween-Short` = sharpe_ratio(excess_ret_halloween_short)) ## # A tibble: 1 x 5 ## `Buy and Hold` Seasonality `Seasonality-Short` Halloween `Halloween-Shor~ ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.184 0.737 0.754 0.774 0.530 The ‘Seasonality-Short’ strategy delivers the highest cumulative excess return in my sample period, but the ‘Halloween’ strategy exhibits a slightly higher Sharpe ratio than the others and thus constitutes a better investment opportunity.\n ","date":1580428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580428800,"objectID":"5f16dee499b553d118a22844c582058c","permalink":"https://christophscheuch.github.io/post/asst-pricing/dax-seasonality/","publishdate":"2020-01-31T00:00:00Z","relpermalink":"/post/asst-pricing/dax-seasonality/","section":"post","summary":"A quick evaluation of seasonal patterns in an equity index","tags":["Academic"],"title":"Seasonality in the DAX","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Asset Pricing"],"content":" As a newbie to the empirical asset pricing literature, I found it quite hard to fully grasp how results come about solely relying on the data descriptions in the corresponding papers. Fortunately, Bali, Engle and Murray provide an excellent textbook that not only provides a comprehensive overview of the empirical research on the cross-section of expected returns, but also offers detailed description of common practices in sample construction and methodology. In this series of notes, I replicate the main results of their textbook in R. Moreover, I try to implement all analyses in a tidy manner using mainly packages beloning to the [tidyverse](https://www.tidyverse.org/) family since it’s fun and fairly transparent.\nIn this first note I start out with the raw monthly CRSP data, compute some descriptive statistics and construct the main sample I use in later analyses. In the next note, I’ll use the constructed sample to investigate the size effect. So let us start with the list of packages that I use throughout the note.\nlibrary(tidyverse) # for grammar library(lubridate) # for working with dates library(scales) # for nicer axes of figures I just downloaded the full monthly CRSP sample from WRDS. The full monthly CRSP sample from December 1925 until December 2018 has about 1.18GB. After reading in the data, I convert all column names to lower case out of a personal preference.\ncrspa_msf \u0026lt;- read_csv(\u0026quot;raw/crspa_msf.csv\u0026quot;) colnames(crspa_msf) \u0026lt;- tolower(colnames(crspa_msf)) nrow(crspa_msf) ## [1] 4568082 Next, I make sure that all relevant variables are correctly parsed.\ncrsp \u0026lt;- crspa_msf %\u0026gt;% transmute(permno = as.integer(permno), # security identifier date = ymd(date), # month identifier ret = as.numeric(ret) * 100, # return (convert to percent) shrout = as.numeric(shrout), # shares outstanding (in thousands) altprc = as.numeric(altprc), # last traded price in a month exchcd = as.integer(exchcd), # exchange code shrcd = as.integer(shrcd), # share code siccd = as.integer(siccd), # industry code dlret = as.numeric(dlret) * 100, # delisting return (converted to percent) dlstcd = as.integer(dlstcd) # delisting code ) nrow(crsp) ## [1] 4568082 As common in the literature, I focus on US-based common stocks in the CRSP database. US-based common stocks are identified with share codes 10 and 11, where the first digit pins down ordinary common shares and the second digit indicates that the secudity has not been further defined (0) or does not need to be further defined (1). I refer to the original CRSP documentation on a full description of the meaning of this distinction.\ncrsp \u0026lt;- crsp %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% select(-shrcd) nrow(crsp) ## [1] 3630251 I apply the following two checks to all data sets I work with. I make sure to keep only distinct observations and I check that there is only one observation by date for each security. The main reason is that I want to work with a nice panel of unique security-date combinations where no pair appears more than once in the data.\ncrsp \u0026lt;- crsp %\u0026gt;% distinct() nrow(crsp) ## [1] 3601076 crsp %\u0026gt;% group_by(permno, date) %\u0026gt;% filter(n() \u0026gt; 1) %\u0026gt;% nrow() == 0 ## [1] TRUE If the last part of the code returns FALSE, I usually investigate where the failure comes from (typically related to reporting errors). Fortunately, everything looks good in the CRSP data.\nFollowing Bali, Engle and Murray, I compute the market capitalization as the absolute value of the product of shares outstanding and the price of the stock as of the end of the last trading day of a given month. I have to take the absolute value since altprc is the negative of average of bid and ask from last traded pricefor which the data is available if there is no last traded price. Since the shares outstanding are reported in thousands of shares, I divide by 1000 such that the resulting measure is in millions of dollars.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(mktcap = abs(shrout * altprc) / 1000, # in millions of dollars mktcap = if_else(mktcap == 0, as.numeric(NA), mktcap))  As a first glance at the data, let us look at the stock exchange composition of the CRSP sample. Stocks listed on NYSDE, AMEX, and NASDAQ are indicated with values of 1 or 31, 2 or 32, and 3 or 33, respectively, in the exchange code filed. I collect stocks traded on all other exchange (e.g., Arca Stock Market, Boston Stock Exchange, Chicago Stock Exchange, etc.) in a separate category.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(exchange = case_when(exchcd %in% c(1, 31) ~ \u0026quot;NYSE\u0026quot;, exchcd %in% c(2, 32) ~ \u0026quot;AMEX\u0026quot;, exchcd %in% c(3, 33) ~ \u0026quot;NASDAQ\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot;)) The figure below plots the number of stocks per exchange. NYSE has the longest history in the data, but NASDAQ exhibits a considerable large number of stocks. Interestingly, the number of stocks on AMEX is decreasing steadily over the last couple of decades. By the end of 2018, there are 2,221 stocks on NASDAQ, 1,274 on NYSE, 167 on AMEX and 3 belonging to the other category.\ncrsp %\u0026gt;% count(exchange, date) %\u0026gt;% ggplot(aes(x = date, y = n, group = exchange)) + geom_line(aes(color = exchange, linetype = exchange)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Securities\u0026quot;, color = \u0026quot;Exchange\u0026quot;, linetype = \u0026quot;Exchange\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Next, let us look at the market capitalization per exchange. To do so, I adjust the market capitalization values for inflation using the using the consumer price index from the US Bureau of Labor Statistics. All values are in end of 2018 dollars for intertemporal comparability. NYSE has by far the largest market capilization, followed by NASDAQ. At the end of 2018, total market value on NYSE was 15,649 billion and 9,386 billion on NASDAQ. AMEX only plays a minor role by now with a total market capitalization of 39 billion at the end of 2018, while other exchanges only make up 15 billion.\ncpi_raw \u0026lt;- read_csv(\u0026quot;raw/CPIAUCNS.csv\u0026quot;) cpi \u0026lt;- cpi_raw %\u0026gt;% filter(DATE \u0026lt;= max(crsp$date)) %\u0026gt;% transmute(year = year(DATE), month = month(DATE), cpi = CPIAUCNS) cpi$cpi \u0026lt;- cpi$cpi / cpi$cpi[nrow(cpi)] crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(cpi, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) crsp %\u0026gt;% group_by(exchange, date) %\u0026gt;% summarize(mktcap = sum(mktcap / cpi, na.rm = TRUE) / 1000) %\u0026gt;% ggplot(aes(x = date, y = mktcap, group = exchange)) + geom_line(aes(color = exchange, linetype = exchange)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Total Market Value (Billions of Dec 2018 Dollars)\u0026quot;, color = \u0026quot;Exchange\u0026quot;, linetype = \u0026quot;Exchange\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Now let us turn to the industry composition of the CRSP sample. I follow the below convention on defining industries using the standard industry classification (SIC) codes.\n## define industry labels crsp \u0026lt;- crsp %\u0026gt;% mutate(industry = case_when(siccd \u0026gt;= 1 \u0026amp; siccd \u0026lt;= 999 ~ \u0026quot;Agriculture\u0026quot;, siccd \u0026gt;= 1000 \u0026amp; siccd \u0026lt;= 1499 ~ \u0026quot;Mining\u0026quot;, siccd \u0026gt;= 1500 \u0026amp; siccd \u0026lt;= 1799 ~ \u0026quot;Construction\u0026quot;, siccd \u0026gt;= 2000 \u0026amp; siccd \u0026lt;= 3999 ~ \u0026quot;Manufacturing\u0026quot;, siccd \u0026gt;= 4000 \u0026amp; siccd \u0026lt;= 4999 ~ \u0026quot;Transportation\u0026quot;, siccd \u0026gt;= 5000 \u0026amp; siccd \u0026lt;= 5199 ~ \u0026quot;Wholesale\u0026quot;, siccd \u0026gt;= 5200 \u0026amp; siccd \u0026lt;= 5999 ~ \u0026quot;Retail\u0026quot;, siccd \u0026gt;= 6000 \u0026amp; siccd \u0026lt;= 6799 ~ \u0026quot;Finance\u0026quot;, siccd \u0026gt;= 7000 \u0026amp; siccd \u0026lt;= 8999 ~ \u0026quot;Services\u0026quot;, siccd \u0026gt;= 9000 \u0026amp; siccd \u0026lt;= 9999 ~ \u0026quot;Public\u0026quot;, TRUE ~ \u0026quot;Missing\u0026quot;)) The figure below plots the number of stocks in the sample in each of the SIC industries. Most of the stocks are apparently in Manufacturing albeit the number peaked somewhere in the 90ies. The number of public administration stocks seems to the the only category on the rise in recent years.\n## plot number of stocks by industry crsp %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% count(industry, date) %\u0026gt;% ggplot(aes(x = date, y = n, group = industry)) + geom_line(aes(color = industry, linetype = industry)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Securities\u0026quot;, color = \u0026quot;Industry\u0026quot;, linetype = \u0026quot;Industry\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Let us repeat the excercise by computing the market value of all stocks belonging to the respective industries. All values are again in terms of billions of end of 2018 dollars. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Towards the end of the sample financial firms and services begin to make up a substantial portion of the market value.\ncrsp %\u0026gt;% filter(!is.na(industry)) %\u0026gt;% group_by(industry, date) %\u0026gt;% summarize(mktcap = sum(mktcap / cpi, na.rm = TRUE) / 1000) %\u0026gt;% ggplot(aes(x = date, y = mktcap, group = industry)) + geom_line(aes(color = industry, linetype = industry)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Total Market Value (Billions of Dec 2018 Dollars)\u0026quot;, color = \u0026quot;Industry\u0026quot;, linetype = \u0026quot;Industry\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() Next, I turn to the calculation of returns. Monthly stock returns are in the ret field (i.e., the return realized by holding the stock from its last trade in the previous month to its last trade in the current month). However, we have to adjust for delistings which are fortunately recorded by CRSP. The adjustment procedure below follows Shumway (1997). Apparently, the adjustment kicks in on both tails of the return distribution as we can see from the summary statistics.\ncrsp \u0026lt;- crsp %\u0026gt;% mutate(ret_adj = case_when(!is.na(dlret) ~ dlret, is.na(dlret) \u0026amp; !is.na(dlstcd) ~ -100, is.na(dlret) \u0026amp; (dlstcd %in% c(500, 520, 580, 584) | (dlstcd \u0026gt;= 551 \u0026amp; dlstcd \u0026lt;= 574)) ~ -30, TRUE ~ ret)) %\u0026gt;% select(-c(dlret, dlstcd)) summary(crsp %\u0026gt;% select(ret, ret_adj))  ## ret ret_adj ## Min. : -99.36 Min. :-100.00 ## 1st Qu.: -6.41 1st Qu.: -6.43 ## Median : 0.00 Median : 0.00 ## Mean : 1.17 Mean : 1.03 ## 3rd Qu.: 6.98 3rd Qu.: 6.94 ## Max. :2400.00 Max. :4700.00 ## NA\u0026#39;s :121850 NA\u0026#39;s :103306 As a main reference point for each stock return, we want to consider the return on the market. According to the Capital Asset Pricing Model (CAPM), cross-sectional variation in expected asset returns should be a function of the covariance between the return of the asset and the return on the market portfolio. The value-weighted portfolio of all US-based common stocks in the CRSP database is one of the main proxies used in the empirical asset pricing literature which I also get from WRDS.\ncrspa_si \u0026lt;- read_csv(\u0026quot;raw/crspa_si.csv\u0026quot;) colnames(crspa_si) \u0026lt;- tolower(colnames(crspa_si)) crsp_market \u0026lt;- crspa_si %\u0026gt;% transmute(year = year(ymd(date)), month = month(ymd(date)), vwretd = vwretd * 100, ewretd = ewretd * 100) crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(crsp_market, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) In most cases, it makes sense to use stock returns in excess of the risk-free security over the same period. I take the monthly risk-free rate from Ken French’s data library and also add the other Fama-French factors while we are already at it. In particular, we also add another measure for the return on the market portfolio provided by Fama and French. The main difference to the CRSP measure lies in the fact that Fama and French exclude firms that are not based in the US, closed-end funds and other securities that are not common stocks. I’ll discuss the other factors in later notes where I’ll try to replicate them.\nfactors_ff \u0026lt;- read_csv(\u0026quot;raw/F-F_Research_Data_Factors.csv\u0026quot;, skip = 3) factors_ff \u0026lt;- factors_ff %\u0026gt;% transmute(date = ymd(paste0(X1, \u0026quot;01\u0026quot;)), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF), smb_ff = as.numeric(SMB), hml_ff = as.numeric(HML),) %\u0026gt;% filter(date \u0026lt;= max(crsp$date)) %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% select(-date) crsp \u0026lt;- crsp %\u0026gt;% mutate(year = year(date), month = month(date)) %\u0026gt;% left_join(factors_ff, by = c(\u0026quot;year\u0026quot;, \u0026quot;month\u0026quot;)) %\u0026gt;% select(-c(year, month)) As a final descriptive statistic, I plot the cumulative returns of our market measures (i.e., equal-weighted, value-weighted and the Fama-French market factor). Clearly, the value-weighted CRSP index and the Fama-French market factor are highly correlated, while the equal-weighted CRSP index stands out due its high returns apparently driven by smaller stocks.\nmarket \u0026lt;- crsp %\u0026gt;% distinct(date, vwretd, ewretd, mkt_ff) %\u0026gt;% pivot_longer(-date, names_to = \u0026quot;portfolio\u0026quot;, values_to = \u0026quot;return\u0026quot;) %\u0026gt;% na.omit() %\u0026gt;% group_by(portfolio) %\u0026gt;% arrange(date) %\u0026gt;% mutate(cum_return = cumsum(return)) %\u0026gt;% ungroup() market %\u0026gt;% mutate(portfolio = case_when(portfolio == \u0026quot;vwretd\u0026quot; ~ \u0026quot;CRSP (Value-Weighted)\u0026quot;, portfolio == \u0026quot;ewretd\u0026quot; ~ \u0026quot;CRSP (Equal-Weighted)\u0026quot;, portfolio == \u0026quot;mkt_ff\u0026quot; ~ \u0026quot;MKT (Fama-French)\u0026quot;)) %\u0026gt;% ggplot(aes(x = date, y = cum_return, group = portfolio)) + geom_line(aes(color = portfolio, linetype = portfolio)) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Cumulative Log Return (in %)\u0026quot;, color = \u0026quot;Portfolio\u0026quot;, linetype = \u0026quot;Portfolio\u0026quot;) + scale_y_continuous(labels = comma, breaks = pretty_breaks()) + scale_x_date(expand = c(0, 0), date_breaks = \u0026quot;10 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_classic() The focus of empirical asset pricing is to examine the ability of different stock characteristics to predict the cross section of future stock returns. This is why I add the (excess) returns for the subsequent month of each stock to the sample. Note that I use beginning of month dates to ensure correct matching of dates.\ncrsp_f1 \u0026lt;- crsp %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;) %m-% months(1), ret_excess = ret - rf_ff, ret_adj_excess = ret_adj - rf_ff, mkt_ff_excess = mkt_ff - rf_ff) %\u0026gt;% select(permno, date_start, ret_f1 = ret, ret_adj_f1 = ret_adj, ret_excess_f1 = ret_excess, ret_adj_excess_f1 = ret_adj_excess, mkt_ff_excess_f1 = mkt_ff_excess) crsp \u0026lt;- crsp %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;)) %\u0026gt;% left_join(crsp_f1, by = c(\u0026quot;permno\u0026quot;, \u0026quot;date_start\u0026quot;)) To wrap up this data set, I arrange everything by security and date before saving the sample and provide some summary statistics of all the variables that were used above. I end up with a monthly CRSP file that has about 700 MB.\ncrsp \u0026lt;- crsp %\u0026gt;% arrange(permno, date) write_rds(crsp, \u0026quot;data/crsp.rds\u0026quot;) summary(crsp) ## permno date ret shrout ## Min. :10000 Min. :1925-12-31 Min. : -99.36 Min. : 0 ## 1st Qu.:22103 1st Qu.:1977-01-31 1st Qu.: -6.41 1st Qu.: 2141 ## Median :49403 Median :1990-08-31 Median : 0.00 Median : 6615 ## Mean :50216 Mean :1987-10-29 Mean : 1.17 Mean : 39672 ## 3rd Qu.:78394 3rd Qu.:2001-10-31 3rd Qu.: 6.98 3rd Qu.: 22510 ## Max. :93436 Max. :2018-12-31 Max. :2400.00 Max. :29206400 ## NA\u0026#39;s :121850 NA\u0026#39;s :3707 ## altprc exchcd siccd mktcap ## Min. : -1832.5 Min. :-2.000 Min. : 0 Min. : 0.0 ## 1st Qu.: 1.6 1st Qu.: 1.000 1st Qu.:3079 1st Qu.: 15.6 ## Median : 10.7 Median : 2.000 Median :3841 Median : 65.2 ## Mean : 25.7 Mean : 2.106 Mean :4493 Mean : 1409.7 ## 3rd Qu.: 25.0 3rd Qu.: 3.000 3rd Qu.:6030 3rd Qu.: 345.2 ## Max. :326000.0 Max. :33.000 Max. :9999 Max. :1099436.1 ## NA\u0026#39;s :71504 NA\u0026#39;s :2380 NA\u0026#39;s :71641 ## exchange cpi industry ret_adj ## Length:3601076 Min. :0.05015 Length:3601076 Min. :-100.00 ## Class :character 1st Qu.:0.23285 Class :character 1st Qu.: -6.43 ## Mode :character Median :0.52382 Mode :character Median : 0.00 ## Mean :0.50056 Mean : 1.03 ## 3rd Qu.:0.70731 3rd Qu.: 6.94 ## Max. :1.00658 Max. :4700.00 ## NA\u0026#39;s :103306 ## vwretd ewretd rf_ff mkt_ff ## Min. :-29.173 Min. :-31.275 Min. :-0.060 Min. :-29.100 ## 1st Qu.: -1.761 1st Qu.: -1.987 1st Qu.: 0.150 1st Qu.: -1.750 ## Median : 1.264 Median : 1.402 Median : 0.390 Median : 1.270 ## Mean : 0.915 Mean : 1.116 Mean : 0.372 Mean : 0.933 ## 3rd Qu.: 3.893 3rd Qu.: 4.231 3rd Qu.: 0.510 3rd Qu.: 3.920 ## Max. : 39.414 Max. : 66.594 Max. : 1.350 Max. : 38.950 ## NA\u0026#39;s :497 NA\u0026#39;s :497 NA\u0026#39;s :3627 NA\u0026#39;s :3627 ## smb_ff hml_ff date_start ## Min. :-16.860 Min. :-13.280 Min. :1925-12-01 ## 1st Qu.: -1.660 1st Qu.: -1.230 1st Qu.:1977-01-01 ## Median : 0.040 Median : 0.260 Median :1990-08-01 ## Mean : 0.146 Mean : 0.369 Mean :1987-09-30 ## 3rd Qu.: 1.860 3rd Qu.: 1.770 3rd Qu.:2001-10-01 ## Max. : 36.700 Max. : 35.460 Max. :2018-12-01 ## NA\u0026#39;s :3627 NA\u0026#39;s :3627 ## ret_f1 ret_adj_f1 ret_excess_f1 ret_adj_excess_f1 ## Min. : -99.36 Min. :-100.00 Min. : -99.52 Min. :-101.31 ## 1st Qu.: -6.41 1st Qu.: -6.43 1st Qu.: -6.79 1st Qu.: -6.81 ## Median : 0.00 Median : 0.00 Median : -0.37 Median : -0.37 ## Mean : 1.17 Mean : 1.03 Mean : 0.80 Mean : 0.66 ## 3rd Qu.: 6.98 3rd Qu.: 6.94 3rd Qu.: 6.64 3rd Qu.: 6.61 ## Max. :2400.00 Max. :4700.00 Max. :2399.66 Max. :4699.24 ## NA\u0026#39;s :122004 NA\u0026#39;s :103476 NA\u0026#39;s :125006 NA\u0026#39;s :106484 ## mkt_ff_excess_f1 ## Min. :-29.130 ## 1st Qu.: -2.040 ## Median : 0.930 ## Mean : 0.558 ## 3rd Qu.: 3.520 ## Max. : 38.850 ## NA\u0026#39;s :28949 Finally, here is the code chunk for setting up the daily return data from CRSP. Note that the raw daily CRSP sample has about 30GB, whereas the final sample is about 3 GB. I just use a scientific cluster with plenty of memory to run this part.\n## read in data crspa_dsf \u0026lt;- read_csv(\u0026quot;raw/crspa_dsf.csv\u0026quot;) colnames(crspa_dsf) \u0026lt;- tolower(colnames(crspa_dsf)) nrow(crspa_dsf) ## keep only common stocks and relevant columns crsp_daily \u0026lt;- crspa_dsf %\u0026gt;% filter(shrcd %in% c(10, 11)) %\u0026gt;% transmute(permno = as.integer(permno), date = ymd(date), ret = as.numeric(ret) * 100 ) nrow(crsp_daily) ## keep only distinct observations crsp_daily \u0026lt;- crsp_daily %\u0026gt;% distinct() nrow(crsp_daily) ## add fama french factors (https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) factors_ff \u0026lt;- read_csv(\u0026quot;raw/F-F_Research_Data_Factors_daily.csv\u0026quot;, skip = 3) factors_ff \u0026lt;- factors_ff %\u0026gt;% transmute(date = ymd(X1), rf_ff = as.numeric(RF), mkt_ff = as.numeric(`Mkt-RF`) + as.numeric(RF)) %\u0026gt;% filter(date \u0026lt;= max(crsp_daily$date)) crsp_daily \u0026lt;- crsp_daily %\u0026gt;% left_join(factors_ff, by = \u0026quot;date\u0026quot;) nrow(crsp_daily) ## drop rows with missing values crsp_daily \u0026lt;- crsp_daily %\u0026gt;% na.omit() nrow(crsp_daily) ## set reference date crsp_daily \u0026lt;- crsp_daily %\u0026gt;% mutate(date_start = floor_date(date, \u0026quot;month\u0026quot;)) ## export sample write_rds(crsp_daily, \u0026quot;data/crsp_daily.rds\u0026quot;)  ","date":1579824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579824000,"objectID":"b057ae3769c2f259d5f198f960ccfd0e","permalink":"https://christophscheuch.github.io/post/asst-pricing/crsp-sample/","publishdate":"2020-01-24T00:00:00Z","relpermalink":"/post/asst-pricing/crsp-sample/","section":"post","summary":"On the preparation of monthly return data for empirical research on the cross-section of expected returns","tags":["Academic"],"title":"Tidy Asset Pricing - Part I: The CRSP Sample in R","type":"post"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":"In Hautsch, et al. (2019), we focus on the market for Bitcoin (BTC) against US Dollar (USD) and estimate the corresponding spotvolatilities for 16 different exchanges. In this note, I replicate the estimation procedure for the spotvolatilities of other cryptocurrencies, namely Ether (ETH), Litecoin (LTC), and Ripple (XRP) against USD.\nBefore I proceed to the results, I briefly sketch the estimation procedure. To estimate the spot volatility, I follow the approach of Kristensen (2010). For each market $i$, asset $j$, and minute $t$, I estimate $\\sigma_{i,j,t}^2$ by $$\\widehat{{\\sigma}_{i,j,t}}^2(h_T) = \\sum\\limits_{l=1}^\\infty K\\left(l - t, h_T\\right)\\left(b_{i,j,l} - b_{i,j,l-1}\\right)^2,$$ where $K\\left(l - t, h_T\\right)$ is a one-sided Gaussian kernel smoother with bandwidth $h_T$ and $b_{i,j,l}$ corresponds to the quoted bid price on market $i$ for asset $j$ at time $l$. The choice of the bandwidth $h_T$ involves a trade-off between the variance and the bias of the estimator. Using too many observations introduces a bias if the volatility is time-varying, whereas shrinking the estimation window through a lower bandwidth increases the variance of the estimator. Kristensen (2010) hence proposes to choose $h_T$ such that information on day $T-1$ is used for the estimation on day $T$, i.e., the bandwith on any day is the result of minimizing the integrated squared error of estimates on the previous day.\nI employ the data that I collected together with my colleague Stefan Voigt. Since January 2018, we gather the first 25 levels of all open buy and sell orders of a couple of exchanges on a minute level using our R package. The spotvolatility estimation procedure from above, however, only rests on the first level of bids. To ensure comparability across asset pairs, I focus on exchanges in our sample that feature trading of all four asset pairs (Binance, Bbitfinex, Bitstamp, Cex, Gate, Kraken, Lykke, Poloniex, xBTCe).\nNow, let us take a look at the resulting volatility estimates for all asset pairs. The app below shows the average daily spotvolatility estimate across all exchanges (solid lines) and corresponding range of average daily spotvolatility estimates (shaded areas). Overall, all four asset pairs exhibit a strong correlation over the last two years. Feel free to play around with the app by comparing asset pairs seperately or focussing on subperiods.\n Shiny App Iframe    \n","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"fa8ef2201f05612cb5e54b1b492fbc4f","permalink":"https://christophscheuch.github.io/post/settlement-latency/spotvolas/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/post/settlement-latency/spotvolas/","section":"post","summary":"An application of the spotvolatility estimator from [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) to other cryptocurrencies","tags":["Academic"],"title":"Volatility in Cryptocurrency Markets","type":"post"},{"authors":["[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1575590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575590400,"objectID":"de2008daaf489ae7401ced461e486911","permalink":"https://christophscheuch.github.io/publication/demand-uncertainty/","publishdate":"2019-12-06T00:00:00Z","relpermalink":"/publication/demand-uncertainty/","section":"publication","summary":"Reward-based crowdfunding allows entrepreneurs to sell claims on future products to finance investments. Entrepreneurs thereby generate demand information and may condition their investment decisions on it. I characterize the profit-maximizing crowdfunding mechanism in a setting where consumers have private information about product valuations. Two types of demand uncertainty matter for the profit-maximizing mechanism: the number of consumers who value the product and the magnitude of their valuation. Entrepreneurs may finance all viable projects by committing to prices that decrease in the number of pledgers, thus granting consumers with high valuations an information rent. If the forgone rent is large, however, entrepreneurs prefer fixed high prices that preclude financing of demand states with low valuations.","tags":null,"title":"Crowdfunding with Private Consumer Valuations","type":"publication"},{"authors":["[Francesco D'Acunto](http://www.francescodacunto.com/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Michael Weber](http://faculty.chicagobooth.edu/michael.weber/)"],"categories":null,"content":"","date":1575417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575417600,"objectID":"a7c6029d7166af6cc3091628ba3cdea0","permalink":"https://christophscheuch.github.io/publication/perceived-precautionary-savings/","publishdate":"2019-12-04T00:00:00Z","relpermalink":"/publication/perceived-precautionary-savings/","section":"publication","summary":"We study the consumption response to the provision of credit lines to individuals that previously did not have access to credit combined with the possibility to elicit directly a large set of preferences, beliefs, and motives. As expected, users react to the availability of credit by increasing their spending permanently and reallocating consumption from non-discretionary to discretionary goods and services. Surprisingly, though, liquid users react more than others and this pattern is a robust feature of the data. Moreover, liquid users lower their savings rate, but do not tap into negative deposits. The credit line seems to act as a form of insurance against future negative shocks and its mere presence makes users spend their existing liquidity without accumulating any debt. By eliciting preferences, beliefs, and motives directly, we show these results are not fully consistent with models of financial constraints, buffer stock models with and without durables, present-bias preferences, uncertainty about future income, bequest motives, or the canonical life-cycle permanent income model. We label this channel the perceived precautionary savings channel, because liquid households behave as if they faced strong precautionary savings motives even though no observables suggest they should based on standard theoretical models. ","tags":null,"title":"Perceived Precautionary Savings Motives: Evidence from FinTech","type":"publication"},{"authors":["Christoph Scheuch"],"categories":["Blockchain"],"content":" In its essence, the distributed ledger technology (DLT) is a digital record-keeping system that allows for the verification, updating and storage of transfers of ownership without the need for a designated third party. It relies on a single ledger that is distributed among many different parties who are incentivized to ensure a truthful representation of the transaction history. Nakamoto first popularized the idea of DLTs in a financial context with the Bitcoin protocol and the underlying concept of blockchain. Nowadays, however, Bitcoin is just one of several hundred applications that use the blockchain technology, while other forms of DLTs, in particular directed acyclic graphs (DAG), are actively explored as well. In the following, we first describe the building blocks of DLT before we turn to a more detailed discussion of blockchains and DAGs.\nFundamentals of distributed ledgers DLT solves the fundamental problems that arise in the context of digital transfer of ownership. Transactions are pieces of information that agents authorize to be sent to other agents. A record-keeping system has to ensure that transactions are signed and recorded in the correct order. In principle, a single authority could verify signatures and consistency of transactions, but it would be prone to failures. As a result, it might be desirable to distribute this type of information to a system with multiple machines that can sustain the failure of single units. A fault-tolerant design would enable a system to continue its operation, even when one or several units stop working.\nTo achieve this goal, DLT essentially combines two fundamental concepts. First, a distributed ledger is based on asymmetrical cryptography that enables digital signatures of transactions. On the one hand, the sender of a transaction wants to be the sole owner of the signature that allows the transfer of assets from her private wealth. On the other hand, a record-keeping system requires information about the identities of the parties involved. Cryptographic algorithms ensure that any private keys are only known to their owners, while public keys may be disseminated widely. That is, everybody can check whether a private key is valid, but nobody can back out the private key from public information.1\nSecond, a distributed ledger is conceptually a distributed system which checks whether transactions should be in the system and in which order. In a distributed system, many machines are connected through a network and ensure that the system keeps operating even when some machines fail or try to mess up the system. For instance, if the sender of a transaction is also a potential validator, then she has an incentive for dishonest behavior, such as double-spending or revoking transactions. Individual machines have to reach some form of consensus about actual transaction histories. This consensus can be achieved through different network structures such as blockchain or DAG.\nBlockchain In the context of blockchain, the typical solution to the consensus problem involves competition among potential validators for the right to append information to the ledger.2 The most common consensus protocol, Proof-of-Work (PoW), involves solving a computationally expensive problem where the winner gets the right to update the ledger and typically receives a reward. This particular form of DLT is called blockchain since transactions are not verified individually, but rather appended to the ledger in blocks. Validators bundle transactions that wait for verification and try to solve the problem. However, the system\u0026rsquo;s protocol limits the number of transactions that can be included in a single block. This limit leads to a queue of unconfirmed transactions and validators are free to choose the transactions they try to append to the blockchain. Average verification times thus not only depend on the number of unconfirmed transactions, but also on the fee associated with a transaction, as validators find it more attractive to include transactions with high fees in their blocks.\nThe computationally difficult problem typically relies on cryptographic hash functions, which map an input of arbitrary size to output of fixed size and cannot be inverted. In the Bitcoin network, validators bundle the information of several transactions and a reference to the current state of the blockchain and plug the data into a hash function. The hash function converts this input into a sequence of characters and numbers of certain length. The system\u0026rsquo;s protocol then requires that the output starts with a certain number of zeros. The probability of calculating a hash that starts with many zeros is very low and to generate a new hash, validators include a random number called nonce that can lead to a very different output. The difficulty of the problem is then determined by the number of leading zeros validators have to find. Depending on total available computational power, the system regularly adjusts the target to achieve an average of 10 minutes between two consecutive blocks.\nWhile validators in a PoW system utilize substantial computational resources to win the competition for block generation, validators might also be randomly chosen based on their wealth. In the so called Proof-of-Stake (PoS) protocol, validators stake their tokens to be able to create blocks. The higher a validator\u0026rsquo;s stake, the higher are the chances of creating the next block. After successfully appending a new block, the validator receives transaction fees just as in the case of PoW. If the validator submits an incorrect block or is offline during a staking period, then she is penalized and (at least partly) loses her stake. The penalty might either arise explicitly through a deduction of funds from the stake or implicitly as dishonest behavior creates a feedback on the value of the stake. In particular, if a validator appends to the blockchain in a way that perpetuates disagreement, then she imposes a cost upon all users of the particular blockchain. Such behavior lowers the value of the whole network and is also reflected in a lower valuation of the misbehaving validator\u0026rsquo;s stake. The endogenous feedback between validators\u0026rsquo; behavior and the value of their stakes incentives them to eventually reach consensus.\nOther consensus protocols combine features of PoW and PoS. For instance, delegated Proof-of-Stake (DPoS) relies both on stakeholders, who elect validators and have voting rights proportional to their stake, and validators, who exert effort to append information to the ledger. The reputation of validators determines their chance for reelection, while stakeholders have incentives to select truthful validators.\nDirected acyclic graphs While blockchains record transactions in blocks, DAGs store information in single transactions. More specifically, any transaction represents a node in a graph (i.e., a set of vertices connected by edges) and each new transaction confirms at least one previous transaction, depending on the configuration of the underlying protocol. The longer the branch on which a transaction is based, the more certain is its validity. Intuitively, only once a transaction is broadcasted sufficiently throughout the network, it is verified. DAGs thus hinge on a steady flow of new transactions that enter the network to verify and reference old transactions. The connections between transactions are directed (i.e., the edges in the form of confirmations are one way) and the whole graph is acyclic (i.e., it is impossible to traverse the entire graph starting from a single edge). Given a high number of transactions, DAG ledgers scale better and can achieve consensus faster than blockchains which rely on fixed block sizes and limited verification rates.\nSettlement latency in distributed systems A distributed system features settlement latency in transaction verification, as it is ex-ante unclear how long it takes until validators achieve consensus or to broadcast that consensus through the network. For PoW, latency depends on the time it takes for validators to find a solution to the computationally expensive problem. In the Bitcoin protocol, for instance, validators append a new block on average every 10 minutes, while the process takes about 20 seconds in the Ethereum protocol. For PoS, latency depends on how long disagreement on the correct order of transactions persists. For both blockchain-based protocols, the information still needs to be distributed to all other nodes in the network, possibly facing technological limitations that prevent instant percolation. Technological limits are also particularly relevant for DAGs which rely on a large number of nodes that verify transactions and distribute information through the network. Overall, any distributed system that refrains from using designated third-parties bearing the counterparty risk associated with transactions thus features settlement latency.\n The most simple illustrative example for asymmetric cryptography is the multiplication of prime numbers. One can easily multiply two prime numbers (private key) to get a large number (public key), but it can be difficult to infer the initial set of numbers from the product.\r^ The problem is more severe in a permissionless blockchain where anybody can access and potentially update the blockchain. Other variants, where only few institutions or individuals are entitled to direct access to the blockchain, so-called permissioned blockchains, limit the problem to few players.\r^   ","date":1575331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575331200,"objectID":"385edcc2abfadfe0677ae2eb7a7d1acd","permalink":"https://christophscheuch.github.io/post/settlement-latency/fundamentals/","publishdate":"2019-12-03T00:00:00Z","relpermalink":"/post/settlement-latency/fundamentals/","section":"post","summary":"A companion piece to [Hautsch et al (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302159) on how latency enters blockchain-based settlement","tags":["Academic"],"title":"Distributed Ledger Technology and Settlement Latency","type":"post"},{"authors":["[Nikolaus Hautsch](https://homepage.univie.ac.at/nikolaus.hautsch/)","[Christoph Scheuch](https://christophscheuch.github.io/)","[Stefan Voigt](https://voigtstefan.github.io/)"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"7a755c2d16372cc64eb49a7f21d349a3","permalink":"https://christophscheuch.github.io/publication/stochastic-latency/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/stochastic-latency/","section":"publication","summary":"Distributed ledger technologies replace trusted intermediaries with time-consuming consensus protocols to record the transfer of ownership. This settlement latency imposes limits to arbitrage and hinders price discovery. We theoretically derive arbitrage bounds that increase with expected latency, latency uncertainty, volatility and risk aversion. Using Bitcoin orderbook and network data, we estimate arbitrage bounds of on average 121 basis points, explaining 91% of the observed cross-market price differences. Consistent with our theory, periods of high latency risk exhibit large price differences, while asset flows chase arbitrage opportunities. Decentralized settlement without centralized clearing thus introduces a non-trivial friction that affects market efficiency.","tags":null,"title":"Trust Takes Time: Limits to Arbitrage in Decentralized Markets","type":"publication"},{"authors":["[Alexander Mürmann](https://www.wu.ac.at/finance/people/faculty/muermann/)","[Thomas Rauter](http://www.thomas-rauter.com/)","[Christoph Scheuch](https://christophscheuch.github.io/)"],"categories":null,"content":"","date":1510790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510790400,"objectID":"56c84d7d745a1cc19c70899dab3e5554","permalink":"https://christophscheuch.github.io/publication/fishing-with-pearls/","publishdate":"2017-11-16T00:00:00Z","relpermalink":"/publication/fishing-with-pearls/","section":"publication","summary":"We provide novel evidence of banks establishing lending relationships with prestigious firms to signal their quality and attract future business. Using survey data on firm-level prestige, we show that lenders compete more intensely for prestigious borrowers and offer lower upfront fees to initiate lending relationships with prestigious firms. We also find that banks expand their lending after winning prestigious clients. Prestigious firms benefit from these relations as they face lower costs of borrowing even though prestige has no predictive power for credit risk. Our results are robust to matched sample analyses and a regression discontinuity design.","tags":null,"title":"Fishing with Pearls: The Value of Lending Relationships with Prestigious Firms","type":"publication"}]